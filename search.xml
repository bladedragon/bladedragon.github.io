<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[我们依然需要仰望天空]]></title>
    <url>%2F2022%2F04%2F08%2Fboy-and-girl-and-earth%2F</url>
    <content type="text"><![CDATA[在《地球外少年少女》的正片中，如果单纯论观感，最后结局的演出和设计确实会让人觉得平庸甚至粗糙，有时我也会觉得有些桥段既视感过于强烈。但是不可否认的是，在其中确实能发掘一些值得思考的闪光点，加之这部作品对世界的架构和科学幻想部分深得我心，以及前半段的“太空求生”相对出色，因此我依然认为这部番看了不亏。遗憾的是久仰矶光雄大名，又因为其处女作《电脑线圈》拥有较高口碑，这样的作品确实有些不符合预期，最后还是需要感慨一下逝去的日本科幻动画黄金时代，不知何时，我们还能再次仰望星空呢。 介绍今天我们来讲讲一部冷门的番剧《地球外少年少女》， 1月番刚出的时候，其实就有关注过这部番剧，但是没想在热度方面竟然如此之低，导致我在1月番快完结的时候才想到有这样一部作品。热度低也确实有它的原因，毕竟这部作品的题材是现在少有的近现实太空科幻作品，整体的叙事紧凑且设定严密，没有毫无逻辑的怪力乱神（至少前期没有），因此在当前异世界大行其道的宅文化风格中独树一帜，与其说这个作品是属于2022年，倒更像是属于十年前或者二十年前科幻风还尚存一席之地的时代。也或许正是因为这个原因，我才对这部作品青睐有加。 当然，虽然从这部作品中我们能窥探一丝当年科幻黄金时代的遗风，但是它终究还是和它的前辈存在相当大的差距，六集的篇幅也无法让我们对这个精心构建的世界有更加深入的了解，结局更是褒贬不一，存在一定的争议。因此，我不能说这部作品能带给大家多少发人深省或者酣畅淋漓的体验，总的来说，它还是属于一道差强人意的快餐式作品。但是如果你喜欢传统科幻文学，那我依然会推荐这部作品，毕竟，这样的作品确实看一部少一部了。 剧情把握为了方便下面对一些关键剧情的阐述，这里还是简单介绍一下这部作品。《地球外少年少女》是矶光雄监督的第二部原创作品，他作为知名的原画师，出现在了许多为人熟知的作品如《机动战士高达Z》、《新世纪福音战士》、《机动战士高达0080 口袋中的战争》、《浪客剑心》等等，尽管他作为监督的作品不多，但所谓近朱者赤，在他的作品中依然能反映出那些经典作品的影子。因此，基于这些理由，我们可以放心期待他的作品，哪怕最终作品是个烂作，也一定有值得解读的地方。 故事发生在近未来，人类的科技水平已经达到了能在太空居住的程度，并有相当一部分人移居到了月球，并生育了第一批后代。但是，人们发现在月球诞生的后代无法适应月球环境，都存在先天性缺陷，只能靠零七科技公司开发的植入物来控制激素分泌得以存活。零七是人类历史上智能最高的AI，他为人类的科技发展做出了巨大贡献，但不出意外的，AI因为高度进化，甚至超过了人类可以掌控的范围，所以和所有电视剧中的桥段一样，人类历史上最高智能的AI 零七 又双 叕暴走了。于是人类对零七进行了安乐死，并于此同时限制了人工智能的发展。同时由于月球上的第二代繁育带来的生存困境和伦理问题，移民计划也受到了大多数人的反对，于是移居月球的人类也开始回归地球。故事主人公失野和他的青梅竹马菜叶就是在月球诞生的孩子之一，作为第一代也是最后一代真正诞生于地球外的孩子，他们即将回到地球，并在安心空间站做最后的适应训练。与此同时，地球上也有三位少年少女因为被活动选中而前往安心空间站参观。就在少年少女相遇之时，他们卷入了彗星撞击事件，故事也由此展开。 故事其实可以分成上下部分，前三集完全可以看成一部合格的太空灾难片，少年少女在摇摇欲坠的太空站不断利用身边资源和科学知识求生。得益于本剧相当翔实的世界观设定和丰富的细节作画，使得观众非常容易沉浸进去，给我的感觉就像是在看太空版《荒野求生》或者《地心引力》，如果你是细节控或者科幻爱好者，看到剧中的各种新奇但又符合逻辑的设定一定会一本满足。 事实上，从各大评分网站上，也明显能看出前半部分的评分要优于后半部分，如果没有后半部分，光看前半部分，观众不需要了解非常多世界观背景，当做传统的爆米花爽片其实也未尝不可。但如果真成这样，这部作品或许大众评价会高一点，但是也就没有品鉴的价值了。当故事来到后半部分，剧情发展虽说不乏反转和爆点，但是由于大量世界观设定和带有作者性的思想理论涌入，相信很多观众都会觉得有点迷惑。动画这这一方面的表现确实不算太好，很多思想的阐述都是以说教或者站桩对话的形式进行，一套看下来确实有点吃力。不过也不能完全归咎于动画制作，很多概念和理论本身就带有很强的学术性质，在加上这些信息相当集中饱和，因此在初次观看的时候难免会觉得云里雾里。 可以看出作者想要在剧中表达的东西很多，但可能是基于剧情容量或者编排，很多都是一带而过，如果有心，可以去一一考究。我也是基于篇幅，只能去主观展开一二，如果真的有喜欢该作品的粉丝，期待可以做出更多更专业的解释。 世界观的构建剧中最让我惊喜的地方，莫过于对未来科技社会的想象和还原。监督并没有去完全天马行空的设计一个架空世界，而是基于现实的科技，去合理想象和预测未来的社会形态。其实这样的设计更考验设计者的专业功底和知识广度。因为架空的社会往往存在一种“无限的能源”作为世界存在的基底，务实一点就是小型核聚变，大胆一点就是例如什么“XX炉”、“XX力”， “XX粒子”，一旦出现了某种违背常识的黑科技，似乎都可以用这种东西来解释。其实这就是设计上的偷懒，或者说“取巧”，毕竟社会的复杂性就是建立在如何利用各种有限的资源上，真正去细究这个世界的每一处细节，会发现我们的世界真的是十分复杂和精巧。因此矶光雄选择构想一个近未来的现实世界时，他所面临的设计难度是非常大的，每一处细节设计都需要基本符合现实的物理规则，这需要设计者充分了解相关行业的各类产品设计，而这样做带来的好处也是显而易见的，那就是真实感，观众能更加自然地融入到这个构想的虚拟世界，代入剧情。事实上，我认为《少年少女》中对于近未来的描绘是相当成功的。我们可以看到很多现实不存在但是很熟悉的东西，比如大脑植入物、触控手套或者主角旁的哈罗；又或者是在不熟悉的东场景看到熟悉的东西，比如在空间站上有商场一样的装饰布置，有自助售卖机等，虽然只是对现实元素的排列组合以及稍稍改进，但是要把这些元素构建成一个合理可信的社会，依然需要填充不少的细节。更何况，作者明显在航天领域方面有着相当的知识储备，例如在穿越空间站区域进行EVA（太空行走）那段，可以看出有相当的专业性。正是这些对场景细节的构建，才让我们能更加沉浸式地带入到剧情中。 当然作者选择构建这么一个和现实如此接近的社会，也是想借助这个虚构地社会去映射现实存在的问题。比如美衣奈这个角色，在参观三人组中，她的塑造是最完整的，从头到尾完成了一个vtuber的本职工作，作为一个第三者记录了太空中发生的一切。反观其他两位，白客大洋虽然前期还能和主角演演对手戏，推动剧情发展，但是后面交了Bright后就没了戏份，充当了背景板，另外一位弟弟博士（人名）则更是从头到尾背景板。不过就算是美衣奈也很明显被砍了很多剧情。美衣奈作为vtuber向地球观众直播的这条线实际上反映作者对消费主义和娱乐之上文化的态度。剧中还是明显压缩了地球观众舆论的剧情，当刚登上空间站到主角登矢的言论片段被上传网上引发误解，再到中途各种断网制造信息差，最后产生的各种化学反应，可以说在主线之外，另外开辟了一个相当现实和讽刺的舞台。但是有趣的是，如果说这些只是科幻映射现实的话， 利用大数据筛选观众喜好的网飞对该剧的注资以及剧中对优衣库的商业植入可以说是现实与科幻相互映射了，如果再搭配饭圈弹幕食用那更是真人体验了。被娱乐化的未来并非全是悲惨和黑暗的，只是在被信息裹挟的时代，我们筛选信息和做出正确判断的成本更高了，大众的集体意志也更容易被转变和引导，其实我们不是必须要给出这种存在方式的对与错，而是选择接受并正视这种存在的必然，并为此而做出改变。 剧情赏析剧中的主线其实是非常经典的AI与人之间的立场问题。现在大家都讨论AI与人，实质上就是过去机器人与人话题的延伸。在此基础上，可以引出了很多新的问题，我们是否能认可我们创造的物体具有自由意志？是否认可创造物成为与人类同等地位的物种？如果这些创造物真的具有了自由意志，他们是否会反噬甚至取代造物主？实际上，根据目前主流的观点，大家都默认认同存在即合理，如果真的出现具有自由意志的AI，我们尊重并认可其独立物种的地位，但是其中会一个关键点，怎样的区分创造物是否有自由意志？在剧中对AI的智能程度进行了量化，“哈罗”和“pod”一层层突破智能限制，最终达到并超过零七的过程看上去就像打怪升级一样简单，实际上其中隐含了一个完整的物种进化的过程，我们知道最终的零七2号是具有自由意志的，并认可其成为新的物种，但是智能程度处于之间的十二，black和bright这些呢？如何证明他们没有自由意志？如果他们有自由意志，我们还能约束和控制他们吗？虽然现在人类已经有图灵测试去检测AI是否具有人类智能，但是这依然只是人类的标准，更何况目前也有不少AI通过了图灵测试，但是我们认可他们的智能水平达到了人类级别吗？深挖下来其实有很多值得思考的话题，这些话题剧中没有给出答案，可能我们至今还无法给出完美的解释，也许只有等到问题真正出现在眼前，我们才能揭晓答案。 毕竟探求自由意志存在这个概念本身是一个学术的问题，我们也不希求在一部影视作品中追求得到结果。于是很多作品便把人与AI的冲突放在沟通上，毕竟如果AI能认同人类并不想毁灭人类，那依然能达成一个HAPPYEND的结局。而本作的AI 零七二号在与人类沟通，理解人类的过程中，抛出了这样一个话题，人作为集体和独立个人的概念，是不是一致。这个话题莫名让人感觉联想到EVA的橙汁结局，同样是在人要保持独立个体的存在还是打破心之壁，作为集体的概念存活上进行选择。当然，学过马哲的大家都明白，物质决定意识，抛开人实体存在而光论集体意识的存在是没有意义的。 但是国外的人们有不同的价值观，自然对此有不同的理解。因此我们也不必过于深究概念，毕竟在做长时间思想博弈的过程中，就有可能会不自觉地掉入虚无主义的泥潭，实际上，我认为对生活真正有指导意义的是区分个人意志和集体意志的边界。剧中人物的表现在我看到就是在演绎个人独立意志与集体意志如何冲突和调和的过程。在登矢为了拯救菜叶和零七2号直接对话的过程中，零七2号更多是在整体的角度的思考存亡问题，但是登矢却只关心菜叶个人的生死，并在对话中频频表示出强烈的个体主观意志，与之相反，菜叶却早早接受所谓的宿命，接受人类群体降下的意志，而不曾做独立的思考，直到登矢的言行重新唤醒菜叶内心对生存的渴望，这就是菜叶的独立意志，因此，二人才成功得以拯救。当然剧中对这段的演出说不上太好，很多概念一扫而过，缺乏合理解释，导致最后像是机械降神，主角凭借自带光环解决一切问题。虽然可能存在过度解读的可能，但我更倾向作者一定有基于思想层面的考量去设计最后的拯救桥段。 《少年少女》的正片中，如果单纯论观感，最后结局的演出和设计确实会让人觉得平庸甚至粗糙，有时我也会觉得有些桥段既视感过于强烈。但是不可否认的是，在其中确实能发掘一些值得思考的闪光点，加之这部作品对世界的架构和科学幻想部分深得我心，以及前半段的“逃生纪实”相对出色，因此我依然认为这部番看了不亏。遗憾的是久仰矶光雄大名，以及其处女作《电脑线圈》的高口碑，这样的作品确实有些不符合预期，总的来说，虽然这部作品不算特别完美，但在当今影视作品题材愈发同质化的形势下，还能有充满思想性和思考价值的作品出现实属难得，对于一般观众而言，算是一部合格水平良作，对于科幻爱好者来说，则更有理由去支持这种小众题材的作品 。 最后还是需要感慨一下逝去的日本科幻动画黄金时代，或许冰冷的机器，深邃未知的太空已经无法再激发人们的兴趣，但在科技飞速发展的现实，如果娱乐作品缺乏对科学技术的普及，对前沿思想的解构，沉默的大多数，是否拥有足够的独立意志做好了迎接未来的准备? 还是会成为被信息裹挟，被言语操控的乌合之众？ 或许有一天我们想起来仰望星空，会发现原来我们早已置于星空之中]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>影评</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[清明杂记]]></title>
    <url>%2F2022%2F03%2F30%2Fanxious%2F</url>
    <content type="text"><![CDATA[杂记结束了淅淅沥沥的阴雨天，4月的第一个周末，总算迎来了阳光明媚的早晨。久违地在起床后沐浴着晨风，安静地思考，才发觉，因为疫情和工作， 让时间流逝得如此之快。转眼间，这一年，就要过去三分之一了。虽然日子一天天正常过，但是脑海中总是布满各种各样阴郁的情绪，感觉无论走什么事情都有一种焦躁的心情，总想做出改变但总是无能为力，是谓之焦虑。 为什么会焦虑呢？ 一个原因是对现状不满而对未来还充满疑惑吧，另外一个原因是缺乏积累和成长的满足。前者也来自于浮躁带来的想一蹴而就的心态，后者则是安于现状的一种反噬吧。 说到底，现在需要的专注和持续的投入，没有什么事情是可以一簇而就的，时间会基于你应有得到回报]]></content>
  </entry>
  <entry>
    <title><![CDATA[2021年终总结]]></title>
    <url>%2F2021%2F12%2F31%2F2021-year-end-summary%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-hNHRGFEj"), narrow: false, autoplay: false, showlrc: false, music: { title: "One Last Kiss", author: "宇多田ヒカル", url: "http://music.163.com/song/media/outer/url?id=1824020871.mp3", pic: "http://p1.music.126.net/l3G4LigZnOxFE9lB4bz_LQ==/109951165791860501.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 继续做梦，继续做对的事情，继续期待。 Do Dream, Do Right, Do Expect 时间过得飞快，很难想象在圣诞节之后，2021年将会成为过去式。一年前的我，在做什么？一年后的我，将做什么？ 这一年，无疑会是我一生中最重要的一年之一。 我尝尽了校园的余味，迈入了名叫社会的圈子。用一段无忧无虑的生活，迎来了一个阶段性的谢幕。不知不觉已经到这样的年纪，曾经的理想生活都已变成了既定的现实。虽然这个现实和最初的梦想有了不小的偏差，但是总归来说还是意料之中的结局。 我是一个不自律的人，因此我必须给自己限定约束，才能得到我最大的满足。这是我20年不断验证的事实。忍耐是我的优点，无法专注是我的缺点。我知道自己的明暗面，也不会给未来过高的期许，但是，这不是给我设限的理由。过去的我总是封闭自己，因此我无法跳出自己给自己设计的逻辑怪圈，其实只要坚持，坚持做对的事情，我依然有无限的可能性。 今天看了冰冰的视频，平时其实都不会去关注，但不得不承认，有的时候人还是需要一点正能量的，也不是虚伪，不是逃避现实，恰恰相反，这些正能量正是驱动我们进步的现实之一。我们自以为理性，看透了现实的阴暗面，但是却无法承认现实中依然存在的美好的东西，这何尝不是一种非理性的想法？不是所有人的人生，都是去辩驳，去证伪，每个人都应该有自己追求的目标，这样，生活才不孤单，或者说，只要你心里还有对未来的期许，那样为之付出的每一步，都富有意义。 因此，新的一年，依然要继续立flag，继续做对的事情，继续过着让自己满意的生活。 将过去归档，存一份念想翻看相册，才明白什么叫光阴似箭，日月如梭。真的这些情景仿佛就在昨天展现，然而却快要跨过一年。 4月，结束了我可能是人生最后一次运动会，我任凭心里感情宣泄，将其化成辞藻，最后洋洋洒洒写了一堆心里话。虽然一如既往佶屈聱牙，充满了中二气息，但是这才是我嘛。这份孤高的气质，现在还留有几分呢? 4月底，我突然有了一个机会做临时实验，这可新鲜。我甚至开始期待这是不是代表可以根治我这毛病又能省钱？（虽然事实证明命运只是擦肩而过） 5月5日，纪念我的第二部手机，虽然一开始只是倔强得说只是临时机，但还是用的挺爽的。 我最后还是来到了一个不上不下的公司，这让我对数码产品也关注得多了一些，我也曾同样的心情下说出了只是一开始在这里，后面一定会跳槽的。但老实讲，心里还是没有底。但是的但是，生活并不是一成不变的，这次如此，不代表下次一定。具体我会变成什么样子，看今年表现。 最后的暑假，记录一下大饼。 是去而不返的校园:( 是武隆！这里有我第一次用心的vlog，有我最好的朋友。 这里有着对学校最后的留念和致意。 最后的答辩。 这里跳过非常多的毕业纪念照片…还是没有敢放出来的勇气..（放一张我很喜欢的契丹物语） 湖南行！与朋友和朋友的女朋友们…不过不妨碍这是一段非常开心的旅程~ （茶颜yyds） 一张背影，代表最好的我们 社会人的开始，二度进厂，做机的时光 年轻人的第一顿饭… 年轻人的第一次装机.. RGB就是信仰！！ 社畜的人生开始变成游戏的模样…. 年轻人的第一次团建… 心情是意外的好 一年就这样走向结束.. 这时今年最后一张照片. END 你完成了哪些计划? 回看去两年前的总结，感觉自己就像断层一样，果然，不记录一点什么，自己就无法知道自己到底活成了什么样子。以前的我还是有很多念想，现在，我具备了一切的条件，想做什么就必须去做吧 致未来对于未来，我不看好，但我充满期待。我活成什么样子，取决于我的执行力如何。 从今年起，我要开始立flag了。现在的我还年轻，比起研究生们，我还有3年时间。我也只有三年时间，必须让我能骄傲地立足于这个社会上。因此明年的主题，是定调，潜心研究。 首先是工作的flag： 明年的述职，一定是要自信和稳操胜券的。 技术上 明年至少要有一个自己的开源玩具 看完至少3本技术书 博客数量不少于6篇吧（感觉有点虚还是，新年就是要硬气点！） 生活上 坚持做饭的良好传统 每个月都要出门换换气 坚持锻炼，要保持肌肉max 习惯上 至少看完三本书（非技术类） 学会AE，至少剪一个视频 未来已来。之后的每一年都会一如既往，但是每一年一定都有不一样的精彩。人生有终点，但奔赴终点不是我们的目标。多抬头，四周望，去发现更加美丽的风景。 继续做梦，继续做对的事情，不留遗憾。]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airflow源码解析——调度器篇]]></title>
    <url>%2F2021%2F11%2F27%2Fairflow-01%2F</url>
    <content type="text"><![CDATA[因为airflow1.0和2.0变更了许多 接下来会通过分析airflow1.0和airflow2.0的源码来加深对airflow的理解 airflow简介airflow是一个工作流模式的分布式任务调度框架，可以实现复杂的工作流调度。因为工作原因需要对airflow进行调研，因此这里记录一下学习airflow过程遇到的问题并吸收一些实现技巧。接下来会根据airflow的各种组件，深入airflow的源码进行讲解。airflow从1.0到2.0，实现的架构进行了一次较大的改变，因此，我在这里同时分析两个大版本的源码，通过分析其中的异同，也可以一窥开发者的设计思路和优化方向。 首先放一张整体的airflow架构图 2.0和1.0在整体结构相似，这里大概会将airflow的解析分为调度器篇 worker(executor)篇 dag处理器篇 webserver篇 接下来的内容默认读者对airflow的基本概念有一定的了解，如果尚未了解相关内容，之后有时间我再考虑写一篇概念相关内容 airflow创建工作流的过程主要如下，注意airflow是一个命令驱动的框架，几乎所有的机制都是从发送命令行开始的 通过命令行创建dag和operator， 配置airflow的基本配置，包括数据库连接，webserver端口，连接板并发数，读取的存放dag文件夹的位置等等 启动调度器，webserver以及executor，这时框架会自动从设置好的的dag文件夹位置读取dag和operator的配置文件并生成dag和task存放进数据库 接着就可以登陆airflow的webserver查看各个dag的运行情况l airflow1.0 调度器部分这里我们选择的版本是1.8.2 首先观察一下airflow的主要结构 大致解释一下各个目录的作用 api: 放置了一些用于代码内接口调用的方法 bin：启动方法 contrib：应该是一些第三方插件的定制方法，包括sensor，executor，hook等 dag：dag的基类 example_dag: 一些dag的样例模板 executor：具体的executor hooks：用于方法内调用的hook，使得可以降低非核心功、能的耦合程度 macros：只有hive的一些方法，我还不是特别了解 migrations： 略 operators：定制的一些operator，即task的模板 security：有关用户认证和登录的方法 task_runner: 执行airflow命令行工具，有点像bash ti_dep: taskinstance 的一些依赖状态 utils: 工具类 www: 页面相关 可以看到目录结构非常之多，因此我们需要抓住一条主线来梳理。本篇选择以调度器作为切入口，我们就从调度器的创建开始吧。 airflow所有机制的运行入口几乎都是从命令行开始的，因此我们直接从命令行的入口方法查看，这里我们查看/bin/cli.py 可以看到再cli.py的CLIFactory类中存在大量的命令定义，这里在之后的学习中还会经常回到这里。我们直接查看scheduler的启动 123456789101112131415161718192021222324252627282930313233def scheduler(args): print(settings.HEADER) # 调度器的核心 job = jobs.SchedulerJob( dag_id=args.dag_id, subdir=process_subdir(args.subdir), run_duration=args.run_duration, num_runs=args.num_runs, do_pickle=args.do_pickle)# 如果挂后台，就将进程挂到daemonContext上 if args.daemon: pid, stdout, stderr, log_file = setup_locations("scheduler", args.pid, args.stdout, args.stderr, args.log_file) handle = setup_logging(log_file) stdout = open(stdout, 'w+') stderr = open(stderr, 'w+') ctx = daemon.DaemonContext( pidfile=TimeoutPIDLockFile(pid, -1), files_preserve=[handle], stdout=stdout, stderr=stderr, ) with ctx: job.run() stdout.close() stderr.close() # 通过信号直接启动进程 else: signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGTERM, sigint_handler) signal.signal(signal.SIGQUIT, sigquit_handler) job.run() 可以看见，核心在于创建schuedulerJob类，同时传入dag_id, 持续时间，并发数等基本参数 继续进入，可以发现我们跳转到了job.py 我们先查看一下schedulerJob的父类，baseJob.py 1234567891011121314151617181920212223242526272829303132333435363738394041class BaseJob(Base, LoggingMixin): __tablename__ = "job" id = Column(Integer, primary_key=True) dag_id = Column(String(ID_LEN),) state = Column(String(20)) job_type = Column(String(30)) start_date = Column(DateTime()) end_date = Column(DateTime()) latest_heartbeat = Column(DateTime()) executor_class = Column(String(500)) hostname = Column(String(500)) unixname = Column(String(1000)) def __init__( pass ) def kill(self): pass def on_kill(self): pass def heartbeat_callback(self, session=None): pass def heartbeat(self): pass def run(self): pass def _execute(self): pass @provide_session def reset_state_for_orphaned_tasks(self, dag_run, session=None): pass 可以发现，这里job首先是作为一张表保存了job的状态，执行情况（这里是sqlalchemy的相关知识） 主要的方法有 is_alive: 通过heartbeat判断是否存活 kill：通过删除数据库记录的方式来结束任务 on_kill：等待子类重写，用于删除任务的时候做一些额外处理 heartbeat_callback：心跳的回调，等待子类重写 heartbeat：发送心跳，下面详细介绍 run：修改表中job的状态，来表示job正在进行，同时执行_exexute _execute： 等待子类重写，如何执行job reset_state_for_orphaned_tasks：重置孤儿任务 下面我们看看heartbeat 12345678910111213141516171819202122232425262728293031def heartbeat(self): session = settings.Session() job = session.query(BaseJob).filter_by(id=self.id).one() make_transient(job) session.commit() session.close() if job.state == State.SHUTDOWN: self.kill() # Figure out how long to sleep for sleep_for = 0 if job.latest_heartbeat: sleep_for = max( 0, self.heartrate - (datetime.now() - job.latest_heartbeat).total_seconds()) # Don't keep session open while sleeping as it leaves a connection open session.close() sleep(sleep_for) # Update last heartbeat time session = settings.Session() job = session.query(BaseJob).filter(BaseJob.id == self.id).first() job.latest_heartbeat = datetime.now() session.merge(job) session.commit() self.heartbeat_callback(session=session) session.close() self.logger.debug('[heart] Boom.') 心跳在调度器运行期间定期发送，通过查表判断调度器状态是否正常 1234567891011121314151617@provide_session def reset_state_for_orphaned_tasks(self, dag_run, session=None):# 从执行器中获取等待入队的任务 queued_tis = self.executor.queued_tasks # 同样考虑正在执行器中执行的任务 running = self.executor.running tis = list() tis.extend(dag_run.get_task_instances(state=State.SCHEDULED, session=session)) tis.extend(dag_run.get_task_instances(state=State.QUEUED, session=session)) # 等待入队和正在执行器执行的任务属于正常情况不会处理 for ti in tis: if ti.key not in queued_tis and ti.key not in running: self.logger.debug("Rescheduling orphaned task &#123;&#125;".format(ti)) ti.state = State.NONE session.commit() 重置孤儿任务则是考虑在调度过程可能因为各种异常情况，如调度器进程突然中止或者没有没有执行器执行任务，就会产生无法继续执行的孤儿任务，这种任务将会在下一次调度前进行回收并再次调度 接下来就进入schedulerJob了，我们直接来看核心方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 def _execute(self): self.logger.info("Starting the scheduler") # 打开锁，对表连接进行独占 pessimistic_connection_handling() logging.basicConfig(level=logging.DEBUG) # dag可以被一些执行器pickle序列化，以便更容易地远程执行 pickle_dags = False if self.do_pickle and self.executor.__class__ not in \ (executors.LocalExecutor, executors.SequentialExecutor): pickle_dags = True # Use multiple processes to parse and generate tasks for the # DAGs in parallel. By processing them in separate processes, # we can get parallelism and isolation from potentially harmful # user code. # some log # Build up a list of Python files that could contain DAGs self.logger.info("Searching for files in &#123;&#125;".format(self.subdir)) # 遍历目录并查找python文件 known_file_paths = list_py_file_paths(self.subdir) self.logger.info("There are &#123;&#125; files in &#123;&#125;" .format(len(known_file_paths), self.subdir)) def processor_factory(file_path, log_file_path): return DagFileProcessor(file_path, pickle_dags, self.dag_ids, log_file_path)# 核心的方法，处理dag，将文件转换成dag processor_manager = DagFileProcessorManager(self.subdir, known_file_paths, self.max_threads, self.file_process_interval, self.child_process_log_directory, self.num_runs, processor_factory) # 执行调度 try: self._execute_helper(processor_manager) finally: self.logger.info("Exited execute loop") # 到这里调度结束，杀死子进程并退出 pids_to_kill = processor_manager.get_all_pids() if len(pids_to_kill) &gt; 0: # First try SIGTERM this_process = psutil.Process(os.getpid()) # 只检查子进程以确保因为子进程死亡但是进程ID被重用导致杀死错误进程的情况 child_processes = [x for x in this_process.children(recursive=True) if x.is_running() and x.pid in pids_to_kill] for child in child_processes: self.logger.info("Terminating child PID: &#123;&#125;".format(child.pid)) child.terminate() timeout = 5 self.logger.info("Waiting up to &#123;&#125;s for processes to exit..." .format(timeout)) # 等待进程被中止 try: psutil.wait_procs(child_processes, timeout) except psutil.TimeoutExpired: self.logger.debug("Ran out of time while waiting for " "processes to exit") # Then SIGKILL child_processes = [x for x in this_process.children(recursive=True) if x.is_running() and x.pid in pids_to_kill] if len(child_processes) &gt; 0: for child in child_processes: self.logger.info("Killing child PID: &#123;&#125;".format(child.pid)) child.kill() child.wait() 可以看到调度的流程是 加锁-&gt; 获取解析后的dag -&gt; 调度 -&gt;调度结束杀死进程 关于解析dag我们单独放一篇出来讲，这里我们直接进入execute_helper一探究竟 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146def _execute_helper(self, processor_manager): self.executor.start() session = settings.Session() self.logger.info("Resetting state for orphaned tasks") # grab orphaned tasks and make sure to reset their state active_runs = DagRun.find( state=State.RUNNING, external_trigger=False, session=session, no_backfills=True, ) for dr in active_runs: self.logger.info("Resetting &#123;&#125; &#123;&#125;".format(dr.dag_id, dr.execution_date)) self.reset_state_for_orphaned_tasks(dr, session=session) session.close() execute_start_time = datetime.now() # Last time stats were printed last_stat_print_time = datetime(2000, 1, 1) # Last time that self.heartbeat() was called. last_self_heartbeat_time = datetime.now() # Last time that the DAG dir was traversed to look for files last_dag_dir_refresh_time = datetime.now() # Use this value initially known_file_paths = processor_manager.file_paths # For the execute duration, parse and schedule DAGs while (datetime.now() - execute_start_time).total_seconds() &lt; \ self.run_duration or self.run_duration &lt; 0: self.logger.debug("Starting Loop...") loop_start_time = time.time() # Traverse the DAG directory for Python files containing DAGs # periodically elapsed_time_since_refresh = (datetime.now() - last_dag_dir_refresh_time).total_seconds() if elapsed_time_since_refresh &gt; self.dag_dir_list_interval: # Build up a list of Python files that could contain DAGs self.logger.info("Searching for files in &#123;&#125;".format(self.subdir)) known_file_paths = list_py_file_paths(self.subdir) last_dag_dir_refresh_time = datetime.now() self.logger.info("There are &#123;&#125; files in &#123;&#125;" .format(len(known_file_paths), self.subdir)) processor_manager.set_file_paths(known_file_paths) self.logger.debug("Removing old import errors") self.clear_nonexistent_import_errors(known_file_paths=known_file_paths) # Kick of new processes and collect results from finished ones self.logger.info("Heartbeating the process manager") simple_dags = processor_manager.heartbeat() if self.using_sqlite: # For the sqlite case w/ 1 thread, wait until the processor # is finished to avoid concurrent access to the DB. self.logger.debug("Waiting for processors to finish since we're " "using sqlite") processor_manager.wait_until_finished() # Send tasks for execution if available if len(simple_dags) &gt; 0: simple_dag_bag = SimpleDagBag(simple_dags) # Handle cases where a DAG run state is set (perhaps manually) to # a non-running state. Handle task instances that belong to # DAG runs in those states # If a task instance is up for retry but the corresponding DAG run # isn't running, mark the task instance as FAILED so we don't try # to re-run it. self._change_state_for_tis_without_dagrun(simple_dag_bag, [State.UP_FOR_RETRY], State.FAILED) # If a task instance is scheduled or queued, but the corresponding # DAG run isn't running, set the state to NONE so we don't try to # re-run it. self._change_state_for_tis_without_dagrun(simple_dag_bag, [State.QUEUED, State.SCHEDULED], State.NONE) self._execute_task_instances(simple_dag_bag, (State.SCHEDULED,)) # Call hearbeats self.logger.info("Heartbeating the executor") self.executor.heartbeat() # Process events from the executor self._process_executor_events() # Heartbeat the scheduler periodically time_since_last_heartbeat = (datetime.now() - last_self_heartbeat_time).total_seconds() if time_since_last_heartbeat &gt; self.heartrate: self.logger.info("Heartbeating the scheduler") self.heartbeat() last_self_heartbeat_time = datetime.now() # Occasionally print out stats about how fast the files are getting processed if ((datetime.now() - last_stat_print_time).total_seconds() &gt; self.print_stats_interval): if len(known_file_paths) &gt; 0: self._log_file_processing_stats(known_file_paths, processor_manager) last_stat_print_time = datetime.now() loop_end_time = time.time() self.logger.debug("Ran scheduling loop in &#123;:.2f&#125;s" .format(loop_end_time - loop_start_time)) self.logger.debug("Sleeping for &#123;:.2f&#125;s" .format(self._processor_poll_interval)) time.sleep(self._processor_poll_interval) # Exit early for a test mode if processor_manager.max_runs_reached(): self.logger.info("Exiting loop as all files have been processed " "&#123;&#125; times".format(self.num_runs)) break # Stop any processors processor_manager.terminate() # Verify that all files were processed, and if so, deactivate DAGs that # haven't been touched by the scheduler as they likely have been # deleted. all_files_processed = True for file_path in known_file_paths: if processor_manager.get_last_finish_time(file_path) is None: all_files_processed = False break if all_files_processed: self.logger.info("Deactivating DAGs that haven't been touched since &#123;&#125;" .format(execute_start_time.isoformat())) models.DAG.deactivate_stale_dags(execute_start_time) self.executor.end() settings.Session.remove() 内容有点长，主要可以分成这几部分 处理前调度的特殊情况 调度和解析dag 执行task-instance 调度结束后处理 先看调度前 1234567891011121314151617181920212223242526272829self.executor.start() session = settings.Session() self.logger.info("Resetting state for orphaned tasks") # grab orphaned tasks and make sure to reset their state active_runs = DagRun.find( state=State.RUNNING, external_trigger=False, session=session, no_backfills=True, ) for dr in active_runs: self.logger.info("Resetting &#123;&#125; &#123;&#125;".format(dr.dag_id, dr.execution_date)) self.reset_state_for_orphaned_tasks(dr, session=session) session.close() execute_start_time = datetime.now() # Last time stats were printed last_stat_print_time = datetime(2000, 1, 1) # Last time that self.heartbeat() was called. last_self_heartbeat_time = datetime.now() # Last time that the DAG dir was traversed to look for files last_dag_dir_refresh_time = datetime.now() # Use this value initially known_file_paths = processor_manager.file_paths 这里会首先寻找dag-run，dag-run是dag每次执行生成的实例，通过dag-run清理之前调度器遗留下来的孤儿任务 除了孤儿任务，首先看调度前有哪些依然正常运行的任务 dag-run有效同时task-instance还在调度中（schedue），这些可以直接被本次调度回收 dag-run有效同时task-instance还在进入队列中（queued），这些会被队列回收 因为 airflow2.0调度器部分airflow2.0因为使用python3作为开发语言，因此在架构上使用了许多新的特性，同时整体的文件组织结构也发生了较大的变化]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nvm安装记录]]></title>
    <url>%2F2021%2F11%2F21%2Fnvmfirst%2F</url>
    <content type="text"><![CDATA[最近新装了一台PC，因此需要同步博客框架。然而时间过去太久，新安装的nodejs16版本太高，hexo暂时无法支持，没办法，只能选择寻找一个nodejs的版本控制工具，来实现nodejs的版本切换。 安装介绍首先介绍一下什么是nvm: nvm全名node.js version management，是一个nodejs的版本管理工具。这里是下载地址,windows就直接用setup版本最方便了，会自动注入环境变量。 如果你的电脑里已经装了nodejs，也没有关系，至少最新版的nvm会自动检测并将你电脑里的nodejs加入自己的管理范围内，但是考虑到环境变量可能会发生变化，最好还是在干净的环境上从零开始装。 注意nvm管理的是nodejs版本，npm依然会使用默认的。如果是从nvm中安装的nodejs，默认的npm将是你第一个安装的nodejs附带 安装过程非常简单 这里是nvm主要的安装位置，会放置核心的组件和安装的nodejs包 这里的路径是选择启动的nodejs包，实际上通过一个快捷方式链接得到。 安装完成之后输入以下命令，如果获得正确版本号说明安装成功 1nvm --version 安装完后nvm的目录结构如图 注意上面的文件夹就是具体下载安装的nodejs包 再次提醒如果你在nvm安装之前已经安装nodejs，他的主体就会被移动到这里，这时最好再次检查一下环境变量是否还能正常生效 接下来就是简单的安装配置环境，列一下命令清单即可 1234567891011121314151617181920212223# 这里是常见命令nvm list #查看已经安装的版本nvm list installed # 查看已经安装的版本nvm list available # 查看网络可以安装的版本nvm version # 查看当前的版本nvm install # 安装最新版本nvmnvm use &lt;version&gt; ## 切换使用指定的版本nodenvm ls # 列出所有版本nvm current #显示当前版本nvm uninstall &lt;version&gt; # 卸载制定的版本# 这里是其他的命令nvm alias &lt;name&gt; &lt;version&gt; ## 给不同的版本号添加别名nvm unalias &lt;name&gt; ## 删除已定义的别名nvm reinstall-packages &lt;version&gt; ## 在当前版本node环境下，重新全局安装指定版本号的npm包nvm on # 打开`nodejs`控制nvm off # 关闭`nodejs`控制nvm proxy # 查看设置与代理nvm node_mirror [url] # 设置或者查看setting.txt中的node_mirror，如果不设置的默认是 https://`nodejs`.org/dist/nvm npm_mirror [url] # 设置或者查看setting.txt中的npm_mirror,如果不设置的话默认的是： https://github.com/npm/npm/archive/.nvm use [version] [arch] # 切换制定的node版本和位数nvm root [path] # 设置和查看root路径 如果正常安装生效的通过nvm ls 就可以看到效果： 常见问题1.之前已存在的nodejs安装nvm之后无法使用一般都是路径问题，常见方法 检查环境变量 在C盘的用户名文件夹下删除.npmrc，实际就是类似于删除缓存 如果无法解决问题再具体看报错信息具体分析 2.nvm运行use命令时报错exit status 1: ��û���㹻��Ȩ��ִ�д˲����� 主要是管理员权限的问题，如果安装文件夹需要管理员权限，执行的cmd又没有管理员模式，就会出现这样的问题 解决方法也很简单，用管理员方式执行cmd或者安装的时候注意不要安装到管理员权限的目录中即可]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>nvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个关灯神器]]></title>
    <url>%2F2021%2F11%2F21%2Fsmart-switch%2F</url>
    <content type="text"><![CDATA[自从租房之后，生活环境发生了很大的变化。其实大部分的变化我尚且还算满意，但每当晚上躺在床上玩手机的时候，当困意袭来时，心里总是非常郁闷：我在这边，灯的开关在床的那边。没办法，只得再度下床关灯，再摸黑爬上床——此时睡意早已消失（大哭 于是，我想到了关灯神器——远程操控开关关灯。 方案设计因为只是想做一个解放双手的小玩具，自然没有打算做的非常复杂。鉴于家里的开关设计（如下图），只需要一个可以通电左右摇摆的舵机即可，给予一定的电信号，就物理控制开关上下拨动 方案很简单，只需要一块单片机和一个舵机就可以，考虑到拨动开关所需的强度，如果开关很硬可以考虑提供额外的电源供电来保持舵机输出功率。好在家里的开关还算听话（勉强刚刚好，鬼知道我调整了多少次角度和高度），这次就不额外添加配件了。 参考B站的诸多”关灯神器”视频, 这里单片机选用了esp8266，类似下图 舵机则是使用了sg90： 总成本不到20元… (Ps:网上淘宝现成的开关神器换个马甲，多装个模块就能卖百来块…可见其中的利润空间，咱们理工男可不能被这种骗了钱去~) 剩下就是控制方案 常见的必然是手机app控制，这里必须借助第三方提供，自己写app也不是不可以，主要是不想徒增学习成本（说白了还是懒😂），连接方法有蓝牙、WiFi，这里可以看自己的开发板需求。 具体实现既然搞定了方案，接下来进行实践环节！ 硬件实现硬件方面很简单，注意看卖家给的引脚图不要插错即可。 注意观察，如果发现线序不对，可能需要自己拆下舵机的线材固定器更改线序。运气很好，我买的不需要，就直接插上连PC就行了 软件实现这里需要用到软件和安装包如下 8266开发板的配置包 开发板开发软件 blink电灯科技第三方应用的适配库文件 COM串口驱动 安装arduino首先安装arduino，直接无脑next即可 安装完成后打开arduino，界面如图： 导入ESP8266配置包首先打开Arduino IDE菜单 &gt; 文件 &gt;首选项，在 附加开发板管理器网址 输入框中，填入以下网址：https://www.arduino.cn/package_esp8266com_index.json 然后点击文件夹中的配置包： 导入配置。 安装三方库文件 添加库中添加blinker-library-0.3.80210803.zip 配置开发板接下来新建文件-&gt; 保存一下 -&gt;如下图配置开发板信息 如果没有8266开发选项，如下图，搜索8266，安装上图对应module 接下来就是编写代码,这边已经提供好了一个模板 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#define BLINKER_WIFI#define BLINKER_MIOT_LIGHT#include &lt;Blinker.h&gt;#include &lt;Servo.h&gt;Servo myservo;//定义舵机char auth[] = "xxxxxxxx"; //点灯Keychar ssid[] = "SELF-WIFI"; //wifi名称char pswd[] = "password"; //wifi密码// 新建组件对象，注意这里的名称要和电灯科技app里保持一致BlinkerButton Button1("test");int counter = 0;//注意如果接入小爱同学，其监听的就是这个电源事件void miotPowerState(const String &amp; state)//电源类操作&#123; BLINKER_LOG("need set power state: ", state); if (state == BLINKER_CMD_ON) &#123; myservo.write(150);//收到“on”的指令后舵机旋转150度 BlinkerMIOT.powerState("on"); BlinkerMIOT.print();//反馈状态 delay(1000);//延时1秒 myservo.write(90);//舵机归零，回到垂直状态 &#125; else if (state == BLINKER_CMD_OFF) &#123; myservo.write(30); //舵机偏转30° BlinkerMIOT.powerState("off"); BlinkerMIOT.print(); delay(1000); myservo.write(90); &#125;&#125;// 按下按键即会执行该函数void button1_callback(const String &amp; state)&#123; BLINKER_LOG("get button state: ", state); if (state=="on") &#123; myservo.write(170);//收到“on”的指令后舵机旋转150 delay(1000);//延时 myservo.write(90);//舵机归零垂直 &#125; else if(state=="press"||state=="tap") &#123; myservo.write(10);//长按开关按键后舵机旋转10 delay(1000);//延时 myservo.write(90);//舵机归零垂直(90度) &#125; &#125;// 如果未绑定的组件被触发，则会执行其中内容void dataRead(const String &amp; data)&#123; BLINKER_LOG("Blinker readString: ", data); counter++; &#125;void setup()&#123; // 初始化串口 Serial.begin(115200); BLINKER_DEBUG.stream(Serial); // 初始化舵机 myservo.attach(2);//舵机的IO口，nodemcu的D4口 myservo.write(90);//上电时舵机归零垂直 BlinkerMIOT.attachPowerState(miotPowerState); // 初始化blinker Blinker.begin(auth, ssid, pswd); Blinker.attachData(dataRead); Button1.attach(button1_callback);&#125;void loop() &#123; BLINKER_LOG("Blinked running..."); Blinker.run();&#125; 接下来，只需要点击左上角编译通过就完成了。 如果编译没有问题，接下来就需要将程序上传到开发板上。首先注意下图端口，经过之前安装com端口驱动，理论上这边端口可以正常显示 如果不能，查看设备管理器看是否端口正常显示。 安装APP还记得代码里的key吗？接下来就是下载电灯科技APP并申请key 一般应用市场都有，如果没有的话可以去官网下. 注册设备下载并注册点灯科技账号之后，在右上角加号-&gt;独立设备-&gt; 网络接入 一般推荐阿里云 接入，但是我目前的版本只能接入点灯科技服务，实测无论是控制还是接入小爱同学都没有影响，这里就看大家选择。 完成设备注册后如上图所示，这时你会获取一个key，填入代码中即可。 开发界面这里大家就可以按照代码中发信指引自己制作开关界面。我这边在网上找到一份简单的界面配置文件 1&#123;¨config¨&#123;¨headerColor¨¨transparent¨¨headerStyle¨¨dark¨¨background¨&#123;¨img¨¨assets/img/headerbg.jpg¨¨isFull¨«&#125;&#125;¨dashboard¨|&#123;¨type¨¨btn¨¨ico¨¨fad fa-lightbulb-on¨¨mode¨Ê¨t0¨¨开灯¨¨t1¨¨文本2¨¨bg¨Ì¨cols¨Í¨rows¨Í¨key¨¨test¨´x´Ë´y´Ì¨speech¨|÷¨cus¨¨on¨¨lstyle¨Ë¨clr¨¨#076EEF¨&#125;&#123;ßAßBßC¨fad fa-lightbulb¨ßEÉßF¨关灯¨ßHßIßJÌßKÍßLÍßMßN´x´Ë´y´ÑßO|÷ßRËßSßT&#125;&#123;ßA¨deb¨ßEÉßJÉßKÑßLÌßM¨debug¨´x´É´y´¤D&#125;÷¨actions¨|¦¨cmd¨¦¨switch¨‡¨text¨‡ßQ¨打开?name¨¨off¨¨关闭?name¨—÷¨triggers¨|&#123;¨source¨ßa¨source_zh¨¨开关状态¨¨state¨|ßQßd÷¨state_zh¨|¨打开¨¨关闭¨÷&#125;÷&#125; 将这份文件导入到配置信息即可。（上面的乱码是真的（汗）） 导入后配置如下： 接下来就可以自己操控查看配置情况了。 接入小爱同学接入小爱同学其实很简单，就看第三方服务方提不提供支持 米家-&gt;接入其他设备-&gt;点灯科技-&gt;同步配置即可 设备安装这一步在实际操作中反倒是最难的，因为每个人情况不同，按钮的强度也不一样，我一开始还是非常担心舵机的功率可能不足以拨动开关的。实际上确实在尝试了很多角度之后总是无法成功实现。 曾经尝试过给舵机扇叶进行延长增大接触面积，但是这些方案最终没有一个很好的实现。最后我尝试回归起点，通过垫高舵机，使得扇叶和开关产生一定的间隔，这样在扇叶旋转的时候，会有一个惯性辅助用力。接着使用纸板和502将舵机固定，并用透明胶在外层进行二次辅助固定，最终总算实现了开关的正常拨动（虽然有的时候还是不太灵敏） 最终实现图如下： 效果这里给出一个链接]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>单片机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM调试的一次经历]]></title>
    <url>%2F2021%2F11%2F07%2Fjvm-commands%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-elXEyvbp"), narrow: false, autoplay: false, showlrc: false, music: { title: "少年少女", author: "银杏BOYZ", url: "https://sharefs.ali.kugou.com/202111211220/2e649d44aa9f0a556b475deead519772/KGTX/CLTX001/9073bdd808ac142de731f73d28aa9f74.mp3", pic: "http://y.qq.com/music/photo_new/T002R300x300M000002fLaUt1Pydsn_2.jpg?max_age=2592000", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 像是经历了一次漂流，最终依然要选择回归现实 记录一次工作中遇到的GC分配失败问题 之前在工作中遇到一次IO占用异常的情况，通过iotop查看IO占用最高的进程，发现都是业务相关的java进程,于是通过查阅gc.log发现了类似如下的异常（图源网络，当场场景已不可复现, 当时场景与下图几乎类似） 问题最关键的是异常一直在输出，这让人不由得怀疑业务逻辑中存在的问题，事实上，通过这些，导致问题的原因其实已经能比较容易就能联想到，但是，本着务实求真（大雾）的态度，还是要细细了解其中出现的个中缘由 基础概念既然看到了内存分配的字样，我们首先自然得先复习一下JVM内存分配的基础概念。 我们知道，JVM中采用分代的内存分配策略，将java堆分成了新生代，老年代。仔细分的话，新生代又能分成eden区和survivor区（分为0和1两个区域，这也是为了更好的进行垃圾回收，比如使用复制算法） 一个对象从创建之初就有两种选择 小对象进入新生代 大对象进入老年代 我们以小对象举例，在进入新生代后，对象首先会被分配到eden区，在这个过程中，就会经历到垃圾回收（GC）。GC又分为minor GC和fullGC（还有major GC，这里不作解释，通常可以认为是fullGC等价）minorGC仅进行新生代范围的垃圾回收，因为范围少，所以回收快，一般不会对执行中的进程 造成很大影响（视收集器算法而不同，minor GC也会暂停线程工作，导致STW），当eden区满后，就会执行一次minorGC， 经过一次minorGC之后，如果对象存活，就会进入survivor区 survivor区的对象存活时间会稍长，因为当存活对象经历了数次GC之后（默认是15次，每进行一次对象年龄+1），对象才会进入老年代。 大对象和经历数次回收的对象存活在老年代，这里区域很大，数据变动也不频繁，代价就是垃圾回收的时候影响很大。如果老年代区域空间占满导致full GC，就会stop the world（暂停当前所有线程工作，会影响任务执行） 至此，一个对象从出生到死亡的整个过程就结束了。现在回到开始，我们可以通过查看gc.log来观察对象内存分配情况 1[GC (Allocation Failure) [ParNew: 367523K-&gt;1293K(410432K), 0.0023988 secs] 522739K-&gt;156516K(1322496K), 0.0025301 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 这是其中一条记录。我们分别看看这些参数代表什么含义： GC：代表进行了一次垃圾回收，前面没有full修饰，说明是minor GC，因此单论这次GC，本身不会对任务造成太大影响· Allocation Failure: 结合上面的minor GC可以得出此次内存分配失败是新生代没有足够的空间导致的，因此会触发minor GC，让部分对象进入老年代 parNew 说明这次GC发生在新生代，同时使用的收集器是parNew收集器，ParNew是一个Serial收集器的多线程版本，会使用多个CPU和线程完成垃圾收集工作，该收集器采用复制算法回收内存，期间会停止其他工作线程,造成STW。 之后的参数代表 GC前内存区域使用容量-&gt;GC后内存区域使用容量(内存区总容量) 后面的时间代表耗时 接着又是一组数据，代表堆区垃圾回收前的大小-&gt;堆区垃圾回收后的大小(堆区总大小) 以及该内存区域GC耗时 最后的Times则是总耗时，分别代表 用户态耗时 内核态耗时 总耗时 分析下可以得出结论： 该次GC新生代减少了367523-1293=366239K Heap区总共减少了522739-156516=366223K 366239 – 366223 =16K，说明该次共有16K内存从年轻代移到了老年代，可以看出来数量并不多，说明都是生命周期短的对象，只是这种对象有很多。 我们需要的是尽量避免Full GC的发生，让对象尽可能的在年轻代就回收掉，所以这里可以稍微增加一点年轻代的大小，让那17K的数据也保存在年轻代中。 (这里介绍几个修改JVM的参数) 排查方法这里针对排查java出现的异常，做一下简单的命令行记录。 jdk内置了若干命令行工具，这些命令在 JDK安装目录下的 bin目录下： jps (JVM Process Status）: 类似 UNIX的 ps 命令。用户查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； jstat（ JVM Statistics Monitoring Tool）: 用于收集 HotSpot虚拟机各方面的运行数据; jinfo (Configuration Info for Java) : 显示虚拟机配置信息; jmap (Memory Map for Java) :生成堆转储快照; jhat (JVM Heap Dump Browser ) : 用于分析 heapdump文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; jstack (Stack Trace for Java):生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 jps显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID） *注意：必须要在进程执行的对应用户权限下操作 123456C:\Users\SnailClimb&gt;jps7360 NettyClient2173967972 Launcher16504 Jps17340 NettyServer 参数 jps -l：输出主类的全名，如果进程执行的是 Jar包，输出 Jar路径 jps -q：只输出进程的本地虚拟机唯一 ID jps -v : 输出虚拟机进程启动时候JVM参数 jps -m： 输出传递给main函数的参数· jstat使用于监视虚拟机各种运行状态信息的命令行工具 1jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] jstat -class vmid ：显示 ClassLoader的相关信息； jstat -compiler vmid ：显示 JIT编译的相关信息； jstat -gc vmid ：显示与 GC 相关的堆信息； jstat -gccapacity vmid ：显示各个代的容量及使用情况； jstat -gcnew vmid ：显示新生代信息； jstat -gcnewcapcacity vmid ：显示新生代大小与使用情况； jstat -gcold vmid ：显示老年代和永久代的信息； jstat -gcoldcapacity vmid ：显示老年代的大小； jstat -gcpermcapacity vmid ：显示永久代大小； jstat -gcutil vmid ：显示垃圾收集信息； jstat -t : 在输出信息上加一个 Timestamp列，显示程序的运行时间。 jinfo输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。 *使用 jinfo 可以在不重启虚拟机的情况下，可以动态的修改 jvm 的参数 1234C:\Users\SnailClimb&gt;jinfo -flag MaxHeapSize 17340-XX:MaxHeapSize=2124414976C:\Users\SnailClimb&gt;jinfo -flag PrintGC 17340-XX:-PrintGC jinfo -flag [+|-]name vmid : 开启或者关闭对应名称的参数。 1234567C:\Users\SnailClimb&gt;jinfo -flag PrintGC 17340-XX:-PrintGCC:\Users\SnailClimb&gt;jinfo -flag +PrintGC 17340C:\Users\SnailClimb&gt;jinfo -flag PrintGC 17340-XX:+PrintGC jmap用于生成堆转储快照.dump文件，除此之外生成dump文件的方法还有 -XX:+HeapDumpOnOutOfMemoryError 参数让虚拟机在 OOM异常出现之后自动生成 dump文件 Linux命令下可以通过 kill -3 发送进程退出信号也能拿到 dump文件 123C:\Users\SnailClimb&gt;jmap -dump:format=b,file=C:\Users\SnailClimb\Desktop\heap.hprof 17340Dumping heap to C:\Users\SnailClimb\Desktop\heap.hprof ...Heap dump file created jhat用于分析 heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果。 12345678910C:\Users\SnailClimb&gt;jhat C:\Users\SnailClimb\Desktop\heap.hprofReading from C:\Users\SnailClimb\Desktop\heap.hprof...Dump file created Sat May 04 12:30:31 CST 2019Snapshot read, resolving...Resolving 131419 objects...Chasing references, expect 26 dots..........................Eliminating duplicate references..........................Snapshot resolved.Started HTTP server on port 7000Server is ready. jstack用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合. ·查找耗时时间长的线程的通用方法： 使用jps查找出java进程的pid，如7777或ps -ef | grep java 使用top -p 7777观察进程情况，然后Shift+h,显示该进程的所有线程。 找出CPU消耗较多的线程``id，如7788，将7788转换为16进制0x1e6c`，注意是小写。 使用jstack 7777 | grep -A 10 0x1e6c来查询出具体的线程状态。-A 10表示查找到所在行的后10行 结语现在有了这些知识储备，相信解决一些常见问题应该不会无从下手了。现在返回到最上面的问题。当时出现了一直打印内存失败的信息，首先判断出这是minor ``GC，如果出现频率低那可以暂时不用考虑，属于正常现象；但是依然需要去思考优化如何减少GC。这次的问题在于一直在有对象进入老年代，可见如果持续i下去，内存必然占满。因此，很可能是代码逻辑出现了问题，例如死循环等，让任务一直不断执行，不断生成新对象，消灭老对象。最后查看代码逻辑，果然定位到了问题所在。 当然，如果是经验丰富的老手一定能在第一时间判断出问题的根源。但是我依然还在不断学习，还是需要通过记录的方式，总结经验，提升能力。]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十一回家小记]]></title>
    <url>%2F2021%2F10%2F04%2Foctober_first%2F</url>
    <content type="text"><![CDATA[感觉就像一场梦。 自己推箱子出门的记忆仿佛就在昨天，而这两天在家里和家人团聚的感觉却是那样朦胧，缺乏实感。 母亲的唠叨一如既往，父亲的后背依然可靠，回到家，感觉自己毫无长大，依然是个需要依赖的孩子 有的时候，还是需要停下来，思考一下现实，回味一会儿过去，让失去理智的大脑重新冷静下来，然后才能明白自己想要做什么，将要做什么，才不会在最关键的时刻，失去了机会，迷失了方向。]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于js运行机制的简单理解]]></title>
    <url>%2F2020%2F06%2F01%2Fjs-runtime%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-HFFIyvxr"), narrow: false, autoplay: false, showlrc: false, music: { title: "青い栞", author: "Galileo Galilei", url: "http://music.163.com/song/media/outer/url?id=417613399.mp3", pic: "http://p1.music.126.net/IMAMMCzMU7InvJhLD7U4xA==/3393092891197502.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 最近在学前端基础，今天梳理一下javascript的运行机制 只是简单理解，完全没入门那种~ 首先必须记录的事Javascript是单线程的，目的是简化和用户的交互逻辑，从而不会出现严重的并发问题。 但是js脚本可以创建多个线程，但是创建的子线程受到主线程控制，同时不能操作DOM。 相关进程和线程在浏览器中加载页面主要用到4个进程： 主进程：（Browser进程）负责页面展示和资源下载等 GPU进程：负责3D图示绘图 第三方插件进程：负责第三方插件的处理 渲染进程：（Render进程），负责js执行，页面渲染等功能 其中渲染进程中又包括： GUI渲染进程 Js引擎线程 事件循环线程 定时器线程 http异步线程 … 这里主要看渲染进程的相关线程 GUI渲染线程主要处理的事情包括： 首先浏览器会解析html代码（实际上html代码本质是字符串）转化为浏览器认识的节点，生成DOM树，也就是DOM Tree 然后解析css，生成CSSOM（CSS规则树） 把DOM Tree 和CSSOM结合，生成Rendering Tree(渲染树) 注意其中存在重绘和回流： 如果修改了一些元素的颜色或者背景色，页面就会重绘（Repaint），如果修改元素的尺寸，页面就会回流（Reflow），当页面需要Repaing和Reflow时GUI多会执行，进行页面绘制。 Js引擎线程负责解析和执行js代码，浏览器同时只能有一个JS引擎线程在运行JS程序，所以js是单线程运行的。 Js线程会阻塞渲染线程，因此&lt;script&gt;标签和其他DOM标签是顺序执行的，一旦执行到&lt;script&gt;标签就会立即执行,所以一般我们把&lt;script&gt;标签放在·body的最后 事件循环线程用来控制事件的循环，维护和管理一个任务队列（task queue）。 下面会讲到关于事件队列的用法 定时器线程单独用来计时，计时完成将定时器执行的操作添加到事件任务队列尾。 异步请求线程执行到一个http异步请求时，便把异步请求事件添加到异步请求线程，等待响应；并把回调函数添加到事件队列，等待js引擎线程执行 EventloopEventloop其实是Js的执行机制，表示了线程之间的协作关系 因为js是单线程，因此任务的执行只能顺序执行，为了将耗时的任务分离出来，在js中实现了两种任务类型，同步任务和异步任务。 所有同步任务在主线程上执行，形成执行栈 任务队列（task queue，就是由事件循环线程维护）负责存放异步任务的运行结果 执行栈的同步任务执行完成后会读取任务队列，将对应的异步任务加入执行栈执行 上述操作不断循环往复 诸如此图 注意一句话: javascript的执行和运行有很大的区别，javascript在不同的环境下，比如node，浏览器，Ringo等等，执行方式是不同的。而运行大多指javascript解析引擎，是统一的。 定时器定时器是一个相当特殊的存在，任务队列中不仅可以放普通事件，也可以放定时事件。 主要由setTimeout()和setInterval()两个函数完成 值得注意的是 定时器不会完全按照规定时间执行 W3C标准规定setTimeout中最小的时间周期是4毫秒，凡是低于4ms的时间间隔都按照4ms来处理。因此代码不会完全精确按照所定时间执行 主线程执行到定时器后，会把定时任务交给定时器线程执行，等到时间了，定时器所要执行的操作会放到任务队列末尾，因此不一定立即执行 setInterval存在累计效应：如果定时器里面的代码执行所需的时间大于定时器的执行周期，会导致有些事件丢失 可以使用setTimeout()代替setInterval() 123456var say = function() &#123; setTimeout(say, 1000) console.log('hello world')&#125;setTimeout(say, 1000) 剩下的以后再总结… 参考链接JavaScript 运行机制详解：再谈Event Loop js运行原理与机制]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次跨域问题]]></title>
    <url>%2F2020%2F06%2F01%2Fcors%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-PJJnecwK"), narrow: false, autoplay: false, showlrc: false, music: { title: "没那么简单", author: "黄小琥", url: "http://music.163.com/song/media/outer/url?id=478384.mp3", pic: "http://cdn.zblade.top/qiniu_img/image-20200601171805737.png", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 最近在做前后端交互的时候遇到了特殊的跨域问题。其实自己平时做项目的时候也多多少少处理过跨域问题，但是这次情况特殊。正好借此机会将跨域问题的解决方法做一下总结，以备不时之需。 情景自己前段时间在学习前端，所以想趁着这个机会自己搞个小工具玩玩。所以就有了如下这个极其简陋的弹幕网站。 为了实现弹幕的持久保存，自己尝试前后端交互将弹幕存储在数据库中，然后前端每间隔一段时间就从后台拉取一次数据。 实现其实比较简单，但是当我在本地开始测试向后台发送请求的时候，问题出现了： 很明显这是一个跨域问题，所以接下来我们就要动用我们所学的知识去解决它。 首先第一步，分析问题原因：什么是跨域？ 什么是跨域？ 跨域是指一个域下的文档或脚本试图去请求另一个域下的资源 跨域其实不是什么bug，而是由于浏览器为了安全起见指定了一系列“同源策略”，这些同源策略可以保证用户信息的安全，防止被恶意的网站窃取数据。 该政策限制网页的某些行为必须限制在与自己“同源”的网页中才能进行，如果不是同源，就会导致该行为无法生效，也就是跨域失败。 这里出现了两个概念，一个是同源，一个是跨域行为 同源的定义包含三个方面 协议相同 域名相同 端口相同 只有三个条件都满足，才能认定两个网页是”同源“ 跨域行为（自己定义的名词，大概就是我们会被同源政策影响到的操作）随着互联网的发展，范围变得越来越宽泛，一般我们常见的跨域行为包括 获取 Cookie、LocalStorage 和 IndexDB 获得DOM和JS对象 AJAX请求 我们可以看到在前后端交互中最常见的AJAX请求也赫然在列，在前后端交互中解决跨域问题的不可避免的。 那为什么要定义同源策略呢？没有跨域限制不是更好吗？ 如果没有跨域限制，网页将很容易受到XSS、CSRF等攻击，因为没有限制，恶意网站同样可以自由地发起攻击，这将大大提高网站的维护成本。因此，同源策略其实是一把双刃剑，只是在保护网页的同时，偶尔总会误伤友军。 知道了问题的根源，我们就可以对症下药，寻找解决方案 解决方案我们可以看到，跨域问题的关键在于我们的M请求处于限制范围内，没有做到同源&gt;，从而导致的。 重点已经标出来了，其实我们解决的方法也就是从这两个思路着手 采用不在同源策略的行为操作 想办法让行为处于同源状态 这里我们指针对AJAX请求，对于其他诸如cookie、iframe等的跨域方案，其实参考相关的博客相信一定能得到答案 1.JSONP跨域我们可以通过在AJAX请求中定义JSONP类型实现跨域，虽说如此，但JSONP本质上采用的是和AJAX完全不同的请求方式。 传统的AJAX请求其实是xhr的异步请求，而JSONP本质上是去构建一个&lt;script&gt;标签，利用script标签中的src不受同源政策的限制，在src中填写后端URL并添加回调函数，获取到的数据就通过回调函数处理。 参考阮一峰的博客,实现思想大致如此： 1234567891011121314function addScriptTag(src) &#123; var script = document.createElement('script'); script.setAttribute("type","text/javascript"); script.src = src; document.body.appendChild(script);&#125;window.onload = function () &#123; addScriptTag('http://example.com/ip?callback=foo');&#125;function foo(data) &#123; console.log('response data: ' + JSON.stringify(data));&#125;; 由于&lt;script&gt;元素请求的脚本，直接作为代码运行。这时，只要浏览器定义了foo函数，该函数就会立即调用。作为参数的JSON数据被视为JavaScript对象，而不是字符串，因此避免了使用JSON.parse的步骤。 如何在AJAX实现： 1234567$.ajax(&#123; url: 'http://www.domain2.com:8080/login', type: 'get', dataType: 'jsonp', // 请求方式为jsonp jsonpCallback: "handleCallback", // 自定义回调函数名 data: &#123;&#125;&#125;); 如何在vue上实现： 123456this.$http.jsonp('http://www.domain2.com:8080/login', &#123; params: &#123;&#125;, jsonp: 'handleCallback'&#125;).then((res) =&gt; &#123; console.log(res); &#125;) 从实现原理上可以看出JSONP还是存在弊端，那就是使用JSONP必须是GET请求，如果要POST请求实现跨域，还是需要使用其他方法 2.WebSocketwebsocket本身就是一种通信协议，通过websocket通信，实际上就可以跨过同源策略， 实现某种意义上的”同源”。 下面是websocket请求的HTTP头信息，重点关注Origin字段，这是实现跨域的关键 12345678GET /chat HTTP/1.1Host: server.example.comUpgrade: websocketConnection: UpgradeSec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw==Sec-WebSocket-Protocol: chat, superchatSec-WebSocket-Version: 13Origin: http://example.com Origin字段表示该请求的请求源，只要Origin字段中的源域名和请求的目的域名是同一个，就可以通过同源策略中的域名一致，实现跨域。 如果允许通信，WebSocket的响应头如下 12345HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: UpgradeSec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk=Sec-WebSocket-Protocol: chat 3.CORS CORS即跨域资源共享”（Cross-origin resource sharing），它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 这是解决跨域问题的常用方法。 实现原理其实现原理如图 CORS请求主要分成两类：简单请求和非简单请求。 满足以下条件的就是简单请求，否则就是非简单请求 （1) 请求方法是以下三种方法之一： HEAD GET POST （2）HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 简单CORS请求只是在请求的时候在http头中加入Origin字段 非简单CORS请求的话，浏览器会在正式通信后先发送预检请求，先询问服务器是否允许请求，得到响应，检查相关字段后就可以做出回应，发起正式请求 假设现在发起一段js脚本 12345var url = 'http://api.alice.com/cors';var xhr = new XMLHttpRequest();xhr.open('PUT', url, true);xhr.setRequestHeader('X-Custom-Header', 'value');xhr.send(); 其中预检请求的请求方法是OPTIONS，具体请求头类似如下 12345678OPTIONS /cors HTTP/1.1Origin: http://api.bob.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 主要关注三个字段 Origin 表示请求来自哪个源 Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 得到响应如下之后就能确认允许跨域请求 123456789101112HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderContent-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 其中 12345678910//表示支持任意跨域请求Access-Control-Allow-Origin: *//表示支持跨域请求的方法Access-Control-Allow-Methods: GET, POST, PUT//当浏览器请求包含Access-Control-Request-Headers的时候必需，表示支持的头信息字段Access-Control-Allow-Headers: X-Custom-Header//允许发送cookie和认证信息Access-Control-Allow-Credentials: true//指定本次预检请求的有效期Access-Control-Max-Age: 1728000 实现方式这里主要是后端的操作，这里用了java springboot的跨域方式做为样例 1234567891011121314151617@Configurationpublic class CORSConfiguration &#123; @Bean public WebMvcConfigurer corsConfigurer() &#123; return new WebMvcConfigurer() &#123; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping("/**") .allowedOrigins("*") .allowedHeaders("*") .allowCredentials(true) .allowedMethods("GET", "POST", "DELETE", "PUT","PATCH") .maxAge(3600); &#125; &#125;; &#125;&#125; 其实就是在服务端响应的时候添加请求头，springboot还支持在不同controller上使用注解添加 4.nginx代理跨域这个原理也简单，其实就是让前端和后端处于同源上，利用nginx的反向代理可以修改请求的域名、端口，也能添加cookie信息啥的实现跨域 123456789101112131415#proxy服务器server &#123; listen 81; server_name www.domain1.com; location / &#123; proxy_pass http://www.domain2.com:8080; #反向代理 proxy_cookie_domain www.domain2.com www.domain1.com; #修改cookie里域名 index index.html index.htm; # 当用webpack-dev-server等中间件代理接口访问nignx时，此时无浏览器参与，故没有同源限制，下面的跨域配置可不启用 add_header Access-Control-Allow-Origin http://www.domain1.com; #当前端只跨域不带cookie时，可为* add_header Access-Control-Allow-Credentials true; &#125;&#125; 另外使用一些中间件的代理方式其原理都是这回事，这里就不加赘述 分析问题一通分析上面解决方案说了一大堆，但最终还是要回归我们的问题，这次，我们开始对症下药。首先再看一遍报错日志： 嗯？好像和想象中的不太一样，常见的跨域问题应该如同： 这种，看起来其中有诈？ 果不其然，通过后台添加跨域设置，我们的报错信息依然没有变化。 这时我们就需要对报错信息好好分析（其实这应该是分析日志的第一步，为了强行引入跨域解决方案，因此特地将分析放在了后面） 这句话引起了我的注意 Cross origin requests are only supported for protocol schemes: http, data, chrome, chrome-extension, https. 通过查阅资料发现，原来这里我的前后端交互都是再本地实现，本地打开html使用的file协议，但是file协议的请求无法被浏览器认可，网上提供的方法如下 在谷歌浏览器下的快捷方式位置 在目标处添加： 1"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe" -args --disable-web-security --user-data-dir --allow-file-access-from-files 大概就是这样，但是我还是不推荐使用这种方法，因为这样的方式并不是特别优雅的解决方法 另外的尝试另外的解决方法就是在本地部署nginx，诸如上面提到过的解决方法，不通过file协议打开文件。 什么是file协议的打开方式？ 大概就是这种 要换成用http形式打开的方式，诸如这种 还有如果不嫌麻烦直接把网页部署到服务器上也是一种解决方法 意外的结果但是！！最终问题还是没有得到解决！这可把我难到了。。 事必有因，经过一个多小时的不懈努力，我终于找到了问题的根源 ———— ajax的请求URL必须以http的格式。。。。 啊啊啊啊果然还是我太菜了。。 因为刚学ajax，对其原理不熟，导致最后出现了这种问题。 参考链接阮一峰的博客 ajax跨域，这应该是最全的解决方案了 前端常见跨域解决方案（全）]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[netty_reactor线程模型]]></title>
    <url>%2F2020%2F05%2F31%2Fnetty-reactorrctor-1%2F</url>
    <content type="text"><![CDATA[Netty是一个封装了JAVA NIO的异步网络应用框架，它简化了我们网络编程，同时保证了高可用和高性能，因此我认为学习netty对以后深入一些RPC框架，或者接触一些网络通信都大有裨益。 今天就是想从源码角度学习一下netty的reactor线程模型，这应该是netty的核心之一，整篇文章包含的只是我作为一个初学者的粗浅见解，同时也借鉴了大量的博客文章，因此对部分内容认识不够深入，或者存在逻辑不通的情况，希望能够谅解。 Reactor简介在接触netty之前，首先先介绍一些基础知识，让我们了解一下什么是Reactor模式 维基百科的解读如下 The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers. 翻译总结主要有这么几点 reactor是一种事件驱动的设计模式 reactor可以同步地接受多个输入源 reactor以多路复用的模式分发多个请求到对应的处理器上去 让我们拿实际情景做例子。 在传统BIO中，一旦遇到多个事件请求并发，事件监听器只会将后面的线程阻塞，等到当前线程处理结束后才能继续进行监听接受后续请求，这样会大大降低吞吐量，增大系统的负载，在高并发的场景下容易造成明显的数据延迟。 因此在NIO中将耗时的IO处理操作和请求的监听接受分离开来，由一个线程专门监听事件请求，当接受一个请求后再开启一条线程专门处理IO请求，从而主线程不阻塞，达到了非阻塞。 至此，Reactor的基本架构已经出来了，在上述文字中已经隐含了Reactor的三种基础角色： Reactor 将I/O事件分派给对应的Handler Acceptor 处理客户端新连接，并分派请求到处理器链中 Handlers 执行非阻塞读/写 任务 经典的reactor线程模型如图所示， 但是这只是最初级的架构，针对具体复杂的场景，还需要做出诸多优化，比如构建处理IO请求的线程池、将监听事件和分发事件进一步解耦、进一步减少资源开销等，这些在这里就不再展开了。 netty中核心的reactor线程模型就是一个相对更成熟和高性能的模型，它的架构更像是下图，是一个多reactor多线程的模型。 其中mainReactor 主要是用来处理网络IO 连接建立操作，通常一个线程就可以处理，而subReactor主要做和建立起来的socket做数据交互和事件业务处理操作，它的个数上一般是和CPU个数等同，每个subReactor一个线程来处理。 关于reactor模式的暂时就介绍这么多了，其实关于事件的处理还有其他的处理模式。在Douglas Schmidt的作品《POSA2》中提到了有四种事件处理模式: Reactor Proactor Asynchronous Completion Token Acceptor-Connector 这些以后有时间再研究，今天就先专注netty的事件处理 Reacor线程的启动和创建得益于netty良好的封装，使得我们只要接触过netty，相信都能直接感受其中reactor模式的存在，下面给一个简单的netty服务端的启动demo (本人环境 netty-all-4.1.48 )： 123456789101112131415161718192021222324252627public class NettyServer &#123; private final static int PORT = 8000; public static void main(String[] args) &#123; //netty的启动引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); //绑定线程组，这时我们今天重点关注对象 NioEventLoopGroup boss = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); //在引导类中配置相关选项，这些暂时不要去在意它 serverBootstrap .group(boss,worker) .option(ChannelOption.TCP_NODELAY,true) .option(ChannelOption.SO_BACKLOG,1024) .option(ChannelOption.SO_KEEPALIVE,true) .channel(NioServerSocketChannel.class) //添加事件处理类，就是reactor中的handler角色 .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoHandler()); &#125; &#125;).bind(port); &#125;&#125; 其中最重要的就是NioEventLoopGroup，因为在这里我们创建了事件的监听和分发器。那什么是NioEventLoopGroup呢？通过词义可以看到词根是EventLoop, 在Js中我们知道Eventloop就是Js的运行机制，是通过单线程循环调度处理事件，这其实也和reactor模式中用单线程监听接受事件非常相似，那再netty中是否就是这个意思呢？让我们先往下看 NioEventLoopGroup因此,现在就让我们扒一扒NioEventLoopGroup的源码，看看里面做了什么。首先进入NioEventLoopGroup，发现它继承了MultithreadEventLoopGroup，先不急看父类，看一下它的构造函数： 1234567891011121314public class NioEventLoopGroup extends MultithreadEventLoopGroup &#123; public NioEventLoopGroup() &#123; this(0); &#125;//...省略中间若干个构造函数//从第一个无参构造函数开始，发现最终到达了这个构造函数public NioEventLoopGroup(...) &#123; super(nThreads, executor, new Object[]&#123;selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject()&#125;); &#125;//...省略其他&#125; 观察一下其中的传入参数,其中大部分参数都被设置了默认值 nThreads ：表示线程池中的线程数，注意这里的线程并非是我们常见的线程，大家先这个thread看成是某个实体，接下来会揭晓这个实体是什么 executor: 传入的线程池实例，这个暂时不表，只要知道是给上述那个实体用的 selectorProvider: 我们需要通过它来实例化 JDK 的 Selector，也就是说那个实体持有selector selectStrategyFactory ：关于selector如何select的策略，之后等揭晓了这个实体之后会做出解答 其他参数 chooserFactory： 选择从线程池中选择线程（那个实体）的策略 rejectedExecutionHandler： 线程池中出现拒绝执行时的策略, netty的默认拒绝策略是抛出异常 接下里我们进入父类查看构造方法 12345private static final int DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt("io.netty.eventLoopThreads", NettyRuntime.availableProcessors() * 2));//看到nThreads如果没有被设置过就被初始化成核心数*2protected MultithreadEventLoopGroup(int nThreads, ThreadFactory threadFactory, Object... args) &#123; super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, threadFactory, args); &#125; 继续进入父类查看 12345678//进入下面的构造方法protected MultithreadEventExecutorGroup(int nThreads, ThreadFactory threadFactory, Object... args) &#123; this(nThreads, (Executor)(threadFactory == null ? null : new ThreadPerTaskExecutor(threadFactory)), args); &#125;protected MultithreadEventExecutorGroup(int nThreads, Executor executor, Object... args) &#123; this(nThreads, executor, DefaultEventExecutorChooserFactory.INSTANCE, args); &#125; 到这里MultithreadEventExecutorGroup会先设置一个ThreadPerTaskExecutor，就是上面所说的executor 再设置一个chooserFactory，用来实现在线程池中选择线程的选择策略，这部分代码比较简单，就不贴出来了 接着this()就会进入正式配置的构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) &#123; //.... //将所有child实例化 for(int i = 0; i &lt; nThreads; ++i) &#123; boolean success = false; boolean var18 = false; try &#123; var18 = true; //注意这里，构造了一个线程池数组 this.children = new EventExecutor[nThreads]; int j; for(int i = 0; i &lt; nThreads; ++i) &#123; boolean success = false; boolean var18 = false; try &#123; var18 = true; //实例化一个child this.children[i] = this.newChild((Executor)executor, args); success = true; var18 = false; &#125; catch (Exception var19) &#123; throw new IllegalStateException("failed to create a child event loop", var19); &#125; finally &#123; //... ////这里指上面实例化child失败，执行下面逻辑 if (!success) &#123; for(j = 0; j &lt; i; ++j) &#123; this.children[j].shutdownGracefully(); &#125; for(j = 0; j &lt; i; ++j) &#123; EventExecutor e = this.children[j]; try &#123; while(!e.isTerminated()) &#123; e.awaitTermination(2147483647L, TimeUnit.SECONDS); &#125; &#125; catch (InterruptedException var22) &#123; Thread.currentThread().interrupt(); break; &#125; &#125; &#125; &#125; //所有child都已经实例化 //设置线程选择策略 this.chooser = chooserFactory.newChooser(this.children); //添加监听器，监听每个线程是否termination FutureListener&lt;Object&gt; terminationListener = new FutureListener&lt;Object&gt;() &#123; public void operationComplete(Future&lt;Object&gt; future) throws Exception &#123; if (MultithreadEventExecutorGroup.this.terminatedChildren.incrementAndGet() == MultithreadEventExecutorGroup.this.children.length) &#123; MultithreadEventExecutorGroup.this.terminationFuture.setSuccess((Object)null); &#125; &#125; &#125;; EventExecutor[] var24 = this.children; j = var24.length; for(int var26 = 0; var26 &lt; j; ++var26) &#123; EventExecutor e = var24[var26]; e.terminationFuture().addListener(terminationListener); &#125; //... &#125; &#125; 接着进入上面的newChild()方法,实际上是进入的NioEventLoop的方法 12345protected EventLoop newChild(Executor executor, Object... args) throws Exception &#123; EventLoopTaskQueueFactory queueFactory = args.length == 4 ? (EventLoopTaskQueueFactory)args[3] : null; //实际上是创建了一个新的NioEventLoop return new NioEventLoop(this, executor, (SelectorProvider)args[0], ((SelectStrategyFactory)args[1]).newSelectStrategy(), (RejectedExecutionHandler)args[2], queueFactory); &#125; 至此可以揭晓谜底了，线程池中的线程，那个实体指的就是NioEventLoop!所以我们整段代码看下来，可以得出结论，NioEventLoopGroup只是一个创建NioEventLoop的实体池，而很多构造方法里的参数最终也是要进入NioEventLoop进行使用。 其实我们看词义和继承关系也能看出个大概，NioEventLoop继承了MultithreadEventLoopGroup，而NioEventLoop继承了SingleThreadEventLoop，也是多和一的关系。 NioEventLoop那接下来我们就要查看NioEventLoop的源码，康康里面究竟卖着什么葫芦药 首先看看它的构造方法 12345678910111213141516171819202122232425262728NioEventLoop(...) &#123; super(parent, executor, false, newTaskQueue(queueFactory), newTaskQueue(queueFactory), rejectedExecutionHandler);//老面孔 this.provider = (SelectorProvider)ObjectUtil.checkNotNull(selectorProvider, "selectorProvider");//老面孔 this.selectStrategy = (SelectStrategy)ObjectUtil.checkNotNull(strategy, "selectStrategy");//selecor，重要的组件 NioEventLoop.SelectorTuple selectorTuple = this.openSelector(); this.selector = selectorTuple.selector; this.unwrappedSelector = selectorTuple.unwrappedSelector;&#125;//看一下父类的构造方法protected SingleThreadEventLoop(...) &#123; super(parent, executor, addTaskWakesUp, taskQueue, rejectedExecutionHandler); this.tailTasks = (Queue)ObjectUtil.checkNotNull(tailTaskQueue, "tailTaskQueue"); &#125;//看一下父类的构造方法 protected SingleThreadEventExecutor(...) &#123; //... //老面孔 this.executor = ThreadExecutorMap.apply(executor, this); //其中最重要的就是这个taskQueue，联想一下js中的Eventloop是不是也有这个东西？ //任务队列，提交给 NioEventLoop 的任务都会进入到这个 taskQueue 中等待被执行 this.taskQueue = (Queue)ObjectUtil.checkNotNull(taskQueue, "taskQueue"); //老面孔 this.rejectedExecutionHandler = (RejectedExecutionHandler)ObjectUtil.checkNotNull(rejectedHandler, "rejectedHandler"); &#125; 发现了很多老面孔了，这些都是直接从外层传递进来的，但是也有一些比较重要的东西，比如selector，这是reactor的重要角色 现在我们其实对NioEventLoop的具体工作流程尚不了解，但是通过对构造函数的分析，相信大家也都明白了这个实体类的重要性。这里做一下总结，接下来会继续分析NioEventLoop的具体工作 从构造上可以看出，NioEventLoop实际上是一个线程池，只不过是一个单线程的线程池，里面持有一个Selector，负责Reactor中最重要的角色 我们利用NioEventLoopGroup构建NioEventLoop，NioEventLoopGroup是一个NioEventLoop池，默认创建2*核心数个NioEventLoop，同时，客户端创建一个NioEventLoopGroup，服务端创建两个NioEventLoopGroup，实际上是多reactor模型，对事件的接受和分发做解耦，这个之后回顾整体架构的时候再细说 执行在netty服务端的demo启动后，代码会执行到NioEventLoop的run方法，让我们直接进入run方法查看eventloop是如何工作的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980protected void run() &#123; int selectCnt = 0; while(true) &#123; while(true) &#123; while(true) &#123; try &#123; int strategy; try &#123; //注意这里，这里的selectStrategy就是我们之前所提到的构造方法里的一个参数 //这里有三个值：-1代表 SELECT -2代表CONTINUE -3代表BUSY_WAIT // strategy = this.selectStrategy.calculateStrategy(this.selectNowSupplier, this.hasTasks()); switch(strategy) &#123; case -3: case -1: long curDeadlineNanos = this.nextScheduledTaskDeadlineNanos(); if (curDeadlineNanos == -1L) &#123; curDeadlineNanos = 9223372036854775807L; &#125; this.nextWakeupNanos.set(curDeadlineNanos); try &#123; if (!this.hasTasks()) &#123; strategy = this.select(curDeadlineNanos); &#125; break; &#125; finally &#123; this.nextWakeupNanos.lazySet(-1L); &#125; case -2: continue; &#125; &#125; catch (IOException var38) &#123; this.rebuildSelector0(); selectCnt = 0; handleLoopException(var38); continue; &#125; ++selectCnt; this.cancelledKeys = 0; this.needsToSelectAgain = false; ///ioRatio 的默认值是50 int ioRatio = this.ioRatio; boolean ranTasks; if (ioRatio == 100) &#123; try &#123; if (strategy &gt; 0) &#123; this.processSelectedKeys(); &#125; &#125; finally &#123; ranTasks = this.runAllTasks(); &#125; &#125; else if (strategy &gt; 0) &#123; long ioStartTime = System.nanoTime(); boolean var26 = false; try &#123; var26 = true; this.processSelectedKeys(); var26 = false; &#125; finally &#123; if (var26) &#123; long ioTime = System.nanoTime() - ioStartTime; this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; &#125; long ioTime = System.nanoTime() - ioStartTime; ranTasks = this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; else &#123; ranTasks = this.runAllTasks(0L); &#125; //... &#125; &#125;&#125; 重点关注其中的一个 switch(strategy)，一个 if (ioRatio == 100) 这里其实主要涉及到三个操作、 select()：轮询，接受注册到reactor线程上的事件 processSelectedKeys()：处理产生网络IO事件的channel runAllTasks()：处理任务队列 这三个操作就是NioEventLoop的核心操作，实际上就包含了channel的接受，分发和处理，下面就分别介绍一下这三个操作 select()截取run方法中的一部分 12345678910111213141516171819switch(strategy) &#123; //这里有三个值：-1代表 SELECT -2代表CONTINUE -3代表BUSY_WAIT case -1: long curDeadlineNanos = this.nextScheduledTaskDeadlineNanos(); if (curDeadlineNanos == -1L) &#123; curDeadlineNanos = 9223372036854775807L; &#125; this.nextWakeupNanos.set(curDeadlineNanos); try &#123; if (!this.hasTasks()) &#123; strategy = this.select(curDeadlineNanos); &#125; break; &#125; finally &#123; this.nextWakeupNanos.lazySet(-1L); &#125; &#125; 这里主要关注select分支，首先要注意的是select是一个阻塞方法，返回值表示多少channel准备就绪，可以进入处理。 看一看select前后处理逻辑，首先设置当前最迟的轮询时间，然后进入select方法 123456789private int select(long deadlineNanos) throws IOException &#123; if (deadlineNanos == 9223372036854775807L) &#123; return this.selector.select(); &#125; else &#123; //设置边界时间为0.995s long timeoutMillis = deadlineToDelayNanos(deadlineNanos + 995000L) / 1000000L; return timeoutMillis &lt;= 0L ? this.selector.selectNow() : this.selector.select(timeoutMillis); &#125; &#125; netty里面定时任务队列是按照延迟时间从小到大进行排序，每次获取任务都是从延迟时间最小的开始获取。select就是通过判断是否有任务正在排队来决定是否进行select 这里，如果时间小于0.995s，表示截止事件时间快到了，那就立刻select一次，调用selectNow()方法。 其中还设置了下一次唤醒时间，如果截止时间到了但是任务还没有被处理，就会自动唤醒 · processSelectedKeys()让我们进入processSelectedKeys()方法看看里面干了什么 12345678910private void processSelectedKeys() &#123; if (this.selectedKeys != null) &#123; //处理优化过的SelectionKey this.processSelectedKeysOptimized(); &#125; else &#123; //处理正常情况的SelectionKey this.processSelectedKeysPlain(this.selector.selectedKeys()); &#125; &#125; 我们知道SelectionKey代表事件返回的对象，里面有感兴趣事件集合，准备就绪的事件集合，channel，selector，attachment等 优化过的SelectionKey和正常的SelectionKey的区别在于netty重写了SelectedSelectionKeySet的selectedKeys属性和publicSelectedKeys属性，使得原来的set变成数组，降低了修改元素的事件复杂度。 看一下processSelectedKeysOptimized()的代码： 123456789101112131415161718192021222324private void processSelectedKeysOptimized() &#123; for(int i = 0; i &lt; this.selectedKeys.size; ++i) &#123; //取出IO事件和channel SelectionKey k = this.selectedKeys.keys[i]; this.selectedKeys.keys[i] = null; //获取selectoinKey中的attachment Object a = k.attachment(); //attachment一般是AbstractNioChannel，存放着具体IO事件 if (a instanceof AbstractNioChannel) &#123; this.processSelectedKey(k, (AbstractNioChannel)a); &#125; else &#123; NioTask&lt;SelectableChannel&gt; task = (NioTask)a; processSelectedKey(k, task); &#125; //部分情况可能出现需要再次轮询的情况 if (this.needsToSelectAgain) &#123; this.selectedKeys.reset(i + 1); this.selectAgain(); i = -1; &#125; &#125;&#125; 什么时候需要再次轮询？ 在channel从selector上移除的时候，调用cancel函数将key取消，并且当被去掉的key到达 CLEANUP_INTERVAL 的时候，设置needsToSelectAgain为true,CLEANUP_INTERVAL默认值为256 因此每满256次会将selectedKeys的内部数组全部清空，然后重新selectAgain重新装载selectionKey runTasks()先看run方法的代码片段 12345678910111213141516171819202122232425262728293031323334 ++selectCnt; this.cancelledKeys = 0; this.needsToSelectAgain = false;//ioRatio是 IO 任务的执行时间比例,如果等于100表示所有时间都被用来处理io任务，默认是50 int ioRatio = this.ioRatio; boolean ranTasks; if (ioRatio == 100) &#123; try &#123; if (strategy &gt; 0) &#123; this.processSelectedKeys(); &#125; &#125; finally &#123; ranTasks = this.runAllTasks(); &#125; &#125; else if (strategy &gt; 0) &#123; long ioStartTime = System.nanoTime(); boolean var26 = false; try &#123; var26 = true; this.processSelectedKeys(); var26 = false; &#125; finally &#123; if (var26) &#123; long ioTime = System.nanoTime() - ioStartTime; this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; &#125; long ioTime = System.nanoTime() - ioStartTime; ranTasks = this.runAllTasks(ioTime * (long)(100 - ioRatio) / (long)ioRatio); &#125; else &#123; ranTasks = this.runAllTasks(0L); &#125; 如果ioRatio为100同时就直接执行IO操作，并最后处理返回的任务 如果ioRatio不是100 就计算io处理限制时间，设置处理返回任务的截止时间，让处理I/O事件的时间和执行任务的时间为1:1。 再看runAllTasks的实现之前，先要明白task究竟是什么。在netty中一共有两种任务，一种是普通任务，一种是定时任务。这里先抛出结论：普通任务存放在taskQueue中，定时任务存放在PriorityQueue中，这部分的代码追踪暂时就不写出来了，让我们首先关注如何去运行致谢tasks吧 进入runAllTasks()看一下: 1234567891011121314151617181920212223242526272829303132333435363738protected boolean runAllTasks(long timeoutNanos) &#123; //等会讲 this.fetchFromScheduledTaskQueue(); //从队列中取出任务 Runnable task = this.pollTask(); if (task == null) &#123; this.afterRunningAllTasks(); return false; &#125; else &#123; long deadline = timeoutNanos &gt; 0L ? ScheduledFutureTask.nanoTime() + timeoutNanos : 0L; long runTasks = 0L; long lastExecutionTime; while(true) &#123; //处理任务，这里的task其实就是一个Runnable类，方法里直接run就行了 safeExecute(task); //记录处理完的任务数 ++runTasks; //每64个任务才检查一次timeout，因为nanoTime()开销比较大 if ((runTasks &amp; 63L) == 0L) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime &gt;= deadline) &#123; break; &#125; &#125; //继续处理下一个任务 task = this.pollTask(); if (task == null) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); break; &#125; &#125; this.afterRunningAllTasks(); this.lastExecutionTime = lastExecutionTime; return true; &#125; &#125; 整体逻辑还算简单，主要先看看fetchFromScheduledTaskQueue()里面做了什么 1234567891011121314151617181920private boolean fetchFromScheduledTaskQueue() &#123; if (this.scheduledTaskQueue != null &amp;&amp; !this.scheduledTaskQueue.isEmpty()) &#123; long nanoTime = AbstractScheduledEventExecutor.nanoTime(); Runnable scheduledTask; do &#123; //在定时任务队列中获取一个离截止时间最近的任务 scheduledTask = this.pollScheduledTask(nanoTime); if (scheduledTask == null) &#123; return true; &#125;//添加普通队列中去 &#125; while(this.taskQueue.offer(scheduledTask)); //如果添加失败，就归还定时任务 this.scheduledTaskQueue.add((ScheduledFutureTask)scheduledTask); return false; &#125; else &#123; return true; &#125; &#125; 之前说过task包含普通任务和定时任务，分别处在不同的队列中。可见这个函数的主要作用就是将定时任务添加到普通队列中去，这样后面处理任务的时候就可以统一一个队列中获取。 为什么是放在普通队列中？因为定时队列是按照时间优先级的顺序排列，普通任务也无法正常的插入定时任务队列，反之快到截止时间的定时任务是可以看作是普通任务来处理的。 到这里runTasks的逻辑也基本讲完了。总结一下主要干了这些事 协调IO时间和处理任务的时间，计算处理任务需要的deadline截止时间 聚合定时任务和普通任务，循环取出任务执行 每执行64次任务就检查一次截止时间，到期退出循环 在afterRunningAllTasks()中，做一些收尾动作 总结至此，EventLoop上的所有任务都开始执行了，整个流程也结束了。· 最后让我们看看reactor模式及对应下各个角色的具体实现，让我们再看一眼 Reactor线程模型的主要实现实体：NioEventLoop client： 请求的事件，可以理解是一个channel acceptor： selector，主要指selector的select()负责的监听并接受事件 mainReactor和sunReactor：在服务端会绑定两个NioEventLoopGroup：parentGroup和childGroup，一般在parentGroup中处理accept事件，再childGroup中处理其他事件。IO事件的处理主要是在processSelectedKeys()函数里，判断SelectionKey的类型，并交给不同的函数处理。 queued tasks:当建立了连接后，待处理任务就会进入tasks queue，等到再runTasks()中循环队列处理，每个任务对应从threadpool中取出一条worker thread进行处理 参考博客【NIO系列】——之Reactor模型 Java NIO浅析-美团技术团队]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy爬虫入门]]></title>
    <url>%2F2020%2F05%2F12%2Fscrapy1%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-uPhkWXAt"), narrow: false, autoplay: false, showlrc: false, music: { title: "Something I Need", author: "OneRepublic", url: "http://music.163.com/song/media/outer/url?id=26060071.mp3", pic: "http://p2.music.126.net/b2nl6jsVbqj23IV8dVvJcg==/7766950139663735.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 现在说到爬虫，大家都会或多或少地将python和爬虫联系在一起，归根到底，是因为python丰富的生态和灵活简单的语法。同时基于python存在有几个强大的爬虫框架，极大地降低了爬虫的难度，提高了编写程序的效率。因此，今天就让我们初探其中一个强大的python框架——scrapy，体验一下使用框架爬虫的快感吧 简介 Scrapy是一个用于爬网网站和提取结构化数据的应用程序框架，可用于各种有用的应用程序，例如数据挖掘，信息处理或历史档案。 ——翻译自官网 推荐直接查看官网，里面甚至有完整的scraoy教程，简直是深入学习框架的必备选择！ 安装在安装scrapy框架的时候，网上有许多方法，针对python的环境不同，可能存在奇奇怪怪的错误，这里我基于python3环境，预装了pip，亲测一遍过 预装环境 Windows 10 + python 3.7.0 + pip20.1 +virtualenv 同时请开启一个新的virtual环境来保证不会出现其他包的依赖冲突 前期安装组件 lxml pyOpenSSL Twisted PyWin32 安装lxml 直接pip安装即可。这是python一个HTML、XML解析库，即使不用框架也是经常使用的 1pip3 install lxml 安装PyWin32 官网下载对应版本的安装包双击安装即可 [pywin32]([https://sourceforge.net/projects/pywin32/files/pywin32/Build%20221/](https://sourceforge.net/projects/pywin32/files/pywin32/Build 221/)) 安装剩余组件 这里首先要介绍wheel wheel是python的一个打包格式，以前python主流的打包格式是.egg文件，但现在*.whl文件也变得流行起来。 wheel其实上是python上一种压缩打包的组件，有点类似于zip之类的，但在本文中你只要知道通过wheel文件格式文件你可以快速将一个库安装到你的python环境中 安装其实也很简单 1pip3 install wheel 这样你的python环境就支持.whl文件格式的安装啦 接下来的步骤就是到各个官网上下载各组件的whl格式，注意要和你的python环境匹配 pyOpenSSL 安装 1pip3 install pyOpenSSL-19.1.0-py2.py3-none-any.whl Twisted注意要和你的python版本对应 像我的环境就是 1pip3 install Twisted-20.3.0-cp37-cp37m-win_amd64.whl 安装scrapy 所有依赖包安装成功后直接pip安装scrapy就不会有问题啦 1pip3 install Scrapy 组件介绍首先创建项目 在你想要放置爬虫项目的文件夹运行，xxx就是你的项目名 1scrapy startproject xxx 顺便记录一下一些基本的操作 创建项目：scrapy startproject xxx 进入项目：cd xxx #进入某个文件夹下 创建爬虫：scrapy genspider xxx（爬虫名） xxx.com （爬取域） 生成文件：scrapy crawl xxx -o xxx.json (生成某种类型的文件) 运行爬虫：scrapy crawl XXX 列出所有爬虫：scrapy list 获得配置信息：scrapy settings [options] 创建完成后你可以看到文件夹下多了这些内容 让我们一个个介绍这些组件（spider_demo是你的爬虫项目名） scrapy.cfg: 项目的配置文件(在项目文件夹的平行目录下) spider_demo/spiders/: 放置spider代码的目录. （放爬虫的地方）也是你放爬虫具体逻辑的地方 spider_demo/items.py: 项目中的item文件.（创建容器的地方，也是定义最终爬虫得到的结果格式的地方） spider_demo/pipelines.py: 项目中的pipelines文件.（实现数据的清洗、存储和验证） spider_demo/settings.py: 项目的设置文件.（爬虫的具体配置，包括启动某个中间件，启动关闭某个功能等） spider_demo/middlewares.py:定义项目的下载器中间件和爬虫中间件 感觉是不是还有点蒙圈？接下来简单介绍一下scrapy运行的原理，这样相信就能更理解这些组件的作用了 官网的流程图 Scrapy是由执行引擎控制执行的 Spider发起请求给Engine Engine安排请求Scheduler和接受下一个爬取请求 Scheduler返回下一个请求 Engine将请求通过Downloader Middlewares发送给Downloader Downloader爬取网页并将返回结果通过Downloader Middlewares发送回Engine 引擎接受响应并通过Spider Middleware转发给Spider处理 Spider的parse()方法对获取到的response进行处理，解析出items或者请求，将解析出来的items或请求，返回给Engine Engine将items发送到Item Pipline,将请求发送到Scheduler 重复步骤1直到没有新的请求 总结一下上面步骤出现的组件 组件名 组件功能 Engine 框架核心，负责整体的数据和信号的调度 框架实现 Scheduler 一个存放请求的队列 框架实现 Downloader 执行具体下载任务的单元 框架实现 Spider 处理下载得到的响应结果，提取需要的是数据（具体的业务逻辑） 自己实现 Item Pipline 处理最终得到的数据，如进行持久化操作 自己实现 Downloader MIddlewares 在正式进行下载任务之前，可以进行一些自定义处理。比如设置请求头，设置代理 自己实现 Spider Midderwares 自定义请求和过滤响应 自己实现 相信这一套组合拳下来应该能对这个框架有了基本的认识，接下来就通过实战来强化一下记忆吧 简单应用这次是根据网上通过爬取猫眼电影的排行榜做的一个demo，以后有时间再换一个更加复杂的demo 实现目标是爬取电影排行榜上的片名、分数和排名，同时将结果以json的格式保存在一个.json文件中 首先你要确定你需要爬取哪些数据，将你需要的数据记录到容器中，在item.py中进行编写： 1234567891011import scrapy#这里我们需要排名、标题、收藏人数、上映时间和分数class SpiderDemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() index = scrapy.Field() title = scrapy.Field() star = scrapy.Field() releasetime = scrapy.Field() score = scrapy.Field() 接下来在Spiders文件夹下新建一个爬虫文件，例如我新建了一个MoyanSpider.py文件 12345678910111213141516171819202122232425262728293031323334import scrapyfrom spider_demo.items import SpiderDemoItemclass MaoyanSpider(scrapy.Spider): #这是爬虫启动的名称，之后启动爬虫就需要用到这个名称 name = "maoyan" #可以爬取的域名可选列表 allowed_domains = ["maoyan.com"] #目标爬取的网址 start_urls = [ "http://maoyan.com/board/7/", "http://maoyan.com/board/4/", ] #处理已经下载的页面 def parse(self, response): dl = response.css(".board-wrapper dd") #通过解析得到具体的数据存到容器中 for dd in dl: item = SpiderDemoItem() item["index"] = dd.css(".board-index::text").extract_first() item["title"] = dd.css(".name a::text").extract_first() item["star"] = dd.css(".star::text").extract_first() item["releasetime"] = dd.css(".releasetime::text").extract_first() score = dd.css('.integer::text').extract_first() if score is not None: item["score"] = score + dd.css('.fraction::text').extract_first() else: item["score"] = 0 #通过yield将结果返回 yield item 这里提一下，scrapy支持各种类型的解析，你可以使用python常见的三大解析库进行解析，但框架也提供了一种自己的解析方式（Selector） 选择器 Xpath 这里不详细叙述，以后有时间再细聊 同时我们需要在setting,py中对配置稍微进行修改(setting.py中有许多默认配置，这里只展示修改的部分) 12345#如果没有自动生成UA，就需要手动定义，但是每次爬取都是同样的UA容易出现验证操作，因此后面还会介绍一种随机生成UA的方法USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'#允许机器人协议，关于机器人协议的具体内容可以自行上网查找ROBOTSTXT_OBEY = False 这样，基本就完成了一个简单的爬虫，只需要执行scrapy crawl maoyan(最后一个是你的爬虫名) 之后还有两个要点，一个是项目的持久化，一个是随机化User-Agent。 先看持久化，这里简单起见就示范将爬虫数据以json格式导出 这里需要修改pipline.py，至于为什么，相信看了之前的组件介绍应该能明白 1234567891011121314151617181920212223import jsonimport codecsclass SpiderDemoPipeline: def process_item(self, item, spider): return itemclass JsonPipline(object): def __init__(self): print("打开文件，准备写入....") self.file = codecs.open("maoyan.json", "wb", encoding='utf-8') def process_item(self, item, spider): print("准备写入...") line = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(line) return item def close_spider(self, spider): print("写入完毕，关闭文件") self.file.close 然后在setting.py中开启自定义的pipline 1234ITEM_PIPELINES = &#123; # &apos;spider_demo.pipelines.SpiderDemoPipeline&apos;: 300, &apos;spider_demo.pipelines.JsonPipline&apos;: 200&#125; 至于随机化UA，先说明添加UA的原理 scrapy首先会读取setting.py里面关于UA的设置，然后经过middleware，如果没有进行自定义操作，就会将配置中的UA添加到请求头中。因此，想要实现随机化UA,实际上就可以在发起网页请求之前，在Download Middleware上做文章。 这里在middleware.py上做了修改,引入了第三方包fake_useragent 123456789101112131415161718192021from scrapy import signalsimport randomfrom fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): # 随机更换user-agent def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 同时在setting.py上做修改 123456789DOWNLOADER_MIDDLEWARES = &#123; 'spider_demo.middlewares.SpiderDemoDownloaderMiddleware': 543, 'spider_demo.middlewares.RandomUserAgentMiddleware': 400, 'scrapy.downloadermiddleware.useragent.UserAgentMiddleware': None,&#125;# 随机选择UA#这个是自己设置的，依赖于fake-useragentRANDOM_UA_TYPE = 'random' 至此，一个简单的爬虫应用就实现了！ 可以看见UA发生了变化 同时生成了一个maoyan.json文件]]></content>
      <tags>
        <tag>python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[春招面经小记]]></title>
    <url>%2F2020%2F05%2F12%2Finterview%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-PESXheQB"), narrow: false, autoplay: false, showlrc: false, music: { title: "追梦赤子心", author: "GALA", url: "http://music.163.com/song/media/outer/url?id=355992.mp3", pic: "http://p2.music.126.net/XDncptlBJ4_LN3hLBx-8aw==/19061133579343591.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 从3月开始正式投递，到现在，大大小小经历了十几场笔试，来来回回也算经历了不少的面试，从一开始的踌躇满志到慢慢开始接受现实，这两个月来的经历确实让我开始认清自己，在梦想和现实中慢慢找寻平衡。 深感自己能力的不足，但是又不甘就这样停歇，既然决定将此作为毕生的事业，那就不要抱怨，一心一意，不断向前吧。 分享这几个月的面试经验 腾讯一开始投提前批的时候期望挺高，结果投了几家公司都杳无音讯，倒是腾讯成为了我的首家面试公司。第一次面试感觉确实很紧张，虽然对腾讯的面试重点有所了解，但是准备依然不充分。面试官还是非常和蔼，这让我慢慢放松了下来，但是最后还是败在了算法题上面 一面计算机网络 IP报文头有哪些？ 当时没有提前准备，凭着很早的记忆勉强说了出来 ttl的作用 生命周期，标记IP数据包转发的最大跳数，防止在计算机网络上无休止地循环 拥塞控制 https怎么优化（这个确实比较懵） 看了一下博客，大致总结这些： False Start：客户端在发送 Change Cipher Spec Finished 同时发送应用数据（如 HTTP 请求），服务端在 TLS 握手完成时直接返回应用数据（如 HTTP 响应）。这样，应用数据的发送实际上并未等到握手全部完成，故谓之抢跑· 启用TLS False Start可以节省一个RTT时间，但是客户端和服务端都需要支持NPN/ALPN，需要采用支持前向保密的密码套件，即使用ECDHE进行密钥交换。 减少证书大小，例如 ECC（Elliptic Curve Cryptography，椭圆曲线密码学）证书 Session Resumption 会话复用：将第一次握手辛辛苦苦算出来的对称密钥存起来，后续请求中直接使用。这样可以节省证书传送等开销，也可以将 TLS 握手所需 RTT 减少到一个。 实现方法：利用sessioinID 判别客户端身份，Session Ticket 是用只有服务端知道的安全密钥加密过的会话信息，最终保存在浏览器端。浏览器如果在 ClientHello 时带上了 Session Ticket，只要服务器能成功解密就可以完成快速握手。 选用合适的加密算法 实现一个TCP服务器（一开始不懂具体要问什么知识点，面试官再补充了一下才大致明白应该是考察IO、socket这些的…）要说出连接socket建立服务端客户端的具体过程规则： 正等待连接请求的一端有一个固定长度的连接队列，该队列中的连接已被TCP接受（完成三次握手），但还没有被应用层接受。注意：TCP接受一个连接是将其放入这个队列，而应用层接受连接是将其从该队列中移出。也就是使用*\.accept()函数来阻塞等待客户的请求**。 应用层指明该队列的最大长度，这个值被称为“积压值(backlog)”，范围为0~5。也就是使用*\.listen(5)函数来指定的队列长度。通常取值5** 当一个连接请求到达时，TCP使用一个算法，根据当前连接队列中的连接数来确定是否接收这个连接。这不同于积压值，积压值说明的是TCP监听的端点已被TCP接受而等待应用层接受的最大连接数。这个积压值对系统所允许的最大连接数，或者并发服务器所能并发处理的客户数，并无影响。 如果对于新的连接请求，该TCP监听的端点的连接队列中还有空间，TCP模块将对SYN进行确认并完成连接的建立。但应用层只有在三次握手中的第三个报文段收到后才会知道这个新连接。另外，当客户进程的主动打开成功但服务器的应用层还不知道这个新连接时，它可能会认为服务器进程已经准备好接受数据了。此后，如果客户程序发送数据，服务器的TCP模块会将接收的数据放入缓存队列。 如果对于新的连接请求，连接队列中已没有空间，TCP将不理会收到的SYN，也不发回任何报文段(包括RST)，这是一个软错误，而不是一个硬错误。如果应用层不能及时接受.listen(5)中的连接，达到积压值(5)，客户的主动打开最终也将超时 nginx负载均衡的策略（当时真的没有复习到这..） weight权重判断 时间片轮转 fair根据响应时间 ip hash url hash Linux 进程间通信的方式，最常使用的有哪些？ 进程的内存结构，堆栈的区别 管道的注意事项 如何避免死锁 避免多次锁定。尽量避免同一个线程对多个 Lock进行锁定。例如上面的死锁程序，主线程要对 A、B 两个对象的 Lock进行锁定，副线程也要对 A、B 两个对象的 Lock进行锁定，这就埋下了导致死锁的隐患。 具有相同的加锁顺序。如果多个线程需要对多个 Lock进行锁定，则应该保证它们以相同的顺序请求加锁。比如上面的死锁程序，主线程先对 A 对象的 Lock加锁，再对 B 对象的 Lock加锁；而副线程则先对 B 对象的 Lock加锁，再对 A 对象的 Lock加锁。这种加锁顺序很容易形成嵌套锁定，进而导致死锁。如果让主线程、副线程按照相同的顺序加锁，就可以避免这个问题。 使用定时锁。程序在调用 acquire() 方法加锁时可指定 timeout参数，该参数指定超过 timeout秒后会自动释放对 Lock的锁定，这样就可以解开死锁了。 死锁检测。死锁检测是一种依靠算法机制来实现的死锁预防机制，它主要是针对那些不可能实现按序加锁，也不能使用定时锁的场景的。 java hashmap的内部结构，介绍了hash算法、rehash机制 数据库 innodb和myISam的区别 redis的数据结构，跳表的实现 随机化层数的算法 完全二叉树的深度 数据结构 归并排序的时间复杂度 深度优先遍历和广度优先遍历使用的数据结构（栈） topK问题 场景题 两个文件存储qq号，如何去重 bitMap+ 布隆过滤器 分布式ID，最节约内存的方法 如果数据库存分布式ID，如何保证分布式ID的唯一性，（数据库同步） 唯一索引自增 redis自增 UUID 雪花算法 代码题 两个树，用数组表示，结合成一个有序数组 做的很勉强，其实不用想太多，直接树化，中序遍历 二面根据一面的情况，自己也感觉应该是挂了 ，但居然还有二面。虽说是二面，实际上感觉就是在刷kpi，甚至没有给代码题，问的问题都很敷衍，感觉就是根据题库随机抽题问，面完后反手就给挂。，真不理解这样的面试意义何在，从面试者的角度这样从一开始就没有结果的面试就是在挥霍我们的期待，有点败坏好感。 计算机网络 IP报文头 传输层报文头 为什么出现IP报文 一个文本怎么从一个主机传到另一个主机 linux 基础命令 虚拟地址空间 c文件到可执行文件的编译过程 动态链接和静态链接 系统库和标准库的区别 一个是商家自行开发，一个是语言标准的函数库 数据库 查询语句 创建索引的sql语句 1CREATE INDEX index_name web开发 你了解过一些安全方面的东西吗 怎么调试BUG 情景题 黑帽子白帽子，至少一人戴黑帽子，如果自己是黑帽子则鼓掌，三次开关灯后才有人鼓掌，问黑帽子几顶（想了好久才想出来的，感觉有点博弈论的感觉） 三顶。如果是一个人戴黑的,第一次熄灯他发现所有人都是白帽,所以他必然黑帽；可是没有人鼓掌,证明所有人看到其他人有戴黑帽的,如果是两个人,那么第二次熄灯那两个人会一起鼓掌,因为他们知道肯定不只一个黑帽子,可自己只看到一个黑帽子,所以肯定自己也是黑帽子,以此类推,结果就是,熄几次灯几个黑帽子 两城市距离s，两列车AB相向而行，一无人机速度c，快于AB，飞到AB后返回，问最后AB相遇的时候无人机飞的距离 AB相向而行,距离为s,速度就是a+b,时间就为s/（a+b)鸽子的速度为c,它直到AB相遇消耗的时间为s/（a+b)那么它所飞行的路程=速度*时间=c*[s/（a+b)]=cs/(a+b) 5月15日更。 正式批明明没有做笔试，结果居然还有电话面。。这个时候我已经没有在准备面试了，所以答得不是很好。。这次的面试官就没有上一次那么亲切了，有点咄咄逼人，不管怎样还是自己技术太菜了。 正式批电话面 UDP和TCP java怎么实现的？ 现在for循环10词连接UDP，端口号是否会释放(SQL连接是否会释放) 如何实现共享内存？（内存淘汰机制，线程安全性） 大文件存放字符串，求出现次数最多的TOP5 一张表，时间，用户ID 奖品ID 班级 求 XX时间段中获奖次数最多的学生ID和领取次数最多的奖品ID 1SELECT stuId,count(*) as count FROM (SELECT * FROM tab WHERE time &lt; xxx AND time &gt; xxx) GROUP BY stuid ORDER BY count(*) LIMIT 5 蘑菇街蘑菇街是我面试了才知道的公司，确实公司现在的运营也不是很好，但不得不说还是一家标准的互联网公司，在后面也通过一些博客专栏作者比如敖丙了解到了公司的内部情况和未来的发展，不得不说对我的准备和未来规划还是起到了一定的影响。最后终止于二面（话说刚看到公司裁员就收到了感谢信…)倒也没什么遗憾的。 一面一面问基础问的有点偏。。 Java基础 swap(Integer a,Ingteger b)如何交换 使用数组传参（这道题不允许） 使用自定义类，set、get（这道题不允许） 由于java中对于非原生类型都说传递引用，而对于原生数据类型都说通过传递值的方式传递参数的，如果仅仅通过传递值的方式来交换a,b的值，在Java中是不可能实现的 Integer虽然不是原生类型，但是jvm对这种类型添加了缓冲池，每次 Java 交换两个变量的数值实现方法 这种方法理论上可行，但是实际操作行不通 Mybatis使用原理 内部原理 ${} 和 #{}的区别 Spring 事务 mysql 事务的实现原理 在事物进行过程中，未结束之前，DML语句是不会更改底层数据，只是将历史操作记录一下，在内存中完成记录。只有在事物结束的时候，而且是成功的结束的时候，才会修改底层硬盘文件中的数据 mysql任意一条DML语句代表事务的开启，同时默认自动提交事务，如果要开启回滚，需要手动提交事务。 事务的提交和回滚是如何实现的 begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。 已知索引A(a,b,c) 现在有sql语句：where b &gt; 1 and a = 1and c =1 请问是否能应用索引？ 现在有sql语句：where a = 1 and b &gt; 1 and c =1 请问是否能应用索引？ 考的是最左匹配原则 sql语句优化的过程 项目架构 介绍一下项目 多线程技术的考量 消息队列 如何防止消息被重复消费 如何防止消息被重复生产 仅考虑项目出现重复消息推送，可以加同步锁 Kafka的消费机制 这个算给自己挖坑的。。明明不太熟悉还要提233 总结 基础知识掌握不牢，尤其是索引这块具体描述的时候无法清楚描述 讲解项目的时候给自己挖坑了 情景题的临场发挥能力，一些基础的讲述能力，MVCC完全没有讲好 二面过了好久的二面。。 问了项目应用 jwt的原理 如何保证数据的一致性 结合应用场景谈谈redis如何保障数据一致性 我还是没答上来，注意这和重复提交、分布式锁是不同的 文件上传为什么不能放在本地（结合无状态考虑） FTP协议上传文件 文件存放在本地的话无法通过用户登录知道用户信息（参照JWT） 线上出现数据缓存不一致问题怎么解决？ 分布式文件系统：原理、问题与方法 作业帮作业帮面试体验还行吧，看的出来问的问题不是很深，和大厂果然没法比，最后收了offer，但是拒了 一面项目架构 简单讲讲你的项目架构、内容、负责部分 怎么实现的爬取 怎么实现的订阅服务 数据库 redis的数据结构 redis的分布式锁实现 redis各种数据结构的应用场景 redis自增如何保证原子性 redis是单线程的 mysql事务、ACID B+树的实现机制 web方面 怎么理解restful设计规范 代码 回型打印二维数组 二面 项目 分布式事务 分布式事务了解过吗？ 分布式事务的特点 二段式提交 三段式提交 计算机基础 …忘记了，比较常规 算法 排序的时间复杂度 递归的时间复杂度 OPPO技术面真不知道oppo光靠一面技术面还没有代码是怎么选人的。。 数组和链表的底层区别 数组的查询和链表的内存查询有什么不同 在操作系统内存管理方面也有不同。正因为数组与链表的物理存储结构不同，在内存预读方面，内存管理会将连续的存储空间提前读入缓存（局部性原理），所以数组往往会被都读入到缓存中，这样进一步提高了访问的效率，而链表由于在内存中分布是分散的，往往不会都读入到缓存中，这样本来访问效率就低，这样效率反而更低了。在实际应用中，因为链表带来的动态扩容的便利性，在做为算法的容器方面，用的更普遍一点。 mysql 线程是异步的吗 部分是部分不是 IO线程异步的原因是并行执行，同时可以刷新邻页 异步通知会出现什么问题 如何保证断电后数据恢复 数据一致性问题 在MySQL5.5以及之前， slave的 SQL线程执行的relay log 的位置只能保存在文件（relay-log.info）里面，并且该文件默认每执行 10000次事务做一次同步到磁盘， 这意味着 slave意外 crash重启时， SQL 线程执行到的位置和数据库的数据是不一致的，将导致复制报错，如果不重搭复制，则有可能会导致数据不一致。 MySQL 5.6 引入参数relay_log_info_repository，将该参数设置为 TABLE时， MySQL 将 SQL 线程执行到的位置存到mysql.slave_relay_log_info 表，这样更新该表的位置和 SQL 线程执行的用户事务绑定成一个事务，这样 slave 意外宕机后， slave 通过 innodb 的崩溃恢复可以把 SQL 线程执行到的位置和用户事务恢复到一致性的状态。 MySQL 5.6引入 GTID复制，每个 GTID对应的事务在每个实例上面最多执行一次， 这极大地提高了复制的数据一致性； MySQL 5.5引入半同步复制， 用户安装半同步复制插件并且开启参数后，设置超时时间，可保证在超时时间内如果 binlog不传到 slave 上面，那么用户提交事务时不会返回，直到超时后切成异步复制，但是如果切成异步之前用户线程提交时在 master 上面等待的时候，事务已经提交，该事务对 master上面的其他 session是可见的，如果这时 master 宕机，那么到 slave 上面该事务又不可见了，该问题直到 5.7 才解决； MySQL 5.7 引入无损半同步复制，引入参 rpl_semi_sync_master_wait_point，该参数默认为 after_sync，指的是在切成半同步之前，事务不提交，而是接收到 slave 的 ACK 确认之后才提交该事务，从此，复制真正可以做到无损的了。 延时性· 5.5 是单线程复制， 5.6 是多库复制（对于单库或者单表的并发操作是没用的）， 5.7 是真正意义的多线程复制，它的原理是基于group commit， 只要master 上面的事务是 group commit的，那 slave 上面也可以通过多个 worker线程去并发执行。 和 MairaDB10.0.0.5 引入多线程复制的原理基本一样。 1mysqldump用于备份，记录一下 redis 锁提前释放了怎么办 主从同步怎么防止锁不丢失 redlock 实现分布式锁的一个非常重要的点就是set的value要具有唯一性，redisson的value是怎样保证value的唯一性呢?答案是UUID+threadId 如果锁丢失了怎么解决 epoll和poll 多线程 volatile和锁的区别 多线程中线程出现异常会怎样 线程代码中产生异常的话，那么这个线程的生命周期就结束了，这种情况称为线程泄漏。 注意，只是这个线程的生命结束了，但其他的线程还是活着的。如果是代码有 BUG，那其他线程走到这一块估计也会挂掉的。 FileStream 阿里电话面阿里确实要求很高，流程也很长，自己面的时候总归有点放弃的感觉。但是从电话面中我确实体验到了什么叫深挖项目，几乎对每一个细节都进行了深挖，只要自己稍微给自己挖坑就凉凉了。。这也算得到了一种经验吧 心得：主要在于对待架构的需求，为什么需要这样做 做项目的时候遇到什么样的问题？答：跨域 跨域问题怎么出现的协议 + 域名+ 端口 解决方法：nginx或者请求头添加允许跨域标识 分布式锁。单机为什么要用分布式锁？ 多个请求同时插入如何保证幂等性 不用redis防止重复请求数据库 幂等（idempotent、idempotence） 在编程中.一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“getUsername()和setTrue()”函数就是一个幂等函数. 高并发的核心技术-幂等的实现方案 分布式系统后台如何防止重复提交 如何防止重复提交 redis+计数器，大于1表示重复，问题就转换成如何生成唯一ID的情景 mysql的唯一索引，出现报错就处理返回 token机制：防止页面重复提交。 原理上通过session token来实现的(也可以通过redis来实现)。当客户端请求页面时，服务器会生成一个随机数Token，并且将Token放置到session当中，然后将Token发给客户端（一般通过构造hidden表单）。下次客户端提交请求时，Token会随着表单一起提交到服务器端。 服务器端第一次验证相同过后，会将session中的Token值更新下，若用户重复提交，第二次的验证判断将失败，因为用户提交的表单中的Token没变，但服务器端session中Token已经改变了。 字节一面字节确实面的很紧张。一来是因为字节的机遇确实大，只要搏一搏就有很大的希望进去，但另一方面，算法又是我的薄弱项，于是自己总是在不断的准备，犹豫，造成了直到面试也是出现了比以往还有激烈的兴趣反应。这就是自己的心态管理还不到位的缘故。最后期望越大，失望越大。 项目 分布式锁实现 数据库 mysql的事务特性和隔离级别 多线程 同步异步、阻塞非阻塞 git和maven git怎么修改已提交的文件 maven的多模块是什么 设计模式 知道的设计模式 单例模式的写法(败笔！绝对的败笔，可见自己平时没有完全理解，一直以为自己能写出来结果在现场却掉链子了) 编程题 二叉树求和 回溯 智力题 单极管 两两触碰能指示对方的好坏，好的能指示正确结果，坏的指示结果不确定，现在好的数量大于n/2，问怎么将好的和坏的区分出来？时间复杂度多少？ 只要 和所有比对，好的大于1/2就可以确定他是好的 美团一面美团是我期望最大的公司，我也为此做出了自以为最充足的准备。但是最后依然惨淡收场。虽然算法题没有写出来是最大的问题，但是这次给我最大的感悟是面试官在面试中指出了我的性格问题，这是第一次我被指出技术以外的问题，这个问题也是我一直不自知的。那就是我在面试中体现出来的强答，自以为是在赌运，但在面试官面前，不懂装懂是最大的无知。 知之为知之，不知为不知，是智也。 这次面试是值得的。 hashmap负载因子？（这个我那时候想错了，是我的失误，但在面试官看来只是我的不懂装懂，因此给他留下了不好的印象） B+树和红黑树的区别 红黑树性能稳定但为了维持稳定需要更多开销，不适合实现快速查找的功能 算法题 倒水问题： 10 7 3 头条一面字节过后的头条，这时的我对面试已经疲倦了，整体的面试状态非常不好，自然没有过 linux 保持后台运行的命令 把程序放后台运行，简单的话，只要在命令后面加一个“&amp;”， 如：php test.php &amp; 或者在运行命令后，按一下 Ctrl+Z，如运行php test.php后，按一下Ctrl+Z 程序在后台运行了，但还是看到输出信息，可以用管道命令把输出定向到/dev/null，如：php test.php &gt;/dev/null 普通的输出信息看不到了，但还是看到一些信息，如错误信息等，需要再添加2&gt;&amp;1命令，如：php test.php &gt;/dev/null 2&gt;&amp;1 程序在后台运行了，但退出当前会话，发现程序还是停止了，此时要用nohup命令，如：nohup php test.php &gt;/dev/null 2&gt;&amp;1 使用nohup后，应确保用exit命令退出当前账户，非常正常退出或结束当前会话，在后台运行的作业也会终止 命令在后台运行了，怎么查看？使用jobs命令可列出当前会话的后台任务，jobs -l 能查看到 PID，进而可以用kill终止某个任务 是终命令可能是：nohup php test.php &gt;/dev/null 2&gt;&amp;1 &amp; linux查看cpu使用率的命令 java 查看CPU使用参数的命令 mysql索引查询 算法 平方根 跳台阶 动态规划！！ CVTE电话面主要是数据库设计相关有点新意 京东电话面搞突袭，都5月了才开始，着实有点尴尬 自动装箱拆箱 int存放在哪里 常量池 基本数据类型l 创建线程有哪几种方法]]></content>
      <tags>
        <tag>面试</tag>
        <tag>感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大道至简——virtualenv介绍]]></title>
    <url>%2F2020%2F05%2F09%2Fvirtualenv%2F</url>
    <content type="text"><![CDATA[python因为丰富的扩展库被大家所青睐，但是当开发环境中的第三方包越来越多的时候，基于该开发环境开发的应用越来越难以移植、迁移。像在操作系统中我们可以用容器（例如docker）将开发环境和生产环境分开，在做python的应用开发的时候，我们同样需要给每个应用提供一个隔离的运行环境，从而避免增加不必要的版本冲突和排查成本。 virtualenv就是干这行的。 安装安装其实非常简单。 首先你必须安装好pip(装python的时候没有人不装吧？) 然后输入命令 1pip3 install virtualenv 等待安装完成就大功告成了！ 注：Python3 已经内置了 venv 模块,实际上不需要手动安装即可。查看是否安装 1python3 -m venv --help 基本操作创建虚拟环境只要在自己希望创建虚拟环境的位置，输入 1virtualenv test 等待一段时间，即可创建一个名称为“test”的虚拟环境，以后，在这个虚拟环境下下载第三方的包都会被放在这个文件夹下面。 下图表示创建成功： 然后你的虚拟环境的所有配置都存放在test文件夹中， 默认的目录结构如下（Windows 10 环境下） 123456- test - Inculde - Lib - Script - tcl - LICENCE.txt 旧版命令创建的虚拟环境会默认继承实际环境存在的第三方包，如果需要创建一个干净的虚拟环境 输入：virtualenv --no-site-packages test 新版默认创建无第三方包的纯净环境 进入虚拟环境windows系统进入Script文件夹，执行activate.bat文件 linux执行: 1source test/bin/activate test是你创建的虚拟环境所在的文件夹 当命令行前面出现虚拟环境名说明已经切换到虚拟环境中，之后你安装的第三方依赖包都会下载到这个虚拟环境中，从而不影响本机实际的python运行环境。实际上也是将这些依赖下载到这个虚拟环境所在的文件夹中 退出虚拟环境windows输入 linux系统执行 1source test/bin/deactive 交接环境这个做法比较常见，出现在你需要将自己的开发环境转移到另外一台电脑或移交给某人的时候，需要将自己的虚拟环境拷贝到另外的地方 做法是 先冻结环境包，将包的版本信息保存在一个文本文件中 1$ pip freeze &gt; requirements.txt 在另外一个环境中直接下载所有依赖 1$ pip install -r requirements.txt 其他命令用法： 1$ virtualenv [OPTIONS] DEST_DIR 选项: 12345678910111213141516171819202122232425262728–version #显示当前版本号。 -h, –help #显示帮助信息。 -v, –verbose #显示详细信息。 -q, –quiet #不显示详细信息。 -p PYTHON_EXE, –python=PYTHON_EXE #指定所用的python解析器的版本，比如 –python=python2.5 就使用2.5版本的解析器创建新的隔离环境。 默认使用的是当前系统安装(/usr/bin/python)的python解析器 –clear #清空非root用户的安装，并重头开始创建隔离环境。 –no-site-packages #令隔离环境不能访问系统全局的site-packages目录。 –system-site-packages #令隔离环境可以访问系统全局的site-packages目录。 –unzip-setuptools #安装时解压Setuptools或Distribute –relocatable #重定位某个已存在的隔离环境。使用该选项将修正脚本并令所有.pth文件使用相当路径。 –distribute #使用Distribute代替Setuptools，也可设置环境变量VIRTUALENV_DISTRIBUTE达到同样效要。 –extra-search-dir=SEARCH_DIRS #用于查找setuptools/distribute/pip发布包的目录。可以添加任意数量的–extra-search-dir路径。 –never-download #禁止从网上下载任何数据。此时，如果在本地搜索发布包失败，virtualenv就会报错。 –prompt==PROMPT #定义隔离环境的命令行前缀。 组成再看一下virtual环境的目录（Windows 10环境下） 123456- [虚拟环境名] - Inculde - Lib - Script - tcl - LICENCE.txt Include ： Lib：库文件 Script： 脚本文件 tcl 首先介绍一下python的运行环境。 在VScode中愉快玩耍 如何在VScode中部署virtual环境 第一种方法在项目的\.vscode\launch.json文件里加了一句:&quot;pythonPath&quot;: &quot;D:\\myproject\\venv\\Scripts\\python.exe&quot;, 第二种方法 本人亲测有效 要在VScode中使用Python的virtualenv虚拟环境,首先在设置同添加虚拟环境所在目录和虚拟环境的识别规则: 12345678910&#123; ... "python.venvPath": "E:\\envs", "python.venvFolders": [ "envs", ".pyenv", ".direnv', ".env" ]&#125; 之后当你想要切换虚拟环境的时候只需要通过命令面板Python: Select Interpreter就会列出所有的虚拟环境. 它其实是在你的项目根目录下新建了一个 ./.vscode/settings.json 配置文件，将解释器路径选项写在里面了，该文件称为工作区设置，可以针对每个项目单独设置配置项。 第三种方法在命令行激活虚拟环境后，直接用 code打开项目文件夹就行了。。。 12XXX&gt;venv\scripts\activate(venv) XXX&gt;code project_name/ 使用 Visual Studio Code（VSCode）搭建简单的Python+Django开发环境的方法步骤 VScode中支持Python虚拟环境 在vscode中启用python的virtualenv 进阶使用virtualenvwrapper管理虚拟环境]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合源码学习（1）]]></title>
    <url>%2F2020%2F03%2F10%2Fcollection1%2F</url>
    <content type="text"><![CDATA[map类相关知识点总结 主要关注map相关的源码 HashTable、HashMap、HashSet 、TreeMap… HashTable 基于位桶+链表实现。 继承Dictionary（陈旧） 实现Cloneable，可被克隆 实现Serializable，可被序列化 hash123int hash = key.hashCode();//0x7FFFFFFF转换为10进制之后是Intger.MAX_VALUE,也就是2^31 - 1int index = (hash &amp; 0x7FFFFFFF) % tab.length; 很容易看出Hashtable的hash算法首先使得hash的值小于等于整型数的最大值，再通过%运算实现均匀散射。 ==由于计算机是底层的运算是基于2进制的，所以HashMap的hash算法使用&amp;运算代替%运算，在运算速度上明显HashMap的hash算法更优。== 为什么一般hashtable的桶数会取一个素数 属性构造1234567891011121314151617public Hashtable(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal Load: "+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry&lt;?,?&gt;[initialCapacity]; threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1);&#125;public Hashtable() &#123; this(11, 0.75f);&#125; Hashtable底层数组的长度可以为任意值，这就造成了当底层数组长度为合数的时候，Hashtable的hash算法散射不均匀，容易产生hash冲突。所以，可以清楚的看到Hashtable的默认构造函数底层数组长度为11（质数），至于为什么Hashtable的底层数组用质数较好，请参考博文； 方法put value不能为null 线程安全 12345678910111213141516171819202122232425262728293031323334353637383940414243public synchronized V put(K key, V value) &#123; // Make sure the value is not null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; for(; entry != null ; entry = entry.next) &#123; if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; V old = entry.value; entry.value = value; return old; &#125; &#125; addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; modCount++; Entry&lt;?,?&gt; tab[] = table; if (count &gt;= threshold) &#123; // Rehash the table if the threshold is exceeded rehash(); tab = table; hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; tab[index] = new Entry&lt;&gt;(hash, key, value, e); count++;&#125; 可以看出HashTable到了jdk1.8了内部结构并没有实质优化，继续使用数组+链表的方式实现。 rehash123456789101112131415161718192021222324252627282930protected void rehash() &#123; int oldCapacity = table.length; Entry&lt;?,?&gt;[] oldMap = table; // overflow-conscious code int newCapacity = (oldCapacity &lt;&lt; 1) + 1; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; newCapacity = MAX_ARRAY_SIZE; &#125; Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; modCount++; threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); table = newMap; for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; //使用头插法将链表反序 e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; Hashtable的扩容将先创建一个长度为原长度2倍的数组，再使用头插法将链表进行反序。 和HashMap的不同之处 HashMap Hashtable 继承机制 继承AbstractMap 继承Dictionary 线程安全 线程不安全 put、remove等方法均被synchronized修饰，同步，线程安全 参数设置 key、value均可为null key、value均不可为null 结构 由位桶+链表+红黑树构成 由位桶+链表构成 添加entry 添加新entry时，放置链表尾部 添加新entry时，放置链表头部 构造上 底层数组的长度必须为2^n ,默认长度16 底层数组的长度可以为任意值，默认长度11 Hash算法上 hash算法通过非常规的设计，将底层table长度设计为2^n（合数）并使用了&amp;运算来代替%运算以减少性能上面的损耗 hash算法首先使得hash的值小于等于整型数的最大值，再通过%运算实现均匀散射 扩容机制上 扩容不需要链表反转，用hashCode新增的bit位查看是否插入旧数组还是插入新数组位置 头插法扩容 知乎：hashMap和hashTable的区别 HashMapHashMap就是典型的拉链法哈希表结构。· 他实际上是一个线性数组。他的静态内部类继承了一个Entry接口。这里注意，在jdk1.8中，在链表中加入了红黑树（平衡二分查找树）。所以1.8版本的HashMap是由数组+（链表+红黑树）实现的。 hash基本概念 Hash，即散列，把任意长度的输入，通过散列算法变成固定长度的输出。由于是不定长到定长的压缩映射，输出值域小于输入值域，所以不同的输入可能有相同的输出，即hash碰撞。 hash算法 123456static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//手写的，源码在不存在这一句，但是原理是类似的，详情可以去看putVal方法int i = (table.length-1) &amp; hash(key) 这里Hash算法的本质是取key的hashCode值、高位运算、取模运算 · 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 //modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化 int modCount; int size; ...... static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//hash code value for this map entry final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; ......&#125; 构造可以看出HashMap的底层数组的长度必须为2^n，这样做的好处是为以后的hash算法做准备 1234567891011121314151617181920212223public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;//该方法返回大于等于cap的最小2次幂的整数static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 方法所有方法集合 123456789101112131415161718192021222324void clear() // 移除所有键值Object clone() // 浅复制，键值本身不会复制V compute(K key, BiFunction&lt;? super K,? super V,? extends V&gt; remappingFunction) // 尝试为指定的键和它当前映射的值计算一个映射（没有则返回null）V computeIfAbsent(K key, Function&lt;? super K,? extends V&gt; mappingFunction) // 如果指定的键没有关联值（或关联null），使用指定的映射函数计算值，非空则加入mapV computeIfPresent(K key, BiFunction&lt;? super K,? super V,? extends V&gt; remappingFunction) // 如果指定键关联了非空值，尝试用给定的键值计算一个映射boolean containsKey(Object key) // 是否包含某键boolean containsValue(Object value) // 是否包含某值Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() // 键值对的Set集合void forEach(BiConsumer&lt;? super K,? super V&gt; action) // 为每个键值对指定特定函数，抛出异常则中断V get(Object key) // 获取某键的值V getOrDefault(Object key, V defaultValue) // 获取某键的值，没有则返回默认值boolean isEmpty() // map是否为空容器Set&lt;K&gt; keySet() // 键的Set集合V merge(K key, V value, BiFunction&lt;? super V,? super V,? extends V&gt; remappingFunction) // 如果指定的键没有关联值或关联null，则用给定的非空值关联它V put(K key, V value) // 添加键值对void putAll(Map&lt;? extends K,? extends V&gt; m) // 添加给定map中所有键值对V putIfAbsent(K key, V value) // 如果指定的键没有关联值或关联null，将它关联新值，并返回原值V remove(Object key) // 移除某键boolean remove(Object key, Object value) // 移除某键值对V replace(K key, V value) // 替换键值对boolean replace(K key, V oldValue, V newValue) // 原值相同则替换键值对void replaceAll(BiFunction&lt;? super K,? super V,? extends V&gt; function) // 调用函数替换每个键值对，抛出异常中断int size() // map大小Collection&lt;V&gt; values() // map的值集合 get/getNode12345678910111213141516171819202122232425262728293031public V get(Object key) &#123; Node&lt;K,V&gt; e; //key是否存在，存在返回key的value值，不存在返回null //hash(key)获得key的hash值 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; //Entry数组 Node&lt;K,V&gt; first, e; int n; //数组长度 K k; // 1. 定位键值对所在桶的位置 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //2.判断键值的hashcode相等，对象相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 3..如果 first 是 TreeNode 类型，则调用黑红树查找方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 具体逻辑： 计算桶在桶数组的位置(first = tab[(n - 1) &amp; hash]) != null 1a % b == (b-1) &amp; a ,当b是2的指数时，等式成立。 判断hashcode是否相等,对象是否相等 判断是否是TreeNode类型 put/putVal 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);//1. onlyIfAbsent参数 &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 初始化桶数组 table if ((tab = table) == null || (n = tab.length) == 0) //扩容方法 n = (tab = resize()).length; // 当前key不存在，新建键值对加入 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果键的值以及节点 hash 等于链表中的第一个键值对节点时，则将 e 指向该键值对 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果节点下引用数据结构为红黑树，调用红黑树插入法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 链表结构，遍历 for (int binCount = 0; ; ++binCount) &#123; //不存在当前需要插入的节点 if ((e = p.next) == null) &#123; //新建一个节点插入 p.next = newNode(hash, key, value, null); //链表长度超过或等于树化阙值（8），对链表进行树化 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //需要插入的节点已经存在了 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null)//1.onlyIfAbsent 判断 e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 具体逻辑： 初始化桶数组，判断是否需要扩容 判断key是否存在，若不存在就新创建一个 节点 若存在，首先判断键的值和节点的hash值是否是链表第一个节点，是则插入 其次判断是否是树节点，是则执行树的插入 如果都不是就遍历链表，插入节点，大于阙值就树化，遇到相等的节点就覆盖（hash相等，键相等） onlyIfAbsent参数 put的onlyIfAbesent的false，putIfAbsent的onlyIfAbsent是true 在放入数据时，如果存在重复的key，那么putIfAbsent不会放入值。 如果传入key对应的value已经存在，就返回存在的value，不进行替换。如果不存在，就添加key和value，返回null resizeresize()只有在两种情况下会被调用：· 由于HashMap实行了懒加载: 新建HashMap时不会对table进行赋值, 而是到第一次插入时, 进行resize时构建table; 当HashMap的size值大于 threshold时, 会进行resize(); 看一下threshold在源码中的注解: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;//将当前table暂存到oldtab来操作 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果老容量大于最大容量 threshold = Integer.MAX_VALUE;//阈值设置为Integer的最大值，好像是2147483647，远大于默认的最大容量 return oldTab;//直接返回当前table，不用扩容 &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // 双倍扩大老内存和老阈值并赋给新的table &#125;else if (oldThr &gt; 0) // 初始化HashMap时添加了初始容量参数 newCap = oldThr; else &#123; //这种情况是初始化HashMap时啥参数都没加 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123;//当只满足老阈值大于0的条件时，新阈值等于新容量*默认扩容因子 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;//把新的阈值赋给当前table @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];//创建容量为newCap的新table table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123;//对老table进行遍历 Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123;//遍历到的赋给e进行暂存，同时将老table对应项赋值为null oldTab[j] = null; if (e.next == null)//将不为空的元素复制到新table中 newTab[e.hash &amp; (newCap - 1)] = e;//只有一个节点时等于是创建一个新的空table然后重新进行元素的put，这里的table长度是原table的两倍 else if (e instanceof TreeNode)// 红黑树进行旋转 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order //HashMap在JDK1.8的时候改善了扩容机制，原数组索引i上的链表不需要再反转。 // 扩容之后的索引位置只能是i或者i+oldCap（原数组的长度） // 所以我们只需要看hashcode新增的bit为0或者1。 // 假如是0扩容之后就在新数组索引i位置，新增为1，就在索引i+oldCap位置 Node&lt;K,V&gt; loHead = null, loTail = null;//用于保存put后不移位的链表 Node&lt;K,V&gt; hiHead = null, hiTail = null;//用于保存put后移位的链表 Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123;//如果与的结果为0，表示不移位，将桶中的头结点添加到lohead和lotail中，往后如果桶中还有不移位的结点，就向tail继续添加 if (loTail == null)//在后面遍历lohead和lotail保存到table中时，lohead用于保存头结点的位置，lotail用于判断是否到了末尾 loHead = e; else loTail.next = e; loTail = e; &#125; else &#123;//这是添加移位的结点，与不移位的类似 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123;//把不移位的结点添加到对应的链表数组中去 loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123;//把移位的结点添加到对应的链表数组中去 hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 具体逻辑 若table为null或容量为0，则使用默认值16扩容，临界值为16*0.75，否则2 若容量是否超过设定的最大容量，重置临界值不扩容返回，否则3 容器容量加倍，临界值加倍· 若容器原来不为空，则需迁移数据 oldTab[i]链表中数据分别移至newTab[i]和newTab[i+oldTab.length]中 经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置 jdk1.8对扩容进行优化，使得扩容不再需要进行链表的反转，只需要知道hashcode新增的bit位为0还是1。如果是0就在原索引位置，新增索引是1就在oldIndex+oldCap位置。 推荐看一下这篇重新认识HashMap，对扩容写的很清楚 线程安全性反例，会造成无限环 主要原因在于 并发下的Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap。· 1234567891011121314151617181920public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， "C"); new Thread("Thread1") &#123; public void run() &#123; map.put(7, "B"); System.out.println(map); &#125;; &#125;.start(); new Thread("Thread2") &#123; public void run() &#123; map.put(3, "A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 简单介绍线程安全的ConcurrentHashMap· jdk1.7 底层采用分段的数组+链表实现，线程安全 通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。) Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术 有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁 扩容：段内扩容（段内元素超过该段对应Entry数组长度的75%触发扩容，不会对整个Map进行扩容），插入前检测需不需要扩容，有效避免无效扩容 jdk1.8 底层数据结构： JDK1.7的 ConcurrentHashMap底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable和 JDK1.8 之前的 HashMap的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本； ② Hashtable(同一把锁) :使用 synchronized来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 WeakHashMapHashMap和WeakHashMap的相同点 它们都是散列表，存储的是“键值对”映射。 它们都继承于AbstractMap，并且实现Map基础。 它们的构造函数都一样。它们都包括4个构造函数，而且函数的参数都一样。 默认的容量大小是16，默认的加载因子是0.75。 它们的“键”和“值”都允许为null。 它们都是“非同步的”。 HashMap和WeakHashMap的不同点HashMap实现了Cloneable和Serializable接口，而WeakHashMap没有。 HashMap实现Cloneable，意味着它能通过clone()克隆自己。 HashMap实现Serializable，意味着它支持序列化，能通过序列化去传输。 HashMap的“键”是“强引用(StrongReference)”，而WeakHashMap的键是“弱引用(WeakReference)”。 WeakReference的“弱键”能实现WeakReference对“键值对”的动态回收。当“弱键”不再被使用到时，GC会回收它，WeakReference也会将“弱键”对应的键值对删除。 这个“弱键”实现的动态回收“键值对”的原理呢？其实，通过WeakReference(弱引用)和ReferenceQueue(引用队列)实现的。 首先，我们需要了解WeakHashMap中： 第一，“键”是WeakReference，即key是弱键。 第二，ReferenceQueue是一个引用队列，它是和WeakHashMap联合使用的。当弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 WeakHashMap中的ReferenceQueue是queue。 第三，WeakHashMap是通过数组实现的，我们假设这个数组是table。 动态回收步骤1将WeakHashMap中的key设置null，并执行gc()。系统会回收key 新建WeakHashMap，将“键值对”添加到WeakHashMap中。将“键值对”添加到WeakHashMap中时，添加的键都是弱键。 实际上，WeakHashMap是通过数组table保存Entry(键值对)；每一个Entry实际上是一个单向链表，即Entry是键值对链表。 当某“弱键”不再被其它对象引用，并被GC回收时。在GC回收该“弱键”时，这个“弱键”也同时会被添加到queue队列中。例如，当我们在将“弱键”key添加到WeakHashMap之后；后来将key设为null。这时，便没有外部外部对象再引用该了key。 接着，当Java虚拟机的GC回收内存时，会回收key的相关内存；同时，将key添加到queue队列中。 当下一次我们需要操作WeakHashMap时，会先同步table和queue。table中保存了全部的键值对，而queue中保存被GC回收的“弱键”；同步它们，就是删除table中被GC回收的“弱键”对应的键值对。 HashSet HashSet基于HashMap来实现的，底层是使用HashMap的Key保存元素，Value统一为类的常量Object对象。 继承AbstractCollection，实现Set、Serializable、Cloneable接口。 Serializable：可被序列化 Cloneable：可被克隆 特性： 无序 唯一 可为null 不同步（非线程安全） 属性1234// 使用map的Key保存元素private transient HashMap map;// map的Value均指向PRESENT，不设置为null是为了防止NullPointerExceptionprivate static final Object PRESENT = new Object(); 构造123456789101112131415161718// 使用HashMap的初始容量0和默认加载因子0.75f来初始化（JDK1.8）public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(Collection&lt;? extends E&gt; c) &#123; // 容量为c的4/3倍和16的最大值 map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); // 添加所有元素 addAll(c);&#125;// 指定初始容量public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity);&#125;// 指定初始容量，加载因子（dummy未使用）HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 方法123456789boolean add(E e) // 添加元素void clear() // 清除Object clone() // 浅复制boolean contains(Object o) // 是否包含boolean isEmpty() // 是否为空Iterator&lt;E&gt; iterator() // 迭代器boolean remove(Object o) // 移除int size() // 集合大小Spliterator&lt;E&gt; spliterator() add1234public boolean add(E e) &#123; // 调用HashMap.put(key, value)方法（key=e，value=PRESENT） return map.put(e, PRESENT)==null;&#125; 方法声明只有当元素尚未存在于集合中时才会添加元素。如果成功添加了元素，则该方法返回true，否则返回false。 remove1234public boolean remove(Object o) &#123; //调用HashMap.remove(key)方法 return map.remove(o)==PRESENT;&#125; HashSet如何保持唯一性？当我们将一个对象放入一个HashSet时，它使用该对象的hashcode值来确定一个元素是否已经在该集合中。 每个散列码值对应于某个块位置，该块位置可以包含计算出的散列值相同的各种元素。但是具有相同hashCode的两个对象可能不相等。 因此，将使用equals（）方法比较同一存储桶中的对象。 hashCode（）与equals（）的相关规定： 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个equals方法返回true 两个对象有相同的hashcode值，它们也不一定是相等的 综上，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。 性能HashSet的性能主要受两个参数影响 - 初始容量和负载因子。 较低的初始容量降低了空间复杂性，但增加了重新散布的频率，这是一个昂贵的过程。 另一方面，高初始容量增加了迭代成本和初始内存消耗。 HashMap 和 HashSet区别TreeMap TreeMap是基于红黑树（Red-Black Tree） TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 属性12345678// 比较器private final Comparator&lt;? super K&gt; comparator;// 根节点private transient Entry&lt;K,V&gt; root;// 树中键值对个数private transient int size = 0;// 树结构的修改次数private transient int modCount = 0; 构造12345678910111213141516171819public TreeMap() &#123; // 比较器为null，使用key的自然顺序来维持TreeMap的顺序，Key要求实现Comparable接口 comparator = null; &#125;public TreeMap(Comparator&lt;? super K&gt; comparator) &#123; this.comparator = comparator; // 设置比较器&#125;public TreeMap(Map&lt;? extends K, ? extends V&gt; m) &#123; comparator = null; // 比较器为null putAll(m); // 放置所有元素&#125;public TreeMap(SortedMap&lt;K, ? extends V&gt; m) &#123; comparator = m.comparator(); // 根据SortedMap的比较器维持TreeMap的顺序 try &#123; buildFromSorted(m.size(), m.entrySet().iterator(), null, null); &#125; catch (java.io.IOException cannotHappen) &#123; &#125; catch (ClassNotFoundException cannotHappen) &#123; &#125;&#125; 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253V put(K key, V value) // 添加键值对void putAll(Map&lt;? extends K,? extends V&gt; map) // 添加map中所有V remove(Object key) // 根据key移除void clear() // 移除所有键值对Map.Entry&lt;K,V&gt; pollFirstEntry() // 移除并返回最小key的键值对Map.Entry&lt;K,V&gt; pollLastEntry() // 移除并返回最大key的键值对V replace(K key, V value) // 替换某key的valueboolean replace(K key, V oldValue, V newValue) // 替换某键值对的valueint size() // 键值对个数Comparator&lt;? super K&gt; comparator() // 返回比较器，根据key的自然排序则返回nullV get(Object key) // 指定key的valueSet&lt;K&gt; keySet() // key集合NavigableSet&lt;K&gt; navigableKeySet() // 和keySet()一样Collection&lt;V&gt; values() // 值集合Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() // 映射的键值对集合boolean containsKey(Object key) // 是否包含某键boolean containsValue(Object value) // 是否包含某值NavigableSet&lt;K&gt; descendingKeySet() // key的降序集合NavigableMap&lt;K,V&gt; descendingMap() // 键值对的降序映射Map.Entry&lt;K,V&gt; ceilingEntry(K key) // key的上限（&gt;=）键值对Map.Entry&lt;K,V&gt; floorEntry(K key) // key的下限（&lt;=）键值对K ceilingKey(K key) // key的上限（&gt;=）键K floorKey(K key) // key的下限（&lt;=）键Map.Entry&lt;K,V&gt; firstEntry() // 最小的键值对Map.Entry&lt;K,V&gt; lastEntry() // 最大的键值对K firstKey() // 最小的keyK lastKey() // 最大的keyMap.Entry&lt;K,V&gt; higherEntry(K key) // 大于key的第一个键值对K higherKey(K key) // 大于key的第一个键Map.Entry&lt;K,V&gt; lowerEntry(K key) // 小于key的第一个键值对K lowerKey(K key) // 小于key的第一个键SortedMap&lt;K,V&gt; headMap(K toKey) // portion &lt; tokey 的部分NavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive) // portion &lt; tokey 的部分（inclusive是否包含边界）SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey) // fromKey &lt;= portion &lt; toKey 的部分NavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive) // fromKey &lt; portion &lt; toKey 的部分（inclusive是否包含边界）SortedMap&lt;K,V&gt; tailMap(K fromKey) // portion &gt;= tokey 的部分NavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive) // portion &gt; tokey 的部分（inclusive是否包含边界）void forEach(BiConsumer&lt;? super K,? super V&gt; action) // 对每个键值对执行相同操作，异常中断void replaceAll(BiFunction&lt;? super K,? super V,? extends V&gt; function) // 对每个键值对执行相同的替换操作，异常中断Object clone() // 浅复制TreeMap实例 使用示例 1234567891011121314// 遍历treeMaptreeMap.forEach(new BiConsumer&lt;Integer, String&gt;() &#123; @Override public void accept(Integer integer, String s) &#123; System.out.println(integer+"-&gt;"+s); &#125;&#125;);// 替换treeMap所有元素treeMap.replaceAll(new BiFunction&lt;Integer, String, String&gt;() &#123; @Override public String apply(Integer integer, String s) &#123; return s+"...."; &#125;&#125;); put123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public V put(K key, V value) &#123; Entry&lt;K,V&gt; t = root; // 根节点为null，直接创建一个节点返回 if (t == null) &#123; compare(key, key); // 类型校验，是否为null root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; Entry&lt;K,V&gt; parent; Comparator&lt;? super K&gt; cpr = comparator; // 比较器不为null，使用比较器维持TreeMap的元素顺序 if (cpr != null) &#123; // 循环，获取插入后的父节点 do &#123; // 记录父节点 parent = t; cmp = cpr.compare(key, t.key); // key&lt;t.key，插入到节点的左边 if (cmp &lt; 0) t = t.left; // key&gt;t.key，插入到节点的右边 else if (cmp &gt; 0) t = t.right; // key==t.key，覆盖原值即可，并返回原值 else return t.setValue(value); &#125; while (t != null); &#125; // 比较器为null，使用key作为比较器进行比较，key需实现Comparable接口 else &#123; if (key == null) throw new NullPointerException(); @SuppressWarnings("unchecked") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; // 同上面一样，获取插入后的父节点 do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; // 创建新节点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) // key小于父节点，插入到左侧 parent.left = e; else // key大于父节点，插入到右侧 parent.right = e; // 插入后，保持红黑树平衡进行调整 fixAfterInsertion(e); size++; modCount++; return null;&#125; rotateLeft12345678910111213141516171819202122232425// 左旋// 重置三个父子节点// 1. p 和 r.left// 2. p.parent 和 r// 3. r 和 pprivate void rotateLeft(Entry&lt;K,V&gt; p) &#123; if (p != null) &#123; Entry&lt;K,V&gt; r = p.right; // 1. p 和 r.left 建立父子节点关系 p.right = r.left; if (r.left != null) r.left.parent = p; // 2. p.parent 和 r 建立父子节点关系 r.parent = p.parent; if (p.parent == null) root = r; else if (p.parent.left == p) p.parent.left = r; else p.parent.right = r; // 3. r 和 p 建立父子节点关系 r.left = p; p.parent = r; &#125;&#125; rotateRight123456789101112131415161718192021222324// 右旋// 重置三个父子节点// 1. p 和 l.right// 2. p.parent 和 l// 3. l 和 pprivate void rotateRight(Entry&lt;K,V&gt; p) &#123; if (p != null) &#123; Entry&lt;K,V&gt; l = p.left; // 1. p 和 l.right 建立父子节点关系 p.left = l.right; if (l.right != null) l.right.parent = p; // 2. p.parent 和 l 建立父子节点关系 l.parent = p.parent; if (p.parent == null) root = l; else if (p.parent.right == p) p.parent.right = l; else p.parent.left = l; // 3. l 和 p 建立父子节点关系 l.right = p; p.parent = l; &#125;&#125; remove123456789101112public V remove(Object key) &#123; // 根据key查找键值对 Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; // 保存原值 V oldValue = p.value; // 删除键值对 deleteEntry(p); // 返回原值 return oldValue;&#125; 对equal的几点补充 集合中大量使用equals方法，实际上，该方法如果没有被重写，则就比较两个对象是否完全相等。 java对equals()的要求 123451. 对称性：如果x.equals(y)返回是&quot;true&quot;，那么y.equals(x)也应该返回是&quot;true&quot;。2. 反射性：x.equals(x)必须返回是&quot;true&quot;。3. 类推性：如果x.equals(y)返回是&quot;true&quot;，而且y.equals(z)返回是&quot;true&quot;，那么z.equals(x)也应该返回是&quot;true&quot;。4. 一致性：如果x.equals(y)返回是&quot;true&quot;，只要x和y内容一直不变，不管你重复x.equals(y)多少次，返回都是&quot;true&quot;。5. 非空性，x.equals(null)，永远返回是&quot;false&quot;；x.equals(和x不同类型的对象)永远返回是&quot;false&quot;。 equals() 与 == 的区别是什么？ 没有被复写的时候等价 hashCode() 的作用 本质作用是确定该对象在哈希表中的索引位置 在散列表中可以判断对象的大小关系 hashCode() 和 equals() 的关系 在会创建“类对应的散列表”的情况下 1)、如果两个对象相等，那么它们的hashCode()值一定相同。 这里的相等是指，通过equals()比较两个对象时返回true。 2)、如果两个对象hashCode()相等，它们并不一定相等。 因为在散列表中，hashCode()相等，即两个键值对的哈希值相等。然而哈希值相等，并不一定能得出键值对相等。补充说一句：“两个不同的键值对，哈希值相等”，这就是哈希冲突。 对conparable和comparator的几点补充Java 中 Comparable 和 Comparator 比较 ComparableComparable 是排序接口。 若一个类实现了Comparable接口，就意味着“该类支持排序”。 即然实现Comparable接口的类支持排序，假设现在存在“实现Comparable接口的类的对象的List列表(或数组)”，则该List列表(或数组)可以通过 Collections.sort（或 Arrays.sort）进行排序。 123456package java.lang;import java.util.*;public interface Comparable&lt;T&gt; &#123; public int compareTo(T o);&#125; 说明：假设我们通过 x.compareTo(y) 来“比较x和y的大小”。若返回“负数”，意味着“x比y小”；返回“零”，意味着“x等于y”；返回“正数”，意味着“x大于y”。 ComparatorComparator 是比较器接口。 我们若需要控制某个类的次序，而该类本身不支持排序(即没有实现Comparable接口)；那么，我们可以建立一个“该类的比较器”来进行排序。这个“比较器”只需要实现Comparator接口即可。 也就是说，我们可以通过“实现Comparator类来新建一个比较器”，然后通过该比较器对类进行排序。 12345678package java.util;public interface Comparator&lt;T&gt; &#123; int compare(T o1, T o2); boolean equals(Object obj);&#125; 说明： 若一个类要实现Comparator接口：它一定要实现compareTo(T o1, T o2) 函数，但可以不实现 equals(Object obj) 函数。 为什么可以不实现 equals(Object obj) 函数呢？ 因为任何类，默认都是已经实现了equals(Object obj)的。 Java中的一切类都是继承于java.lang.Object，在Object.java中实现了equals(Object obj)函数；所以，其它所有的类也相当于都实现了该函数。 int compare(T o1, T o2)是“比较o1和o2的大小”。返回“负数”，意味着“o1比o2小”；返回“零”，意味着“o1等于o2”；返回“正数”，意味着“o1大于o2”。 比较Comparable是排序接口；若一个类实现了Comparable接口，就意味着“该类支持排序”。而Comparator是比较器；我们若需要控制某个类的次序，可以建立一个“该类的比较器”来进行排序。 我们不难发现：Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。 1一个类本身实现了Comparable比较器，就意味着它本身支持排序；若它本身没实现Comparable，也可以通过外部比较器Comparator进行排序。 ConcurrentHashMap(jdk1.7)属性1234567891011 final Segment&lt;K,V&gt;[] segments; transient volatile HashEntry&lt;K,V&gt;[] table;static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; //其他省略&#125; Segment继承了ReentrantLock，所以它就是一种可重入锁（ReentrantLock)。在ConcurrentHashMap，一个Segment就是一个子哈希表，Segment里维护了一个HashEntry数组，并发环境下，对于不同Segment的数据进行操作是不用考虑锁竞争的。（就按默认的ConcurrentLevel为16来讲，理论上就允许16个线程并发执行） 构造segment构造方法 12345Segment(float lf, int threshold, HashEntry&lt;K,V&gt;[] tab) &#123; this.loadFactor = lf;//负载因子 this.threshold = threshold;//阈值 this.table = tab;//主干数组即HashEntry数组 &#125; ConcurrentHashMap构造方法 123456789101112131415161718192021222324252627282930313233343536373839//初始化方法有三个参数，如果用户不指定则会使用默认值，initialCapacity为16，loadFactor为0.75（负载因子，扩容时需要参考），concurrentLevel为16。public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //MAX_SEGMENTS 为1&lt;&lt;16=65536，也就是最大并发数为65536 if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; //2的sshif次方等于ssize，例:ssize=16,sshift=4;ssize=32,sshif=5 int sshift = 0; //ssize 为segments数组长度，根据concurrentLevel计算得出 int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; //segmentShift和segmentMask这两个变量在定位segment时会用到 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //计算cap的大小，即Segment中HashEntry的数组长度，cap也一定为2的n次方. int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; //创建segments数组并初始化第一个Segment，其余的Segment延迟初始化 Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); this.segments = ss; &#125; 初始化完成，我们得到了一个 Segment 数组。我们就当是用 new ConcurrentHashMap() 无参构造函数进行初始化的，那么初始化完成后： Segment数组长度为 16，不可以扩容.Segment数组的大小ssize是由concurrentLevel来决定的，但是却不一定等于concurrentLevel，ssize一定是大于或等于concurrentLevel的最小的2的次幂 Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，后面的代码会介绍 当前 segmentShift的值为 32 - 4 = 28，segmentMask 为 16 - 1 = 15，姑且把它们简单翻译为移位数和掩码，这两个值马上就会用到 方法put123456789101112131415161718public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; //concurrentHashMap不允许key/value为空 if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 //hash函数对key的hashCode重新散列，避免差劲的不合理的hashcode，保证散列均匀 int hash = hash(key); // 2. 根据 hash 值找到 Segment 数组中的位置 j //返回的hash值无符号右移segmentShift位与段掩码进行位运算，定位segment int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3. 插入新值到槽 s 中 //调用代理到segment上的put方法 return s.put(key, hash, value, false); &#125; 从源码看出，put的主要逻辑也就两步： 1. 定位segment并确保定位的Segment已初始化 2. 调用Segment的put方法 关于segmentShift和segmentMask· segmentShift和segmentMask这两个全局变量的主要作用是用来定位Segment，int j =(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask。 segmentMask：段掩码，假如segments数组长度为16，则段掩码为16-1=15；segments长度为32，段掩码为32-1=31。这样得到的所有bit位都为1，可以更好地保证散列的均匀性 segmentShift：2的sshift次方等于ssize，segmentShift=32-sshift。若segments长度为16，segmentShift=32-4=28;若segments长度为32，segmentShift=32-5=27。而计算得出的hash值最大为32位，无符号右移segmentShift，则意味着只保留高几位（其余位是没用的），然后与段掩码segmentMask位运算来定位Segment。 *concurrentHashMap代理到Segment上的put方法，Segment中的put方法是要加锁的。只不过是锁粒度细了而已: * 123456789101112131415161718192021222324252627282930313233343536373839404142434445final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value);//tryLock不成功时会遍历定位到的HashEnry位置的链表（遍历主要是为了使CPU缓存链表），若找不到，则创建HashEntry。tryLock一定次数后（MAX_SCAN_RETRIES变量决定），则lock。若遍历过程中，由于其他线程的操作导致链表头结点变化，则需要重新遍历。 V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash;//定位HashEntry，可以看到，这个hash值在定位Segment时和在Segment中定位HashEntry都会用到，只不过定位Segment时只用到高几位。 HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123;// node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。// 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; //若c超出阈值threshold，需要扩容并rehash。扩容后的容量是当前容量的2倍。这样可以最大程度避免之前散列好的entry重新散列，具体在另一篇文章中有详细分析，不赘述。扩容并rehash的这个过程是比较消耗资源的。 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; 初始化槽: ensureSegment ConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0]，对于其他槽来说，在插入第一个值的时候进行初始化。这里需要考虑并发，因为很可能会有多个线程同时进来初始化同一个槽 segment[k]，不过只要有一个成功了就可以。 1234567891011121314151617181920212223242526272829private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg;&#125; 总的来说，ensureSegment(int k)比较简单，对于并发操作使用 CAS 进行控制。如果当前线程 CAS 失败，这里的 while 循环是为了将 seg 赋值返回。 获取写入锁: scanAndLockForPut 往某个 segment 中 put 的时候，首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value)，也就是说先进行一次 tryLock() 快速获取该 segment 的独占锁，如果失败，那么进入到 scanAndLockForPut 这个方法来获取锁。 123456789101112131415161718192021222324252627282930313233343536373839private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; &#125; // 重试次数如果超过 MAX_SCAN_RETRIES（单核1多核64），那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 这个方法有两个出口，一个是 tryLock() 成功了，循环终止，另一个就是重试次数超过了 MAX_SCAN_RETRIES，进到 lock() 方法，此方法会阻塞等待，直到成功拿到独占锁。这个方法就是看似复杂，但是其实就是做了一件事，那就是获取该segment的独占锁，如果需要的话顺便实例化了一下 node。 rehash segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry[] 进行扩容，扩容后，容量为原来的 2 倍。 首先，我们要回顾一下触发扩容的地方，put 的时候，如果判断该值的插入会导致该 segment 的元素个数超过阈值，那么先进行扩容，再插值 该方法不需要考虑并发，因为到这里的时候，是持有该 segment 的独占锁的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头结点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 这里的扩容比之前的 HashMap 要复杂一些，代码难懂一点。上面有两个挨着的 for 循环，第一个 for 有什么用呢？如果没有第一个 for 循环，也是可以工作的，但是，这个 for 循环下来，如果 lastRun 的后面还有比较多的节点，那么这次就是值得的。因为我们只需要克隆 lastRun 前面的节点，后面的一串节点跟着 lastRun 走就是了，不需要做任何操作。我觉得 Doug Lea 的这个想法也是挺有意思的，不过比较坏的情况就是每次 lastRun 都是链表的最后一个元素或者很靠后的元素，那么这次遍历就有点浪费了。不过 Doug Lea 也说了，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆。 get1234567891011121314151617public V get(Object key) &#123; Segment&lt;K,V&gt; s; HashEntry&lt;K,V&gt;[] tab; int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; //先定位Segment，再定位HashEntry if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null; &#125; get方法无需加锁，由于其中涉及到的共享变量都使用volatile修饰，volatile可以保证内存可见性，所以不会读取到过期数据。 计算 hash 值，找到 segment 数组中的具体位置，或我们前面用的“槽” 槽中也是一个数组，根据 hash 找到数组中具体的位置 到这里是链表了，顺着链表进行查找即可 并发分析put 操作的线程安全性 初始化槽，这个我们之前就说过了，使用了 CAS 来初始化 Segment 中的数组。 添加节点到链表的操作是插入到表头的，所以，如果这个时候 get 操作在链表遍历的过程已经到了中间，是不会影响的。当然，另一个并发问题就是 get 操作在 put 之后，需要保证刚刚插入表头的节点被读取，这个依赖于 setEntryAt 方法中使用的 UNSAFE.putOrderedObject。 扩容。扩容是新创建了数组，然后进行迁移数据，最后面将 newTable 设置给属性 table。所以，如果 get 操作此时也在进行，那么也没关系，如果 get 先行，那么就是在旧的 table 上做查询操作；而 put 先行，那么 put 操作的可见性保证就是 table 使用了 volatile 关键字。 remove 操作的线程安全性get 操作需要遍历链表，但是 remove 操作会”破坏”链表。如果 remove 破坏的节点 get 操作已经过去了，那么这里不存在任何问题。如果 remove 先破坏了一个节点，分两种情况考虑。 1、如果此节点是头结点，那么需要将头结点的 next 设置为数组该位置的元素，table 虽然使用了 volatile 修饰，但是 volatile 并不能提供数组内部操作的可见性保证，所以源码中使用了 UNSAFE 来操作数组，请看方法 setEntryAt。2、如果要删除的节点不是头结点，它会将要删除节点的后继节点接到前驱节点中，这里的并发保证就是 next 属性是 volatile 的。 ConcurrentHashMap(1.8)构造1234567891011// 这构造函数里，什么都不干public ConcurrentHashMap() &#123;&#125;public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; 方法put12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果数组"空"，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else &#123; // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果发现了"相等"的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 到了链表的最末端，将这个新值放到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 红黑树 Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // binCount != 0 说明上面在做链表操作 if (binCount != 0) &#123; // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount &gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // addCount(1L, binCount); return null;&#125; 初始化数组 initTable() 1234567891011121314151617181920212223242526272829private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // 初始化的"功劳"被其他线程"抢去"了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 链表转红黑树: treeifyBin前面我们在 put 源码分析也说过，treeifyBin 不一定就会进行红黑树转换，也可能是仅仅做数组扩容。我们还是进行源码分析吧。 123456789101112131415161718192021222324252627282930313233private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // MIN_TREEIFY_CAPACITY 为 64 // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 后面我们再详细分析这个方法 tryPresize(n &lt;&lt; 1); // b 是头结点 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 加锁 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; // 下面就是遍历链表，建立一颗红黑树 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; // 将红黑树设置到数组相应位置中 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 扩容：tryPresize如果说 Java8 ConcurrentHashMap 的源码不简单，那么说的就是扩容操作和迁移操作。这个方法要完完全全看懂还需要看之后的 transfer 方法，读者应该提前知道这点。这里的扩容也是做翻倍扩容的，扩容后数组容量为原来的 2 倍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) &#123; // c：size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 0.75 * n &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) &#123; // 我没看懂 rs 的真正含义是什么，不过也关系不大 int rs = resizeStamp(n); if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 2. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法 // 此时 nextTab 不为 null if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 1. 将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 我是没看懂这个值真正的意义是什么？不过可以计算出来的是，结果是一个比较大的负数 // 调用 transfer 方法，此时 nextTab 参数为 null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125; 个方法的核心在于 sizeCtl 值的操作，首先将其设置为一个负数，然后执行 transfer(tab, null)，再下一个循环将 sizeCtl 加 1，并执行 transfer(tab, nt)，之后可能是继续 sizeCtl 加 1，并执行 transfer(tab, nt)。所以，可能的操作就是执行 1 次 transfer(tab, null) + 多次 transfer(tab, nt)，这里怎么结束循环的需要看完 transfer 源码才清楚 HashMap和ConcurrentHashMap源码jkd7~8全解析]]></content>
      <tags>
        <tag>java</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表问题整理]]></title>
    <url>%2F2020%2F02%2F23%2FlinkedList%2F</url>
    <content type="text"><![CDATA[链表可以说是最常见的数据结构之一。看似简单但是真正应用起来有很强的灵活性，现在将链表问题归纳一下，从链表的常见变式到具体应用，争取对链表问题一网打尽！ 单链表 相比于数组插入删除时O(n)的时间复杂度，链表的时间复杂度只要O(1) 但是链表随机访问的性能没有数组好，需要 O(n) 的时间复杂度 尾部插入1234567891011121314151617private void insertToTail(int value)&#123; Node node = new Node(value,null); if(null == head)&#123; head = node; &#125;else &#123; Node q = head; while(q.next!= null )&#123; q = q.next; &#125; //关键步骤，不能调换顺序 node.next = q.next; q.next = node; &#125; &#125; 索引插入1234567891011121314151617181920212223private void insert(int value,int index)&#123; Node node = head; Node preNode = null; int pos = 0; while(node != null &amp;&amp; index != pos)&#123; preNode = node; node = node.next; pos++; &#125; Node newNode= new Node(value,node.next); if(node == null)&#123; return ; &#125; if(preNode ==null)&#123; newNode.next = head; head = newNode; &#125;else&#123; newNode.next = preNode.next; preNode.next = newNode; &#125; &#125; 通过值删除节点注意头节点的特殊处理 123456789101112131415161718192021222324252627private void deleteByNode(int data)&#123; if(head == null)&#123; return ; &#125; Node node = head; //应该是保存插入节点的上一个节点 Node q = null; while(node!= null &amp;&amp; node.data != data)&#123; //保存上一个节点 q = node; //使得循环继续下去 node = node.next; &#125; //说明没有找到节点 if(node == null)&#123; return; &#125; //删除head头节点的时候，此时q依然为null(没有进入循环)，必须特殊处理，精髓所在 if(q == null)&#123; head = head.next; &#125;else&#123; q.next = q.next.next; &#125; &#125; 通过值查找1234567private Node findByValue(int data)&#123; Node node = head; while(node != null)&#123; node = node.next; &#125; return node; &#125; 双链表从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 比如，删除操作的情况存在两种情况： 删除结点中“值等于某个给定值”的结点 删除给定指针指向的结点。 第一种情况链表的时间复杂度都是O(n)，第二种情况双链表无需查找，O(1)时间复杂度删除节点 LinkedHashMap用的就是双向链表 插入操作以尾部插入举例 1234567891011121314private void insertToTail(int data)&#123; DuoNode node = new DuoNode(data); DuoNode last = tail; if(count == 0)&#123; head = node; tail = node; &#125;else&#123; last.next = node; node.prev = last; tail = node; &#125; ++count; &#125; 删除操作删除头部 1234567891011private DuoNode deleteHead()&#123; DuoNode h = head; if(count &gt;0)&#123; //两句话最好不要反着来 head = h.next; head.prev = null; --count; return h; &#125; return null; &#125; 删除尾部 123456789private DuoNode deleteTail() &#123; DuoNode q = tail; if (count &gt; 0) &#123; tail = tail.prev; tail.next = null; --count; &#125; return q; &#125; 查找123456789101112private DuoNode findIndex(int index) &#123; if(index &gt;count)&#123; return null; &#125; DuoNode node = head; int num = 0; while(num &lt; index) &#123; node = node.next; ++num; &#125; return node; &#125; 循环链表循环链表的优点是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表 私有属性123456789101112public class CircleLinkedList &#123; private Node head; private Node tail; private int count; CircleLinkedList()&#123; head = null; tail = null; count = 0; &#125; //...&#125; 插入操作12345678910111213private void insertHead(int data)&#123; Node node = new Node(data,null); if(count== 0)&#123; head = node; tail = node; &#125;else&#123; node.next = head; tail.next = node; head = node; count++; &#125; &#125; 应用LRU缓存淘汰算法思路：维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部； 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 构造方法 比起普通链表多维护一个容量 1234567891011121314public class LRUByLinkedList&#123; private Node head; //最大容量 private int size; //已存储容量 private int count; public LRUByLinkedList(int capacity)&#123; head = null; size = capacity; count = 0; &#125; //...&#125; 插入操作 12345678910111213private void insert(int data)&#123; Node preNode = findNode(data); //更新缓存，旧缓存清除 if(null != preNode)&#123; delete(preNode); &#125;else&#123; if(count &gt;= size)&#123; //如果缓存溢出，删除最早的缓存 deleteToTail(); &#125; &#125; insertToHead(data); &#125; 查找缓存 1234567891011private Node findNode(int data) &#123; Node node = head; while(null != node &amp;&amp; null != node.next)&#123; Node nextNode = node.next; if(data == nextNode.data)&#123; return node; &#125; node = nextNode; &#125; return null; &#125; 详细代码 单链表反转递归法 注意开始边界条件 当递归的最后节点的时候将最后第二个节点的next节点指向最后第二个节点，然后最后一个节点成为了头节点 123456789101112/** * 链表反转 ——递归法（精髓） * @param head * @return */ public Node reverseList(Node node) &#123; if (node == null || node.next == null) return node; Node p = reverseList(node.next); node.next.next = node; node.next = null; return p; &#125; 非递归法 要点：1. 必须维护前驱节点 2. 反转将后继指针指向前驱节点 123456789101112131415161718192021/** * 单链表反转 * @param list * @return */public static Node reserve(Node list)&#123; Node cur = list; //存储链表的上一个节点，遍历完之后变成头节点 Node pre = null; while(cur != null)&#123; //暂存指向下一个节点的引用 Node next = cur.next; //反转，将存储的下一个节点指向上一个节点 cur.next = pre; //下面两部将链表向下移动 pre = cur; cur = next; &#125; //遍历完之后pre变成头节点 return pre;&#125; 环的检测123456789101112131415161718192021//使用快慢指针法。追及问题，只要慢指针追上快指针，说明存在环路 public static boolean checkCircle(Node list)&#123; if(list== null) &#123; return false; &#125; Node fast = list.next; Node slow = list; //注意截止条件 while(fast != null &amp;&amp; fast.next != null)&#123; fast = fast.next.next; slow = slow.next; if(fast == slow)&#123; return true; &#125; &#125; return false; &#125; 两个有序链表的合并123456789101112131415161718192021222324252627282930313233public static Node mergeSortList(Node la,Node lb)&#123; Node p = la; Node q =lb; Node head ; //处理头部的特殊情况 if(p.data&lt;q.data)&#123; head = p; p = p.next; &#125;else&#123; head = q; q = q.next; &#125; Node r = head; while(q != null &amp;&amp; p != null) &#123; if (q.data &lt; p.data) &#123; r.next = q; q = q.next; &#125; else &#123; r.next = p; p = p.next; &#125; &#125; if(p.next == null)&#123; r.next = q; &#125; if(q.next == null)&#123; r.next =p; &#125; return r; &#125; 可以通过维护一个哨兵让代码更加简洁 12345678910111213141516/** * 有序合并（哨兵机制优化） * @param la * @param lb * @return */public static Node mergeTwoListsA(Node la,Node lb)&#123; Node soldier = new Node(0,null); Node p = soldier; //正常的合并逻辑 return p;&#125; 递归法 1234567891011121314151617181920212223/** * 递归合并链表 * @param l1 * @param l2 * @return */ public Node mergeTwoLists(Node l1, Node l2) &#123; if (l1 == null) &#123; return l2; &#125; else if (l2 == null) &#123; return l1; &#125; else if (l1.data &lt; l2.data) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; else &#123; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125; &#125; 删除链表的倒数n个节点快指针比慢指针前k个节点，然后快指针遍历到尾部的时候慢指针刚刚好到删除节点，注意保存删除节点的上一个节点 12345678910111213141516171819202122232425262728private void deleteLastKth(Node list,int index)&#123; Node fast = list; //注意这里的i！！！！ int i = 1; while(fast != null &amp;&amp; i&lt;index)&#123; fast = fast.next; i++; &#125; if(fast == null)&#123; return ; &#125; Node slow = list; Node prev = null; //遍历到最后fast指针指向最后一个节点，注意快慢指针之间的间隔 //因为选择使用prev作为删除节点，所以next !=null while(fast.next != null)&#123; fast = fast.next; prev = slow; slow = slow.next; &#125; if(prev != null)&#123; prev.next = prev.next.next; &#125;else&#123; list = list.next; &#125; return ; &#125; 求链表的中间节点快慢指针法 123456789101112private static Node fndMiddleNode(Node list )&#123; Node fast = list; Node slow = list; //第一个条件是防止head为null，第二个是为了使得fast指针遍历到最后一个节点 while(fast != null &amp;&amp; fast.next != null)&#123; fast = fast.next.next; slow = slow.next; &#125; return slow; &#125; 判断是否回文串思路： 使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。 1234567891011121314151617181920212223242526272829303132//使用的public boolean isPalindrome(Node list)&#123; Node slow = list; Node fast = list; Node prev = null; if(list == null)&#123; return false; &#125; while(fast != null &amp;&amp; fast.next != null)&#123; fast = fast.next.next; Node next = slow.next; //链表反向 slow.next = prev; //前驱指针后移 prev = slow; //慢指针后移 slow = next; &#125; //跨过中点，使得prev和slow同步，这里存在节点数是奇数还是偶数的情况 if(fast != null)&#123; slow = slow.next; &#125; if(slow != null)&#123; if(slow.data != prev.data)&#123; return false; &#125; slow = slow.next; prev = prev.next; &#125; return true; &#125;]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一篇文章攻略ARP协议]]></title>
    <url>%2F2020%2F02%2F15%2Farp%2F</url>
    <content type="text"><![CDATA[ARP协议全称地址解析协议（ Address Resolution Protocol），即通过IP地址解析MAC地址的协议。 当主机之间需要相互通信，并且已经获取到对方主机的IP地址后，处于三层的数据包要向下封装，为了能让数据通过物理链路传输到对方，需要知道对方的MAC地址。ARP协议就是通过目的IP地址去寻找目的MAC地址的协议。 报文格式首先查看一下ARP协议的报文格式 Source/Destination Hardware/Protocol Address: 从之前所述可以得知ARP协议通过IP地址去查找MAC地址，因此报文中必定含有源目IP地址和源目MAC地址。同时据此也可以看出ARP协议是工作在网络层的。除此之外，还有一些其他字段 Hardware/Protocol Type：MAC地址/IP地址类型，一般硬件地址类型是以太网，三层协议类型是IP Hardware/Protocol Length:MAC地址/IP地址长度，单位是字节 基本工作流程看完了ARP报文的基本结构，接下来就让我们看看ARP协议是如何具体工作的吧 动态ARPARP协议最常见的应用就是动态ARP 如图，假设新加入的一台主机Host_1想要向Host_3发送数据，此时Host_1已经拥有了Host_3的IP地址 1.ARP请求 ARP协议会维护ARP表，里面存放着IP地址和MAC地址的映射关系。Host_1会查找自己的ARP表项，如果找到，直接单播发送到目的主机；如果没有，全网段广播IP地址 2.ARP响应 收到ARP请求的主机都会查看请求报文的目的IP地址是不是自己的，如果不是，直接丢弃；如果是，因为知道报文发送方的IP地址和MAC地址，所以将发送单播ARP响应到Host_1；同时将该ARP报文中源IP地址和源MAC地址缓存在自己的ARP表中 3.ARP缓存 收到响应后主机Host_1将检查自己的IP地址和响应的IP地址是否匹配，如果匹配成功，将该映射关系缓存在ARP表中，并封装数据传输到对端主机 静态ARP相比较动态ARP自动获取MAC地址，静态ARP需要网络管理员手动将IP地址和MAC地址的映射关系写入到ARP表中，静态的ARP表项不会老化 有两种静态表项 短静态表项：只绑定IP地址和MAC地址，不绑定VLAN和接口。此时该表项不能直接用于数据转发，需要发起ARP请求并收到ARP响应后，匹配响应的映射关系和静态表项相同后，将VLAN和接口填入ARP表项后，该表项才能正式生效 长静态表项：不仅绑定IP地址和MAC地址，同时绑定VLAN和接口，此时该表项直接生效. 配置静态ARP的场景 静态ARP虽然配置繁琐，不灵活，但是不会老化，也不会被动态ARP表项覆盖，因此在特定场景下有特殊的用途，下面介绍几种典型应用 动态ARP容易被攻击，同时也可能会出现老化而消失的现象，因此为了确保某台重要设备的稳定性和安全性，可以选择配置相对安全可靠的静态ARP。 当网络中用户设备的MAC地址为组播MAC地址时，可以在路由器上配置静态ARP表项。缺省情况下，设备收到源MAC地址为组播MAC地址的ARP报文时不会进行ARP学习。 当需要禁止某个IP地址访问设备时，可以将该IP地址和一个不存在的MAC地址绑定。 老化机制存在在ARP表中的动态ARP表项存在有效时间，过期将删除该表项，从而释放ARP表的容量，提高设备的性能 动态ARP表项的老化参数有：老化超时时间、老化探测次数和老化探测模式 老化超时时间：ARP达到超时时间后会发送老化探测报文（其实就是ARP请求），如果收到响应，更新ARP表项，如果没有，探测结束 老化探测次数：到达一定老化探测次数未收到响应，删除该表项 老化探测模式：探测报文支持广播和单播 注：当接口down掉后，该接口相关的所有动态ARP表项将全部删除 其他特性免费ARP可用于主机IP冲突检测：当主机的IP地址发生变更后或者链路刚刚UP后，为了防止和已存在的IP地址冲突，ARP协议将主动广播包含主机IP地址的ARP请求，如果收到ARP响应，说明该IP地址已经被使用，则存在冲突。 通告新的MAC地址：由于设备新换了网卡等情况导致MAC地址发生变更，此时可以发送免费ARP来让其它主机更新表项 用于VRRP组主备模式之间切换： 发生主备变换后，MASTER设备会广播发送一个免费ARP报文来通告发生了主备变换 代理ARP如果ARP报文需要发往同一网段但不同物理网络的主机时，就需要通过代理ARP来实现请求的代理。开启代理ARP的设备将接受本网段的ARP请求，同时查找路由表，如果路由表中有相应的IP地址，说明通过该设备可以通往目的设备，则代理ARP设备会以自己的MAC地址发送ARP响应，之后便通过该设备进行数据转发。 代理ARP共有三种模式 路由式代理： 需要互通的主机（主机上没有配置缺省网关，无法知道）处于相同的网段但不在同一物理网络（即不在同一广播域）的场景 VLAN内代理： 需要互通的主机处于相同网段，并且属于相同VLAN，但是VLAN内配置了端口隔离的场景。 VLAN间代理： 需要互通的主机处于相同网段，但属于不同VLAN的场景 代理ARP功能强大，一般用于网关路由器，可以防止用户配置错误网关地址依然可以通信，但是由于代理ARP对通信无感知，因此可能被黑客利用实施ARP欺骗等攻击，对网络安全造成风险。同时，在配置静态路由时如果使用错误使用代理ARP也会导致学习到错误的MAC地址。 RARP属于ARP的逆向协议， 可以将MAC地址解析为IP地址的协议。 一般会在无盘工作站、帧中继协议中出现，但是现在使用较少。 that ’s all~]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>ICT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String类源码学习]]></title>
    <url>%2F2020%2F02%2F12%2FstringBuilder%2F</url>
    <content type="text"><![CDATA[恰好最近项目做到了string类相关的问题，就借着网上的分析，较深入地比较了一下stringBuilder类和String类的源码，本来还应该有一个Stringbuffer类，因为时间原因所以只能下次补上了。 StringBuilder类和string类的区别相同：底层均采用字符数组value来保存字符串 区别：String类的value数组有final修饰，指向不可改，同时private未提供修改value数组的方法，==因此String类一旦赋值即不可变==。StringBuilder类的value数组没有final修饰，可以改变指向，==且可以扩容，扩容通过新建字符数组完成==。 String类构造函数1.默认的无参和含参构造函数12345678public String() &#123; this.value = "".value; &#125;public String(String original) &#123; this.value = original.value; this.hash = original.hash; &#125; 都是直接在现有常量上进行赋值（将常量的指针指向参数） 2.参数是数组的构造函数123public String(char value[]) &#123; this.value = Arrays.copyOf(value, value.length); &#125; 123456789/**Array类**/public static char[] copyOf(char[] original, int newLength) &#123; char[] copy = new char[newLength]; System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; &#125; 调用Arrays的copyOf方法，返回复制完的新数组。 常用函数length()返回的是字符数组value的长度 123public int length() &#123; return value.length;&#125; isEmpty()判断字符数组长度是否为0 123public boolean isEmpty() &#123; return value.length == 0; &#125; charAt()返回value对象的索引 123456public char charAt(int index) &#123; if ((index &lt; 0) || (index &gt;= value.length)) &#123; throw new StringIndexOutOfBoundsException(index); &#125; return value[index];&#125; equals()重写equals()方法,判断为真的情况是：两个对象相同，同时对应的value字符数组内容也相同 其中：可能是同一对象也可能是内容相同的不同String 1234567891011121314151617181920212223public boolean equals(Object anObject) &#123; //判断是不是同一个对象 if (this == anObject) &#123; return true; &#125; //判断是不是String对象，强转String类型 if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; compareTo()返回：如果第一个不相同字符之差，如果一个字符串是另一个的子串（前lim个字符相同），返回两个字符串长度之差 123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2;&#125; substring()1.substring(int) 该函数返回从给定参数位置起到字符串结束的新字符串。如果给定从0开始，则返回原本的String对象，否则返回一个新的String对象。 12345678910public String substring(int beginIndex) &#123; if (beginIndex &lt; 0) &#123; throw new StringIndexOutOfBoundsException(beginIndex); &#125; int subLen = value.length - beginIndex; if (subLen &lt; 0) &#123; throw new StringIndexOutOfBoundsException(subLen); &#125; return (beginIndex == 0) ? this : new String(value, beginIndex, subLen); &#125; 2.substring(int, int) 如果要获取的子串是从0到最后，则返回原本的String对象，否则返回一个新的String对象。 1234567891011121314public String substring(int beginIndex, int endIndex) &#123; if (beginIndex &lt; 0) &#123; throw new StringIndexOutOfBoundsException(beginIndex); &#125; if (endIndex &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(endIndex); &#125; int subLen = endIndex - beginIndex; if (subLen &lt; 0) &#123; throw new StringIndexOutOfBoundsException(subLen); &#125; return ((beginIndex == 0) &amp;&amp; (endIndex == value.length)) ? this : new String(value, beginIndex, subLen); &#125; replace()返回：如果是相同则返回原来的String对象，不同则先找到第一个不同的字符然后将字符复制到新的字符数组中 123456789101112131415161718192021222324252627public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ //找到第一个不同的索引下标 while (++i &lt; len) &#123; if (val[i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) &#123; buf[j] = val[j]; &#125; while (i &lt; len) &#123; char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(buf, true); &#125; &#125; return this; &#125; 可见，String是依赖字符常量表实现的； 同时所有对String发生修改的方法返回值都是一个新的String对象，没有修改原有对象； 因此是线程安全的 StringBuilder类源码构造函数1 .默认构造函数 Stringbuilder使用了父类AbstractStringBuilder构造，默认长度16.注意这里的value没有final修饰，权限为默认，因此内部数值可以改变· 123public StringBuilder() &#123; super(16);&#125; 1234/*父类*/ AbstractStringBuilder(int capacity) &#123; value = new char[capacity]; &#125; 2.传入字符串· 会设置初始容量为传入字符串长度加上16，再通过append函数将str写入，append函数见常用方法 1234public StringBuilder(String str) &#123; super(str.length() + 16); append(str); &#125; 常用函数append()append函数调用时会首先查看是否超出容量上限 1234567891011@Override public StringBuilder append(Object obj) &#123; return append(String.valueOf(obj)); &#125; @Override public StringBuilder append(String str) &#123; //调用父类 super.append(str); return this; &#125; 123456789public AbstractStringBuilder append(String str) &#123; if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; &#125; 容量检查函数 参数是当前对象的value中字符长度与传入字符串长度之和，也即是value的容量最小值。 如果需要的容量最小值大于目前value容量，新申请一块内存复制进去 扩容的新容量为当前value的容量2倍加2，如果扩容后的容量还是比需要的最小容量小，则直接扩容为需要的最小容量，再将当前value内容复制给一个新的长度为newCapacity的字符数组，再将value指向这个扩容后的新数组。即扩容是通过开辟新数组完成的，返回的也是新创建的新数组。 1234567891011121314151617181920private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125; &#125;//-----------------中间省去很多函数...---------private int newCapacity(int minCapacity) &#123; // overflow-conscious code int newCapacity = (value.length &lt;&lt; 1) + 2; if (newCapacity - minCapacity &lt; 0) &#123; newCapacity = minCapacity; &#125; return (newCapacity &lt;= 0 || MAX_ARRAY_SIZE - newCapacity &lt; 0) ? hugeCapacity(minCapacity) : newCapacity; &#125; 执行完ensureCapacityInternal函数后，this对象的value数组已经指向一个扩容后的新数组，并且之前的value数组里的值也复制到新的value数组中，接下来执行getChars函数 复制字符函数 该函数是将调用的string对象的value数组从srcBegin到srcEnd复制给目标数组dst，从dst数组的第dstBegin位置开始。 append函数中执行完str.getChars函数后就将参数str的内容追加到StringBuilder对象的value数组后面，再更新count值，返回调用对象。 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin);&#125; 删除函数1234567891011@Override public StringBuilder delete(int start, int end) &#123; super.delete(start, end); return this; &#125; @Override public StringBuilder deleteCharAt(int index) &#123; super.deleteCharAt(index); return this; &#125; delete()函数通过覆盖原理，将删除的元素覆盖掉· deleteCharAt()删除索引为index处的字符。通过调用数组复制函数来完成，将索引后面的内容依次复制到从索引开始的位置上，即通过覆盖的原理完成，更新count。 1234567891011121314151617181920212223242526/*父类调用函数*/ public AbstractStringBuilder delete(int start, int end) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (end &gt; count) end = count; if (start &gt; end) throw new StringIndexOutOfBoundsException(); int len = end - start; if (len &gt; 0) &#123; System.arraycopy(value, start+len, value, start, count-end); count -= len; &#125; return this; &#125;public AbstractStringBuilder deleteCharAt(int index) &#123; if ((index &lt; 0) || (index &gt;= count)) throw new StringIndexOutOfBoundsException(index); System.arraycopy(value, index+1, value, index, count-index-1); count--; return this; &#125; 插入函数123456789101112131415 @Override public StringBuilder insert(int index, char[] str, int offset, int len) &#123; super.insert(index, str, offset, len); return this; &#125;/*简单拿这个举例子，后面char类型可以换成其他类型*/ @Override public StringBuilder insert(int offset, char c) &#123; super.insert(offset, c); return this; &#125; 首先确保value数组容量足够，然后通过数组复制，将索引位置开始全部向后移一位，再将索引位置赋值c，更新count。 12345678910/*父类调用函数*/public AbstractStringBuilder insert(int offset, char c) &#123; ensureCapacityInternal(count + 1); System.arraycopy(value, offset, value, offset + 1, count - offset); value[offset] = c; count += 1; return this; &#125; 对比StringBuffer以后有机会在写吧。。。 对比来源 差别 String StringBuffer StringBuilder 常量 / 变量 常量 变量 变量 线程是否安全 安全 安全 非安全 所在内存区域 Constant String Pool(常量池) heap heap 是否能被继承 否 否 否 代码行数 3157 718 448 使用场景 在字符串不经常变化的场景 在频繁进行字符串运算（如拼接、替换、删除等）， 并且运行在多线程环境 在频繁进行字符串运算（如拼接、替换、和删除等）， 并且运行在单线程的环境 场景举例 常量的声明、少量的变量运算 XML 解析、HTTP 参数解析和封装 SQL 语句的拼装、JSON 封装 AbstractStringBuilder：StringBuffer类与StringBuilder类都继承了AbstractStringBuilder，抽象父类里实现了除toString以外的所有方法。StringBuilder：自己重写了方法之后，全都在方法内super.function()，未做任何扩展。同时从类名语义上来说String构建者，所以没有subString方法看来也合情合理；StringBuffer：在重写方法的同时，几乎所有方法都添加了synchronized同步关键字；]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis基础特性和sqlsession原理]]></title>
    <url>%2F2020%2F01%2F20%2Fmybatis%2F</url>
    <content type="text"><![CDATA[主要总结一下mybaitis的基础配置，基本特性，最后简单分析一下sqlsession的原理，了解了运行过程。 var ap = new APlayer({ element: document.getElementById("aplayer-NgQswVho"), narrow: false, autoplay: false, showlrc: false, music: { title: "漂流", author: "菅原纱由理（THE SxPLAY）", url: "http://music.163.com/song/media/outer/url?id=1397665609.mp3", pic: "http://p2.music.126.net/DpcMK6vdj7vD-yY3QUHwng==/109951164433935873.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 简单配置介绍简单来说，Mybatis的配置主要分为以下几步： 编写POJO即JavaBean，最终的目的是将数据库中的查询结果映射到JavaBean上； 配置与POJO对应的Mapper接口：里面有各种方法，对应mapper.xml中的查询语句； 配置与POJO对应的XML映射：编写缓存，SQL查询等； 配置mybatis-config.xml主要的Mybatis配置文件：配置数据源、扫描mapper.xml等。 注意：以上的配置并没有严格的前后顺序； 借用一个比较清晰的配置流程图 可以看出mapper接口实现类的获得是通过mybatis-config.xml-&gt;SqlSessionFactoryBuilder-&gt;SqlSessionFacotry-&gt;SqlSession-&gt;mapper 生命周期 SqlSessionFactoryBuilder：作用就是创建一个构建器，一旦创建了SqlSessionFactory，它的任务就算完成了，可以回收。 SqlSessionFactory：作用是创建SqlSession，而SqlSession相当于JDBC的一个Connection对象，每次应用程序需要访问数据库，我们就要通过SqlSessionFactory创建一个SqlSession，所以SqlSessionFactory在Mybatis整个生命周期中存在（每个数据库对应一个SqlSessionFactory，是单例产生的）。 SqlSession：生命周期是存在于请求数据库处理事务的过程中，是一个线程不安全的对象（在多线程的情况下，需要特别注意），即存活于一个应用的请求和申请，可以执行多条SQL保证事务的一致性。 Mapper：是一个接口，并没有实现类。它的作用是发送SQL，返回我们需要的结果或者发送SQL修改数据库表，所以它存活于一个SqlSession内，是一个方法级别的东西。当SqlSession销毁的时候，Mapper也会销毁。 基础特性延迟加载即系统延迟执行查询，一般用于嵌套查询的时候，嵌套在内的SQL查询会延迟加载，等到真正需要使用该查询的时候才加载。就像懒人你说一下，他动一下，绝不会多执行半步，因此该特性也称之为懒加载。 懒加载必须配置config.xml，并且只能通过association或collection实现，毕竟只有在存在映射关系的业务场景里你需要使用懒加载 配置语句 123&lt;settings&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;setting name="aggressiveLazyLoading" value="false"/&gt; &lt;/settings&gt; 使用时需要注意延迟加载必须使用resultMap，resultType不具有延迟加载功能。 一级缓存 系统默认开启,缓存的范围为一个SqlSession 只有一个SqlSession下的相同查询才会应用缓存，不同sqlSession下的即使查询相同一级缓存也不会生效 二级缓存 解决SqlSession相互隔离的情况，缓存范围为一个Mapper接口 二级缓存默认是不开启的，需要进行配置，Mybatis要求返回的POJO必须是可序列化的，即POJO实现Serializable接口。 缓存的配置只需要在XML配置&lt;cache/&gt;即可，或者指定算法，刷新时间间隔，缓存状态，大小等 例： 映射语句文件中所有select语句将会被缓存； 映射语句文件中所有insert、update和delete语句会被刷新缓存； 缓存使用默认的LRU最近最少使用算法回收； 根据时间表，缓存不会任何时间顺序刷新 缓存会存储列表集合或对象的1024个引用 缓存被视为可read/write的缓存，意味着是不可以被共享的，而可以被安全地修改。 自定义缓存通过Mybatis实现的接口，使用redis等进行缓存 实践配置待补充 SqlSession原理SqlSession提供select/insert/update/delete方法 映射器（Mapper）其实就是一个动态代理对象，进入到MapperMethod的execute方法就能简单找到SqlSession的删除、更新、查询、选择方法. 从底层实现来说：通过动态代理技术，让接口跑起来，之后采用命令模式，最后还是采用了SqlSession的接口方法（getMapper()方法等到Mapper）执行SQL查询（也就是说Mapper接口方法的实现底层还是采用SqlSession接口方法实现的）。 SqlSession重要的四个对象 Execute：调度执行StatementHandler、ParmmeterHandler、ResultHandler执行相应的SQL语句； StatementHandler：使用数据库中Statement（PrepareStatement）执行操作，即底层是封装好了的prepareStatement ParammeterHandler：处理SQL参数 ResultHandler：结果集ResultSet封装处理返回。 SqlSession的四大对象Execute起java和数据库交互桥梁的作用，参与整个SQL执行过程。分类 SIMPLE简易执行器（默认） REUSE重用预处理执行器 BATCH批量更新、批量专用处理器 源码 12package org.apache.ibatis.session;public enum ExecutorType &#123; SIMPLE, REUSE, BATCH&#125; 作用 调度其他对象，完成预编译、配置参数和返回结果集 StatementHanlde分类(分别对应不同执行器) SimpleStatementHandler PrepareStatementHandler CallableStatementHandler 作用 专门处理数据库会话。进行预编译并调用ParameterHandler配置参数。大致来讲只是对数据库的连接做了封装 工作流程 通过调用RoutingStatementHandler对象生成StatemenetHandler RoutingStatementHandler查找相应的statementHandler对象 statementHandler调用数据库的方法 ParameterHandler作用 对预编译的参数进行设置 工作流程 从parameterObject中取到参数，然后使用typeHandler（注册在Configuration中）进行参数处理 ResultSetHandler作用 组装结果返回结果集 运行过程总结 prepare预编译 parameterize设置参数 doUpdate/doQuery执行SQL 神人的总结 参考链接 Mybatis缓存（1）——–系统缓存及简单配置介绍 Mybatis的SqlSession运行原理 MyBatis从入门到放弃六：延迟加载、一级缓存、二级缓存]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的锁]]></title>
    <url>%2F2020%2F01%2F09%2Fconcurrent5%2F</url>
    <content type="text"><![CDATA[《并发编程的艺术》阅读笔记第五章，图文绝配 你锁我，我锁你，两者互不相让，然后就进入了死局，这像极了爱情。 一、Lock接口提供了synchronized不具有的特性： 尝试非阻塞地获取锁：tryLock()，调用方法后立刻返回 能被中断地获取锁：lockInterruptibly():在锁的获取中可以中断当前线程 超时获取锁:tryLock(time,unit)，超时返回 Lock接口的实现基本都是通过==聚合了一个同步器的子类来完成线程访问控制的。== 使用注意事项 unlock方法要在finally中使用，目的保证在获取到锁之后，最终能被释放 lock方法不能放在try块中，因为如果try catch抛出异常，会导致锁无故释放 二、队列同步器 队列同步器AbstractQueuedSynchronizer是用来构建锁或其他同步组件的基础框架。它使用一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。 同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态。同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态,这样就可以方便实现不同类型的同步组件（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等） ==同步器==是实现锁的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。理解两者的关系： 锁是面向使用者的，它定义了使用者与锁交互的接口，隐藏了实现细节； 同步器是面向锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待和唤醒等底层操作。 队列同步器的接口与示例 同步器的设计是基于模板方法模式的，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态 getState()：获取当前同步状态。 setState(int newState)：设置当前同步状态。 compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。 同步器提供的模板方法基本上分为3类：==独占式获取与释放同步状态==、==共享式获取与释放同步状态==和==查询同步队列中的等待线程情况==。自定义同步组件将使用同步器提供的模板方法来实现自己的同步语义。 工作原理1MUtux源代码略 队列同步器的实现分析1.同步队列通过一个FIFO双向队列来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成一个Node并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。 首节点是获取同步状态成功的节点，首节点在释放同步状态时，会唤醒后继节点，而后继节点在获取同步状态成功时将自己设置为首节点。 节点的属性类型与名称以及描述 2.独占式同步状态获取和释放123456 public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp;acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //如果上述操作失败，则阻塞线程 selfInterrupt(); &#125;//上述代码主要完成了同步状态获取、节点构造、加入同步队列以及在同步队列中自旋等待的相关工作， 代码分析：首先尝试获取同步状态，如果获取失败，构造独占式同步节点(独占式 Node.EXCLUSIVE)并将其加入到节点的尾部，然后调用acquireQueued，使节点一死循环的方式去获取同步状态，如果获取不到就阻塞节点中的线程。 两个死循环：入队、入队后 addWaiter和enq方法· 在“死循环”中只有通过CAS将节点设置成为尾节点之后，当前线程才能从该方法返回，否则，当前线程不断地尝试设置。可以看出，enq(final Node node)方法将并发添加节点的请求通过CAS变得“串行化”了 1addWaiter方法尝试快速添加，但是存在出现并发导致节点无法正常添加成功（获取尾节点==null），因此enq方法无限循环添加节点，将节点加入到尾部 acquireQueued方法 ·==只有前驱节点是头结点才能尝试获取同步状态==，原因： 头结点是成功获取到同步状态的节点，而头结点的线程释放了同步状态后，将会唤醒其后继节点，后继节点的线程被唤醒后需要检查自己的前驱节点是否为头节点。 维护同步队列的FIFO原则。节点之间互不通信，便于对过早通知的处理（过早通知是指前驱节点不是头节点的线程由于中断而被唤醒） 释放同步状态使用release方法 12345678910public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0 //unparkSuccessor(Node node)方法使用LockSupport（在后面的章节会专门介绍）来唤醒处于等待状态的线程 unparkSuccessor(h); return true; &#125; return false;&#125; 总结：在获取同步状态时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋，移出队列（停止自旋）的条件是前驱节点是头结点且成功获取了同步状态。在释放同步状态时，同步器调用tryRelease方法释放同步状态，然后唤醒头结点的后继节点 3.共享式同步状态获取和释放主要区别：同一时刻是否有多个线程同时获取到同步状态 共享式访问资源时，其他共享式的访问均被允许，而独占式访问被阻塞。独占式访问资源时，同一时刻其他访问均被阻塞。 tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态 releaseShared·方法和独占式主要区别在于tryReleaseShared(int arg)方法必须确保同步状态（或者资源数）线程安全释放，一般是通过循环和CAS来保证的，因为释放同步状态的操作会同时来自多个线程。 12源码细节还是有很多没有看懂比如interrupt的放置时机，如何保证共享的时候线程安全性、共享获取同步状态中的传播和信号都是什么意思。。。 4.独占式超时获取同步状态 通过调用同步器的doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true，否则，返回false。该方法提供了传统Java同步操作（比如synchronized关键字）所不具备的特性。 响应中断的同步状态获取过程 在Java 5中，同步器提供了acquireInterruptibly(int arg)方法，这个方法在等待获取同步状态时，如果当前线程被中断，会立刻返回，并抛出InterruptedException。 doAcquireNanos(int arg,long nanosTimeout)方法在支持响应中断的基础上，增加了超时获取的特性。针对超时获取，主要需要计算出需要睡眠的时间间隔nanosTimeout，为了防止过早通知，nanosTimeout计算公式为：nanosTimeout-=now-lastTime，其中now为当前唤醒时间，lastTime为上次唤醒时间，如果nanosTimeout大于0则表示超时时间未到，需要继续睡眠nanosTimeout纳秒，反之，表示已经超时· 12345独占式超时获取同步状态和独占式获取同步状态流程上非常相似主要区别在于未获取到同步状态时的处理逻辑。acquire(int args)在未获取到同步状态时，将会使当前线程一直处于等待状态，而doAcquireNanos(int arg,long nanosTimeout)会使当前线程等待nanosTimeout纳秒，如果当前线程在nanosTimeout纳秒内没有获取到同步状态，将会从等待逻辑中自动返回当时间小于等于一个超时自旋门限时则不再进行超时等待，而是进入快速的自旋过程 三、重入锁（ReentrantLock） synchronized关键字隐式地支持重入 ReentrantLock不像synchronized隐式支持，在调用lock方法时，已经获取到锁的线程，能够再次调用lock方法获取锁而不被阻塞。 公平的获取锁，也就是等待时间最长的线程最优先获取锁，也可以说锁获取是顺序的 事实上，公平的锁机制往往没有非公平的效率高，但是公平锁的好处在于：公平锁能够减少“饥饿”发生的概率，等待越久的请求越是能够得到优先满足。 1.重入的实现两个问题： 再次获取锁 锁需要识别获取锁的线程是否为当前占据锁的线程，如果时，再次成功获取 最终释放 要求锁对于获取进行自增计数 12问题：意义何在？防止出现循环获取锁影响性能或者造成死锁 1可重入获取锁的机制，在获取的时候如果不是第一次获取，状态加一，实际上没有进行CAS操作，因此在释放锁的时候要求state为0，才能彻底释放锁 2.公平锁与非公平锁的区别：如果一个锁是公平的，那么锁的获取顺序就应该符合请求的绝对时间顺序，也就是FIFO 公平锁：CAS成功，且是队列的首节点（判断多了一层对前去前驱节点的判断） 非公平锁：CAS成功即可重入锁的默认实现是非公平锁，原因：虽然会导致饥饿，但是非公平锁的的开销少（线程切换次数少），从而可以有更高的吞吐量。 四、读写锁（ReentrantReadWriteLock） 前文中的锁基本都是排他锁，在同一时刻只允许一个线程访问。 读写所在同一时刻可以允许多个读线程访问，但在写线程访问时，所有读线程和其他写线程均被阻塞。（保证了写操作的可见性） 读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 读写锁的实现分析1.读写状态的设计依赖自定义同步器，读写锁的自定义同步器需要在同步状态（一个int值）上维护多个读线程和一个写线程的状态，高16位表示读，低16位表示写。 位运算 当前同步状态表示一个线程已经获取了写锁，且重进入了两次，同时也连续获取了两次 读锁。读写锁是如何迅速确定读和写各自的状态呢？答案是通过位运算。假设当前同步状态 值为S，写状态等于S&amp;0x0000FFFF（将高16位全部抹去），读状态等于S&gt;&gt;&gt;16（无符号补0右移16位）。当写状态增加1时，等于S+1，当读状态增加1时，等于S+(1&lt;&lt;16)，也就是 S+0x00010000。2.写锁的获取与释放 写锁是一个支持重入的排他锁，如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取或者该线程不是已经获取写锁的线程，则当前线程进入等待状态。 ==读锁存在，写锁不能获取：== 1读写锁要确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。 3.读锁的获取与释放在没有其他写线程访问时，读锁总会被成功地获取。如果写锁已经被其他线程获取，则进入等待状态。 读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是（1&lt;&lt;16）。 ==读状态的线程安全由CAS保证== 4.锁降级（写锁降级成为读锁）定义：==把持住写锁==，再获取到读锁，随后释放写锁的过程 writeLock.lock(); readLock.lock(); writeLock.unlock(); 这边不是很理解。。。。 锁降级的前提是所有线程都希望对数据变化敏感，但是因为写锁只有一个，所以会发生降级。如果先释放写锁，再获取读锁，可能在获取之前，会有其他线程获取到写锁，阻塞读锁的获取，就无法感知数据变化了。所以需要先hold住写锁，保证数据无变化，获取读锁，然后再释放写锁。锁降级中读锁获取的必要性： 为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程获取了写锁并修改了数据，那么当前线程无法感知到数据的更新.如果当前线程获取读锁，则另一个线程会被阻塞，直到当前线程使用数据并释放锁之后，另一个线程才能获取写锁进行数据更新。五、LockSupport工具 LockSupport定义了一组的公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具 在Java 6中，LockSupport增加了park(Object blocker)、parkNanos(Object blocker,long nanos)和parkUntil(Object blocker,long deadline)3个方法，用于实现阻塞当前线程的功能，其中参数blocker是用来标识当前线程在等待的对象（以下称为阻塞对象），该对象主要用于问题排查和系统监控。 六、Condition接口 Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式，但是这两者在使用方式以及功能特性上还是有差别的 1.Condition接口和示例 Condition在调用方法之前先获取锁 12在添加和删除方法中使用while循环而非if判断，目的是防止过早或意外的通知，只有条件符合才能够退出循环。回想之前提到的等待/通知的经典范式，二者是非常类似的。 2.Condition的实现分析 ConditionObject是同步器AbstractQueuedSynchronizer的内部类，因为Condition的操作需要获取相关联的锁，所以作为同步器的内部类也较为合理。每个Condition对象都包含着一个队列（以下称为等待队列），该队列是Condition对象实现等待/通知功能的关键。 等待队列 等待队列是一个FIFO的队列，在队列中的每个节点都包含了一个线程引用，该线程就是在Condition对象上等待的线程，如果一个线程调用了Condition.await()方法，那么该线程将会释放锁、构造成节点加入等待队列并进入等待状态（和同步队列类似） 在Object的监视器模型上，一个对象拥有一个同步队列和等待队列，而并发包中的Lock（更确切地说是同步器）拥有一个同步队列和多个等待队列 上述节点引用更新的过程并没有使用CAS保证，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全 等待 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。 1234源码略1. 使用locksupport中的park()方法进入等待状态，判断是否唤醒节点的标志是查看节点是否在同步队列上，因为通知condition在唤醒节点之前后将节点转移到同步队列上唤醒后注意还有判断唤醒方式是通知还是中断2. 从队列角度，线程加入Condition的等待队列实质是构造了新的节点加入等待队列 通知 调用Condition的signal()方法，将会唤醒在等待队列中等待时间最长的节点（首节点），在唤醒节点之前，会将节点移到同步队列中。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程间通信]]></title>
    <url>%2F2020%2F01%2F07%2Fconcurrent4%2F</url>
    <content type="text"><![CDATA[Java并发编程基础《并发编程的艺术》阅读笔记第四章，重点在于线程间的通信 一、线程简介首先对线程做一个简单的介绍 使用多线程的原因： 1.更多的处理器核心：一个线程在一个时刻只能运行在一个处理器核心上 2.更快的响应时间 3.更好的编程模型JAVA程序运行所需线程（jdk1.8） 线程优先级 操作系统基本采用时分的形式调度运行的线程，操作系统会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配，线程分配到的时间片多少就决定了线程使用处理器资源的多少。 有些操作系统会忽略对线程优先级的设定 在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程 线程的状态 6种，在给定的时刻只能处于一种状态 NEW：初始状态，线程被构建，但还没有调用start方法 RUNNABLE：运行状态，java线程将==就绪和运行==两种状态统称为运行状态 ``BLOCKED`：阻塞状态，表明线程阻塞于锁 WAITING：等待状态，等待其他线程的通知或中断 TIME_WAITING：超时等待状态，可以在指定的时间自行返回 TERMINATED：终止状态，表示当前线程已经执行完毕 线程状态变迁 Java将操作系统中的运行和就绪两个状态合并称为==运行状态==。 ==阻塞状态==是线程阻塞在进入synchronized关键字修饰的方法或代码块（获取锁）时的状态 但是==阻塞==在java.concurrent包中Lock接口的线程状态却是等待状态，因为java.concurrent包中Lock接口对于阻塞的实现均使用了LockSupport类中的相关方法 Daemon线程（守护线程） 指在程序运行的时候在后台提供一种通用服务的线程，比如垃圾回收线程。不属于程序中不可或缺的部分。 当一个java虚拟机中不存在非守护线程时，虚拟机将会退出。可以通过设置Thread.setDaemon(true)将线程设置为daemon线程，必须在线程启动前设置 ps. 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑,可能不执行 二、启动和终止线程一个新构造的线程对象是==由其父线程==来进行空间分配的，而子线程的各种属性==继承自父线程，同时还会分配一个唯一的ID。== 123看Thread.init()源码jdk1.8的源码中添加了很多关于ThreadGroup的判断操作估计是对线程的安全性和异常情况做了更完善的区分 启动线程：start方法：当前线程同步告知虚拟机：只要线程规划器空闲，应立即启动调用start方法的线程，==最好在线程创建之前设置线程名称，有利于排查错误== 中断： 可以理解为线程的一个标识符属性，它标识一个运行中的线程是否被其他线程进行了中断操作。是一种简便的线程间交互方式。线程通过isInterrupted（）方法判断是否被中断，也可以调用静态方法Thread.interrupted()对当前的线程中断标识位进行复位。 从java的API可以看出，许多声明抛出InterruptedException的方法在抛出这个异常前，java虚拟机会先将该线程的中断标志位清除，然后再抛异常。此时调用isInterrupted返回false 当线程处于等待状态或者有超时的等待状态时（TIMED_WAITING，WAITING）我们可以通过调用线程的·interrupt()方法来中断线程的等待，此时线程会抛InterruptedException异常。 但是当线程处于BLOCKED状态或者RUNNABLE（RUNNING）状态时，调用线程的interrupt()方法也只能将线程的状态位设置为true。停止线程的逻辑需要我们自己去实现。 过期API:suspend(),resume(),stop() 原因：占有资源的时候容易发生死锁问题，让线程共工作在不确定状态下 终止线程的做法:一种方法可以使用interrupt()方法，或者利用一个boolean变量将线程终止，使线程在终止时有机会去清理资源 三、线程间通信（重点）1、volatile和synchronized关键字volatile:告知程序任何对该变量的访问均需要从共享内存中获取，而对他的改变必须同步刷新回主内存，他能保证所有线程对变量访问的可见性。 synchronized：确保多个线程在同一个时刻，只能有一个线程处于方法或同步块中，保证了线程对变量访问的可见性和排他性。不能保证重排序关于synchronized：本质是对一个对象的监视器（monitor）的获取，而这个获取过程是排他的，也就是同一时刻只能有一个线程获取到有synchronized所保护对象的监视器。任意一个对象都拥有自己的监视器。 任意线程对Object的访问，首先要获得Object的监视器。如果获取失败，线程进入同步队列，线程状态变为BLOCKED。当访问Object的前驱（获得了锁的线程）释放了锁，则该释放操作唤醒阻塞在同步队列中的线程，使其重新尝试对监视器的获取。 2、等待/通知机制（生产者-消费者模型） 等待/通知机制，是指一个线程A调用了对象O的wait方法进入等待状态，而另一个线程B调用了对象O的notify或notifyall方法，线程A收到通知后从对象O的wait方法返回，进而执行后续操作。 123此处的代码示例失效，wait线程没有正常被唤醒，也没有发生重排序原因时因为获取的锁不是同一个锁，因此导致无法正常解锁和加锁，需要把同步的对象设置为public static确保获得对象是同一个///示例不是这个问题，是因为一开始没有wait...、😂 notify无法正常唤醒wait方法的问题 注意： （1）使用wait、notify、notifyAll时需要先对调用对象加锁 （2）调用wait方法后，线程状态由RUNNING变为WAITING，并将当前线程防止到对象的等待队列 （3）notify或notifyAll方法调用后，等待线程依旧不会从wait返回，需要调用notify或notifyAll的线程释放锁之后，等待线程才有机会从wait返回 （4）notify方法将等待队列中的等待线程从等待队列中移到同步队列中，被移动的线程从WAITING变为BLOCKED （5）从wait方法返回的前提是获得了调用对象的锁 3、等待/通知经典范式12345678910111213141516171819等待方：1.获取对象的锁 2.如果条件不满足，那么调用对象的wait方法，被通知后仍要检查条件 3.条件满足则执行对应的逻辑 伪代码： synchronized(对象)&#123; while(条件不满足)&#123; 对象.wait(); &#125; 对应的处理逻辑 &#125;通知方：1.获得对象的锁 2.改变条件 3.通知所有等待在对象上的线程 伪代码： synchronized(对象)&#123; 改变条件 对象.notify(); &#125; 4、管道输入输出流 管道输入/输出流和 普通的文件输入/输出流 或者 网络输入/输出流 不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存 PipedOutputStream、PipedInputStream(面向字节数据) PipedReader、PipedWriter(面向字符) 对于piped类型的流，使用时必须先调用connect方法进行绑定，否则会抛出异常 管道输入缓冲区大小默认1024字节 管道的详细解析 123写示例代码的注意事项：1. write写入 read读出2. 打印的时候用的是System.out.print 而不是 System.out.prineln.后者会最后加回车 5、Thread.join()含义：当前线程等待thread线程终止之后才从thread.join返回。另外还有两个join(long millis)和join(long millis,int nanos)具备==超时特性==的方法（如果线程在给定的时间内没有终止，那么将会从该超时方法返回）。 123join()等价于wait(0)，只要等指定线程锁释放就可以抢占锁如果指定时间后，就需要指定时间之后才能重新拥有锁（wait加参数本质上还是定时调用notify()方法，只是这段代码在JVM里面，比较底层）join如果加参数，第一次代表按照参数设置wait唤醒时间，之后循环检测线程活性，如果线程始终存活，才开始传入设置值和当前时间的差值 6、ThreadLocal ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。 是一个线程安全的局部变量 四、应用实例一、等待超时模式 调用一个方法时等待一段时间（一般来说是给定一个时间段），如果该方法能够在给定的时间段之内得到结果，那么将结果立刻返回，反之，超时返回默认结果。 12345678910public synchronized Object get(long mills) throws InterruptedException &#123; long future = System.currentTimeMillis() + mills; long remaining = mills; // 当超时大于0并且result返回值不满足要求 while ((result == null) &amp;&amp; remaining &gt; 0) &#123; wait(remaining); remaining = future - System.currentTimeMillis(); &#125; return result;&#125; 典型案例：数据库连接池模式 1源码略 CountDownLatch： countDownLatch这个类使一个线程等待其他线程各自执行完毕后再执行。 是通过一个计数器来实现的，计数器的初始值是线程的数量。每当一个线程执行完毕后，计数器的值就-1，当计数器的值为0时，表示所有线程都执行完毕，然后在闭锁上等待的线程就可以恢复工作了。 二、线程池技术 客户端可以通过execute(Job)方法将Job提交入线程池执行，而客户端自身不用等待Job的执行完成。除了execute(Job)方法以外，线程池接口提供了增大/减少工作者线程以及关闭线程池的方法。 这里工作者线程代表着一个重复执行Job的线程，而每个由客户端提交的Job都将进入到一个工作队列中等待工作者线程的处理。 线程池的本质就是使用了一个线程安全的工作队列连接工作者线程和客户端线程，客户端线程将任务放入工作队列后便返回，而工作者线程则不断地从工作队列上取出工作并执行 典型案例：实现一个web服务器 1源码略]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年终总结]]></title>
    <url>%2F2020%2F01%2F05%2Fyear-end-summery%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-qqqWfHgy"), narrow: false, autoplay: false, showlrc: false, music: { title: "Love Yourself", author: "J R Price", url: "http://music.163.com/song/media/outer/url?id=1381583209.mp3", pic: "http://p2.music.126.net/wR8cPX0w4gj179SCzUHI6A==/109951164258264118.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 9102年….然而已经是2020年了，我还是没有养成周记月记的习惯。。本来以为只会拖几天的十月总结拖到了十一月，又从十一月拖到了十二月…最终顺利跨过新年，从月度总结上升成了年度总结。。。约定好的事情一定要做到，这是我要说的第一件事，也是2020年我的第一个FLAG。 时间过的太快的了，尽管我每天都在感叹时间的飞逝，但还是眼睁睁地看着时间一点点从我手中流走。确实，我心太大了，想了太多事情但是完成的却寥寥无几，以为2019年世界会发生翻天覆地的变化，但实际上，自己还是那个自己，没有什么突飞猛进的成就，也没有脱胎换骨的改变，只能说在人生的轨迹上又向前滑行了一年。似水流年，一如既往，直至今日我依然在为自己的选择而犹豫，年初的豪言壮语好像也成为了自我吹嘘的空言，但是平淡归平淡，对我而言。依然是极其重要和充满意义的一年。 2019の生活年初,我上大二下，寒假还在ICT进行学习，当时的我刚刚从组织部离职中解脱出来，经历了一系列的心理问题，最终还是学会了勇敢面对，并决定要往技术方向发展（尽管那个时候对到底走什么技术还是没有很明确的概念） ​ 四月，第二次参加校运会，这次的状态大不如前，完全没有了大一时候的斗志，无论从增重十斤的身体还是从抗拒跑步的心态都已经显示了自己对求胜欲望的衰退，最后的结果可想而知，自己在成绩上大幅退步，我们学院也是七年来第一次没有拿到第一。 最后还是和大一期望的一样参加了市级的大学生运动会，但是意料之中没有取得名次，一来是自己的身体素质确实不比大一的时候，二来是自己抗拒跑步的情绪在训练中得到了最大的释放，训练能不去就不去，就算去了也没有很认真的训练，因此成绩始终没有得到提高。大运会也是我目前为止运动的顶点，之后的生活虽然和田径队依然有交集，但是自己的训练量还是因为自己的学习重心和目标的设定慢慢下降，2020年的大运会大概率就不会去了，所以，最后的辉煌，依然值得纪念。（平常依然要锻炼啊喂！！） 五月劳动节假期，父母自从把我送到重庆后第一次来看我，这一次，我做了充足的计划，虽然最后很多安排还是没有实现，但总体还是过的非常开心。 大学室友，一辈子都不会忘记的朋友。尽管有的时候还是会嫉妒他们的才能，但正是这种”妒忌心”督促着我追赶他们的脚步 暑假刚回来，成为了班学长，大学里二度体会到初入大学的青涩时光。 记录一次5G进校园，宣传噱头大于实际效果。 大三上才体会到了电影进校园的“大学生待遇”，看了罗小黑 ICT战队举办了一次近乎成功的招新会，从此从学员角度看问题上升到了管理者角度看问题，其中给我最大启示的是青松，作为我们之中的核心，凝聚了我们管理团队，同时在能力上做了表率，平时看他专注学习，还以为他会不闻窗外事，事实上，他知道的，了解的，认识的比我们之中任何人都广泛，深刻。这是我所要学习的东西 225一次任性的出行，体验了我2019最完美的一天（或半天） ict管理组团建 第一次参加异地比赛，虽然奖小比赛规模小，但是还是很有意义（形式大于内容） 第一次参加大厂宣讲会，遗憾没有体验大厂笔试题，虽然我一直想表示自己对这些的漠视，但内心深处还是非常渴望（尽管知道自己就算拥有这次机会，凭现在的能力依然无法应试）所以，加油吧！！ 此外还有就业中心助理团，匆匆走来又匆匆离去，是对自己选择不慎重的回应，好在算是和平分手，没有给双方带来更大的遗憾。 2019の学习大二下依然在红岩做项目，但是却因为自己意志不坚定导致工作的投入不够。当时的我迷茫于自己的未来方向，自卑于自己的逻辑能力不足从而开始抗拒深入学习计算机能力，一个领奖工具，一个啦啦队投票都没有得到完美稳定实现。 暑假包括大三前半段全身心投入ICT学习，但是学习的进度依然不达标，一来是自己的态度有点松懈，导致学习的进度放缓，另外则是学习的方法上，死钻一点不懂变通导致时间大把浪费。最后一个暑假的最大成果就是考过了一个HCNP 什么时候开始决定想走开发方向的?大概是加入冷刃的时候，那个时候其实已经有了相关的念头，但是决心不够，直到冷刃的创始人啸凡学长画了“大饼”才让我最后下定决心。什么是最完美的大饼？就是别人知道这个是大饼还吃得津津有味，这大概就是营销的最高境界了吧，不得不说这次我是真的上头了。 但是尽管到2019结束，我在这方面所做的努力依然有限，虽然一直以自己课多，内容难做借口，但是自己清楚，其实还是有很多零散时间可以利用起来学习。。。不管怎样，现在所有阻碍已经几乎消失了，你没有理由了吧？ 总结2019有很多难忘的经历，但是更多暴露的是心态，选择上的缺点，致使在这关键的一年里仍然遗憾多多，但我不希求命运，一切靠自己争取。只要你拼了，机会就会出现。]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型]]></title>
    <url>%2F2019%2F12%2F29%2Fconcurrent3%2F</url>
    <content type="text"><![CDATA[《并发编程的艺术》阅读笔记第三章,先从底层讲起。 基础不牢，地动山摇。 一、基础1、并发的两个关键问题线程间通信和线程间同步 线程通信机制： ==共享内存==：隐式通信，显式同步 ==消息传递==：显式通信，隐式同步 Java的并发采用的是共享内存模型。 2、java内存结构 java中所有的实例域、静态域和数组元素都存储在堆内存中；局部变量、方法定义参数、异常处理器参数不会在线程之间共享 JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程读/写共享变量的副本。 本地内存是JMM的一个抽象概念，并不真实存在，涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化 线程A与线程B之间要通信必须经过两个步骤： 线程A把本地内存A中更新过的共享变量刷新到主内存中去 线程B到主内存中去读取A之前已更新过的共享变量 JMM通过控制主内存与每个线程的本地内存之间的交互来为java程序员提供内存可见性保证 3、指令重排序 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 java最终执行的指令序列: 源码——–》编译器优化重排序——-》指令级并行重排序———-》内存系统重排序———》最终的指令序列 第一个属于==编译器重排序==，后面两个属于==处理器的重排序==,JMM通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证 对于编译器，JMM的编译器会禁止特定类型的编译器重排序。==对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定的内存屏障指令==，从而禁止特定类型的处理器重排序。 4、内存屏障分类load：读缓冲区 store：写缓冲区 四种内存屏障： LoadLoad Barriers：确保load1的数据先于laod2及后续所有load指令进行装载 StoreStore Barriers：确保store1的数据对其他处理器的可见性先于store2及后续所有存储指令 LoadStore Barriers：确保load装载先于store的存储刷新到内存 StoreLoad Barriers：该屏障前的指令全部完成之后才会执行后面的指令（开销大） 5、先行发生（happens-before）1程序员必须遵循的编程原则 ==JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。== 其中一些顺序规则（针对程序员）： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前，，并不意味着前一个操作必须要在后一个操作之前执行！ 123前一个要求对后一个操纵可见不是意味着前一个操作必须在后一个操作之前执行？JMM要求如果A happens-before B那么首先必须保证执行结果（A的执行结果对B **不一定可见** ）必须和逻辑中的happens-before相同，其次，A的排序顺序在B之前（这里是指重排序之前的顺序，后期允许通过重排序调整A和B的操作顺序），但是最终结果必须和逻辑上的保持一致，因此从表面上看是按序执行了 二、重排序数据依赖性 在单线程程序中，对存在控制依赖的操作重排序不会改变执行结果；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 详见30页的例子as-if-serial 不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义 三、顺序一致性内存模型 如果程序是正确同步的，程序的执行将具有顺序一致性（Sequentially Consistent）——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 未同步程序在顺序一致性模型中给虽然整体执行顺序是无序的但是所有线程都只能看见一个一致的的整体执行顺序。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见 JMM就没有这个保证。因为只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。 JMM的处理逻辑：临界区内的代码可以重排序；JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图 JMM的具体实现的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门 顺序一致性内存模型和JMM区别 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。 ==JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。== 处理器总线工作机制 在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的 总线事务包括读事务（Read Transaction）和写事务（Write Transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字 总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其他的处理器和I/O设备执行内存的读/写 (从而保证当单个总线事务之中的内存读写具有原子性) 在一些32位的处理器上，如果要求对64位数据的写操作具有原子性，会有比较大的开销。==当JVM在这种处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行== 注意，在JSR-133之前的旧内存模型中，一个64位long/double型变量的读/写操作可以被拆分为两个32位的读/写操作来执行。从JSR-133内存模型开始（即从JDK5开始），仅仅只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作在JSR-133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。 四、volatile内存语义volatile变量特性： 可见性：对一个volatile变量的读，总是能看到（任意线程）对这个变量最后的写入 原子性：对任意单个volatile变量的读、写具有原子性（包括long、double），但类似volatile++ 这种复合操作不具有原子性。volatile写-读的内存语义： 写：当写一个volatile变量时，JMM会把线程对应的==本地内存中的共享变量值刷新到主内存== 读：当读一个volatile变量时，JMM会把该线程==对应的本地内存置为无效==。线程接下来从主内存中读取共享变量 123volatile读的时候必须保证主内存中的数据不会更改，因此volatile读如果是第一个操作怎不能实现重排序volatile写的时候必须保证之前的数据能正常写入主内存，因此volatile写如果是第二个操作的话不能实现重排序普通读写是在本地内存，但是有概率会写入主内存，因此具有随机性 volatile内存语义的实现为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 JMM内存屏障插入策略(保守!)： 在每个volatile写操作前面插入StoreStore屏障 在每个volatile写操作后面插入StoreLoad屏障 在每个volatile读操作后面插入一个LoadLoad、一个LoadStore 12为什么volatile写操作之前不用插入loadstore来避免普通读和volatile写之间的重排序？猜测：内存屏障之间存在包含关系，比如storeload就可以实现其他三个所有功能 volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升 实际情况编译器在生成字节码的时候可以优化选择是否添加内存屏障，但是注意一般最后一个storelaod不能省略，因为无法判断return后是否还会有下一个volatile读/写出现 不同的处理器平台也会对内存屏障做出优化 在功能上，锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势 五、锁的内存语义锁的释放和获取的内存语义（和volatile一样） 线程释放锁时，会把本地内存中的共享变量刷新到主内存中（对应volatile写） 线程获取锁时，会将线程对应的本地内存置为无效，从而临界区代码必须从主内存读取共享变量（对应volatile读） 锁内存语义的实现： 分析ReentrantLock源码 ReentrantLock的实现依赖于Java同步器框架AbstractQueuedSynchronizer（本文简称之为AQS）。AQS使用一个整型的volatile变量（命名为state）来维护同步状态，这个volatile变量是ReentrantLock内存语义实现的关键。 ReentrantLock锁分为公平锁和非公平锁 公平锁 1现在tryAcqyuire方法有点变化，在查看是不是第一个获取锁的对象处添加了hasQueuedPredecessors()方法，一个用来查询是否有线程比现在线程等待时间更长 非公平锁调用CAS：如果当前状态值等于预期值，则以原子方式将同步状态设置为给定的更新值。 公平锁和非公平锁语义总结： 公平锁和非公平锁释放时，==最后都要写一个volatile变量state== 公平锁获取时，==首先会去读volatile变量== 非公平锁获取时，首先会==用CAS更新volatile变量，这个操作同时具有volatile读和volatile写的内存语义== 可以看出：锁释放-获取的内存语义的实现至少有下面两种方式 利用volatile变量的写-读所具有的内存语义 利用CAS所附带的volatile读和volatile写的内存语义 CAS是如何同时具有volatile读和volatile写的内存语义的？ 多处理器环境，会为cmpxchg指令加上lock前缀，单处理器不用加（单处理器会维护自身的顺序一致性） 原先使用总线锁定，后来考虑开销使用缓存锁定，大大降低lock前缀指令的执行开销 lock指令能防止重排序 把写缓冲区中的所有数据刷新到内存中 上面2、3两点具有的内存屏障效果，足以同时实现volatile读和volatile写的内存语义 concurrent包的通用实现模式 ==首先，声明共享变量为volatile== ==然后，使用CAS的原子条件更新来实现线程之间的同步== ==同时，配合以volatile的读、写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信== 六、final域的内存语义1、final域的重排序规则（1）写：在==构造函数内对==一个final域的写入，与==随后把这个被构造对象的引用赋值给一个引用变量==，这两个操作不能重排序 （2）读：初次读一个包含final域的对象的引用，与==随后初次读这个final域==，这两个操作不能重排序。 2、写final域的重排序规则==禁止把final域的写重排序到构造函数之外==包含两方面： 1、编译器： JMM禁止编译器把final域的写重排序到构造函数之外 2、处理器： 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外上述规则可以确保： 在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域不具有这个保障。 3、读final域的重排序规则处理器：在一个线程中，初次读对象引用与初次读该对象所包含的final域，JMM禁止处理器重排序这两个操作 编译器：编译器会在读final域操作的前面插入一个LoadLoad屏障· 上述重排序规则可以确保：在读一个对象的final域之前，一定会先读包含这个final域的对象的引用（针对少数处理器存在间接依赖关系的操作做重排序） 4、当final域为引用类型对于引用类型，写final域的重排序规则增加下面的约束： 在构造函数内对一个final引用对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作不能重排序。对于存在数据竞争的线程无法保证可见性 5、为什么final域不能从构造函数内溢出在构造函数返回前，被构造对象的引用不能为其他线程所见，因为此时的final域可能还没有初始化 (构造函数内可能发生重排序)。 ==(在引用变量为任意线程和可见之前，引用变量所指向的对象的final域必须要正确初始化,这是写final域的重排序规则)== 七、happens-beforeas-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序执行的。 happens-before关系给编写正确同步的多线程程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序执行的。 这么做的目的：为了在不改变程序的执行结果的前提下，尽可能地提高程序执行的并行度。 完整hgappen-before规则 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回 八、双重检查锁定与延迟初始化双重检查锁定 问题描述：(一般是单例模式)在上锁之前加一个非空判断，上锁后再次非空判断，实现双重检查锁定 原因：在构造实例时，对象引用指针的操作和初始化操作可能会被重排序，这就导致在if(instance==null)的时候认为对象已经创建，但这个时候还没有进行初始化 1.分配对象的内存空间 2.初始化对象 3.设置instance指向内存空间 4.初次访问对象 3和2可能会被重排序，导致1342这样的问题 一开始单例模式一旦出现并发，就可能出现初始化两个对象的问题 后来选择加锁，加锁能解决问题，但是出现严重的性能开销 后来就选择在加锁前加一层非空判断，这样就可以只在第一次初始化，后期不用加锁但 是正是由于添加了一层非空判断，导致多线程在进行这个判断的时候，读取操作跳过了等待时间直接读取对象，但此时由于锁内空间的重排序，导致此时对象还没有初始化完成。从而造成严重的后果 解决方式： volatile。利用volatile的语义禁止重排序。在单例的懒汉模式中，必须给实例添加volatile修饰符 优势: ==除了可以对静态字段实现延迟初始化外，还可以对实例字段实现延迟初始化。== 基于类初始化的解决方案,使得线程访问之前就完成初始化 JVM在类的初始化阶段（即在Class被加载后，且被线程使用之前），会执行类的初始化。在执行类的初始化期间，JVM会去获取一个锁。这个锁可以同步多个线程对同一个类的初始化。 123456备注：类初始化的几种情况1）T是一个类，而且一个T类型的实例被创建。2）T是一个类，且T中声明的一个静态方法被调用。3）T中声明的一个静态字段被赋值。4）T中声明的一个静态字段被使用，而且这个字段不是一个常量字段。5）T是一个顶级类（Top Level Class，见Java语言规范），而且一个断言语句嵌套在T内部被执行。 JVM初始化期间的同步过程初始化阶段 第1阶段：通过在Class对象上同步（即获取Class对象的==初始化锁==），来控制类或接口的初始化。这个获取锁的线程会一直等待，直到当前线程能够获取到这个初始化锁。 ==此时只有一个线程给能获得初始化锁== 第2阶段：线程A执行类的初始化,，同时线程B在初始化锁对应的condition上等待 初始化instance过程(==可能发生重排序，但是对于其他线程不可见==) 分配内存空间 复制给引用变量 初始化对象 第3阶段：线程A设置初始化完成标志，然后唤醒在condition中等待的所有线程。 第4阶段：线程B结束类的初始化处理。 线程A在第2阶段的A1执行类的初始化，并在第3阶段的A4释放初始化锁； 线程B在第4段的B1获取==同一个初始化锁==，并在第4阶段的B4之后才开始访问这个类 第5阶段：线程C执行类的初始化的处理。 线程A在第2阶段的A1执行类的初始化，并在第3阶段的A4释放锁；`` 线程C在第5阶段的C1获取==同一个锁==，并在在第5阶段的C4之后才开始访问这个类 九、JAVA内存模型综述处理器内存模型常见 处理器内存模型 Total Store Ordering内存模型(TSO) 放松程序中==写-读==操作的顺序 Total Store Ordering内存模型（PSO) 继续放松程序中==写-写==操作的顺序 Relaxed Memory Order内存模型（RMO） 继续放松程序中==读-写==操作的顺序 PowerPC内存模型 继续放松==读-读==操作的顺序 所有处理器内存模型都允许写-读重排序，原因：它们都使用了写缓存区. 写缓存区可能导致写-读操作重排序 由于写缓存区仅对当前处理器可见，这个特性导致当前处理器可以比其他处理器先看到临时保存在自己写缓存区中的写。 内存模型之间关系同处理器内存模型一样，越是追求执行性能的语言，内存模型设计得会越弱。 JMM的内存可见性保证按程序类型可分为 单线程程序： 不会出现内存可见性问题 正确同步的多线程程序 具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同） 未同步/未正确同步的多线程程序 JMM为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0、null、false）。 但最小安全性并不保证线程读取到的值，一定是某个线程写完后的值。==最小安全性保证线程读取到的值不会无中生有的冒出来，但并不保证线程读取到的值一定是正确的==。 旧内存模型的修补JDK1.5之后对volatile的内存语义、final的内存语义做了增强]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发机制的底层实现原理]]></title>
    <url>%2F2019%2F12%2F15%2Fconcurrent2%2F</url>
    <content type="text"><![CDATA[《并发编程的艺术》阅读笔记第二章。主要针对volatiile和synchronized做了总结 一、volatile的应用如果volatile变量修饰符使用得当，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 1. volatile的定义和实现原理有volatile变量修饰符的共享变量进行写操作的时候会多出一个lock前缀的指令 lock前缀的指令在多核处理器中引发两件事情 （1）将当前处理器缓存行的数据写回内存 但是锁总线开销比较大，因此现在的LOCK信号基本锁缓存，使用缓存一致性机制确保修改的原子性（缓存锁定） （2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效 MESI(修改、独占、共享、无效)控制协议维护内存和其他处理器缓存一致性 嗅探技术保证内部缓存、系统内存和其他数据缓存在总线上保持一致 为了提高处理速度，处理器不直接和内存通信，而是先将内存中的数据读到cache中再进行操作，但操作完全不知道何时会写到内存。如果对声明了volatile变量的进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再进行计算操作就会有问题。所以，多处理器下，要实行缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期，如果过期，就将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存中。 2. volatile使用优化将共享变量追加到64字节(貌似不生效了，在源码中没看到) LinkedTransferQueue 一些处理器的Cache的高速缓存行是64字节，不支持部分填充缓存行。通过追加到64字节的方式来填满高速缓冲区的缓存行，避免头结点和尾节点加载到同一个缓存行，使头、尾节点在修改时不会互相锁定。 并不是所有使用volatile变量的时候都要追加到64字节 缓存行非64字节宽的处理器不适用 共享变量不会被频繁读写的情况不适用，反而会因为追加字节导致性能消耗增加 二、synchronized的实现原理与应用（重量级锁）synchronize基础：java中的每个对象都可以作为锁。具体表现为3种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的class对象 对于同步方法块，锁是synchronize括号里的对象 锁到底存在哪里？锁里面会存储什么信息？· Synchronized在JVM中的实现原理JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的表现细节不同。 代码块同步使用monitorenter和monitorexit指令实现，而方法同步是使用另外一种实现方式实现的，细节并没有在JVM中说明。但是，方法同步同样可以使用上述两个指令实现。 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。 任何对象都有一个monitor与之关联，且当一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 同步方法:JVM使用ACC_SYNCHRONIZED标识来实现。即JVM通过在方法访问标识符(flags)中加入ACC_SYNCHRONIZED来实现同步功能。同步方法会在class文件中的access_flags中存放ACC_SYNCHRONIZED，而access_flags是存放在常量池的 同步方法是隐式的。一个同步方法会在运行时常量池中的method_info结构体中存放ACC_SYNCHRONIZED标识符。当一个线程访问方法时，会去检查是否存在ACC_SYNCHRONIZED标识，如果存在，则先要获得对应的monitor锁，然后执行方法。当方法执行结束(不管是正常return还是抛出异常)都会释放对应的monitor锁。如果此时有其他线程也想要访问这个方法时，会因得不到monitor锁而阻塞。当同步方法中抛出异常且方法内没有捕获，则在向外抛出时会先释放已获得的monitor锁 monitor 管程 (英语：Monitors，也称为监视器) 是一种程序结构，结构内的多个子程序（对象或模块）形成的多个工作线程互斥访问共享资源。 这是一个概念，目的是为了简化同步调用的过程，封装了同步操作，避免直接使用PV信号量。在java中的具体实现就是ObjectMonitor ObjcetMonitor的关键字段 _count：记录owner线程获取锁的次数。这句话很好理解，这也决定了synchronized是可重入的。 _owner：指向拥有该对象的线程 _WaitSet：存放处于wait状态的线程队列。 _EntryList：存放等待锁而被block的线程队列。 想要获取monitor的线程先进入monitor的__EntryList队列阻塞等待 如果在程序里调用了wait()方法，则该线程进入_WaitSet队列，wait()会释放monitor锁，即将_owner赋值为null并进入_WaitSet队列阻塞等待 当程序里其他线程调用了notify/notifyAll方法时，就会唤醒_WaitSet中的某个线程，这个线程就会再次尝试获取monitor锁。如果成功，则就会成为monitor的owner。 具体实现 在博客上找一下1、java对象头synchronized用的锁是存在java对象头里的 数组类型3个字宽（3*4字节） 非数字类型2个字宽（2*4字节） mark word里存储的数据会随着锁标志位的变化而变化 64位虚拟机下的存储结构 2、锁的升级与对比 from JDK1.6 锁一共有4中状态，由低到高为：无锁、偏向锁、轻量级锁、重量级锁，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级（提高获得锁和释放锁的效率） 偏向锁：大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低，引入了偏向锁。 当一个线程访问同步块并获取锁时，会在对象头和栈帧的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单测试对象头里的mark word里是否存储着指向当前线程的偏向锁。 如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下mark word中偏向锁的标识是否设置成1：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 （1）偏向锁的撤销 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间上没有正在执行的字节码）。 首先==暂停==拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的mark word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁。 这一块还是不清楚，得上网再看看 如果另外的线程和现有线程竞争偏向锁，是如何判断是否继续偏向的？（2）关闭偏向锁 java6、7默认启用偏向锁，但是在程序启动后会有几秒延迟，如有必要可以关闭延迟-XX:BiasedLockingStartupDelay=0,如果确定程序里所有的锁通常处于竞争状态，可以通过JVM参数关闭偏向锁：-XX:UseBaisedLocking=false,那么程序会默认进入轻量级锁状态 轻量级锁（1）加锁 线程在执行同步块之前，JVM会先在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头中的Mark word复制到锁记录中（Displaced Mark World）。然后线程尝试使用CAS将对象头中的Mark word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 （2）解锁 解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。 因为自旋会消耗CPU，为了避免无用的自旋，一旦升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 3、锁的优缺点对比 偏向锁： 优点：加锁解锁不需要额外的消耗 缺点：如果线程间存在锁竞争，会带来额外的锁撤销的消耗​ 适用场景：适用于只有一个线程访问同步块的场景 轻量级锁： 优点：竞争的线程&lt;mark&gt;不会阻塞&lt;/mark&gt;，提高了程序的响应速度 缺点：如果始终得不到锁竞争的线程，使用自旋会消耗CPU 使用场景：追求**响应时间**，同步执行速度非常快 重量级锁： 优点：&lt;mark&gt;线程竞争不使用自旋，不会消耗CPU&lt;/mark&gt; 缺点：线程阻塞，响应时间慢 适用场景：追求**吞吐量**，同步块执行时间较长 自我理解 偏向锁一开始指向持有锁的线程，之后出现锁竞争后撤销偏向锁，偏向锁是否会进化到轻量级锁存疑。 轻量级锁一开始需要复制一份markworld内容（即hashcode或者其他所锁信息）到本地线程栈帧，然后markworld修改为指向线程的指针 如果出现锁竞争，markworld将膨胀为重量级锁 三、原子操作的实现原理1、术语CAS：比较并交换 缓存行：缓存的最小操作单位 内存顺序冲突：一般由假共享引起，出现内存顺序冲突时，CPU必须清空流水线 假共享：多个CPU同时修改同一个缓存行的不同部分而引起其中一个CPU的操作无效2、处理器实现原子操作（1）通过总线锁保证原子性 如果多个处理器同时对共享变量进行读改写操作（i++），那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致。 总线锁：当一个处理器在总线上输出LOCK#信号时，其他处理器的请求将被阻塞住，那么该处理器可以独占共享内存。 （2）使用缓存锁保证原子性 总线锁定把CPU和内存之间的通信锁住了，锁定期间，其他处理器不能操作其他内存地址的数据，因此总线锁定的开销比较大。 缓存锁定：内存区域如果被缓存在处理器的缓存行中，并且在LOCK期间被锁定，那么当他执行锁操作会写到内存时，处理器不在总线上声言LOCK #信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时，会使缓存行无效。 两种情况不会使用缓存锁定： 1、数据不能被缓存在处理器内部，或操作的数据跨多个缓存行，此时用总线锁定 2、有些处理器不支持缓存锁定3、Java实现原子操作（锁和循环CAS）（1）循环CAS机制 处理器的CMPXCHG指令 自旋CAS：循环进行CAS操作直至成功为止 CAS实现原子操作的三大问题： ABA问题：A到B再到A，CAS检查值时会以为没有发生变化，实际却发生了变化，解决方式是在变量前面追加版本号：1A到2B到3C AtomicStampedReference类解决方法： 循环时间长开销大：自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 只能保证一个共享变量的原子操作：此时用锁或者将几个共享变量合并 （2）锁机制 除了偏向锁，另外两种锁都使用了循环CAS机制,即当一个线程进入同步块的时候使用循环CAS的方式获取锁，当他退出同步块的时候使用循环CAS释放锁。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPv6地址解析]]></title>
    <url>%2F2019%2F12%2F10%2Fipv6%2F</url>
    <content type="text"><![CDATA[为战队上交的微信推送任务，这里做一下搬运。 据称，11月26日，负责英国、欧洲、中东和部分中亚地区互联网资源分配的欧洲网络协调中心宣布，全球所有43亿个IPv4地址已全部分配完毕。意味着再也没有新接入的互联网设备在也无法分配到新的IP地址。在目前的形势下，利益相关的同学们第一反应会想到什么呢？是提高IP地址利用率的无类域间路由还是为IPv4续命立下汗马功劳的NAT技术？ 不得不说，靠IPv4协议也就图个乐呵，想要彻底解决地址分配问题还得看我IPv6协议 今天由于篇幅有限，主要专注于IPv6的地址解析 IPv6的优势 地址空间大，理论上存在2^128个地址，几乎不用担心地址短缺问题 减少路由表大小，路由器处理报文性能提高 安全性得到提高 服务质量好，服务类型扩展灵活丰富 支持可移动主机和网络 兼容性强，可完全向下兼容IPv4 … IPv6地址IPv6地址的128位地址被分成8段，每16位一段，分别被转成4位十六进制数这种表示方法称之为“冒号十六进制表示法”。 地址格式： X:X:X:X:X:X:X:X 压缩表示 地址中包含的连续两个或多个均为0的组，可以用双冒号“::”来代替。 如FC00:0:130F:0:0:9C0:876A:130B可写作FC00:0:130F::9C0:876A:130B 注意： ::代表的0位数必须最大化； 错误示例：FC00:0:130F:0:0:9C0:876A:130B不能写作FC00:0:130F::0:9C0:876A:130B 不能只代表一个全0段； 错误示例：FC00:1:130F:0:1:9C0:876A:130B不能写作FC00:1:130F::1:9C0:876A:130B 如果多个连续全0段长度相同，必须的代表最前面的那个 错误示例：FC00:0:130F:0:0:9C0:0:0不能写作FC00:0:130F:0:0:9C0:: 前导0可以省略 如FC00:0000:130F:0000:0000:09C0:876A:130B可以写作FC00:0:130F:0:0:9C0:876A:130B 地址结构 一个IPv6地址可以分为如下两部分： 网络前缀：n比特，相当于IPv4地址中的网络ID 接口标识：128-n比特，相当于IPv4地址中的主机ID 表示形式如FF00::/8代表地址/前缀长度 接口标识可通过三种方法生成：手工配置、系统通过软件自动生成或IEEE EUI-64规范生成。其中，EUI-64规范自动生成最为常用 地址分类 单播地址 一对一 常见的单播地址 未指定地址::/128 该地址可以表示某个接口或者节点还没有IP地址，可以作为某些报文的源IP地址 环回地址::1/128 就是本地地址 全球单播地址:带有全球单播前缀,目前已经分配的全球路由前缀的前3bit均为001 带有全球单播前缀的IPv6地址，其作用类似于IPv4中的公网地址 链路本地地址 ：使用了特定的本地链路前缀FE80::/10 链路本地地址是IPv6中的应用范围受限制的地址类型，只能在连接到同一本地链路的节点之间使用 唯一本地地址：前缀固定为·FC00::/7 也是一种应用范围受限的地址，它仅能在一个站点内使用，类似于IPv4中的私网地址 组播地址 IPv6的组播与IPv4相同，用来标识一组接口，一般这些接口属于不同的节点. IPv6组播地址的前缀是FF00::/8 标志用来标识组播地址身份 scop用来限制组播数据流在网络中发送的范围 group ID标识组播组 任播地址 任播地址标识一组网络接口（通常属于不同的节点）。目标地址是任播地址的数据包将发送给其中路由意义上最近的一个网络接口。主要用来在给多个主机或者节点提供相同服务时提供冗余功能和负载分担功能。 注：IPv6任播地址仅可以被分配给路由设备，不能应用于主机。任播地址不能作为IPv6报文的源地址 整理了一下一个IPv6主机上一个接口可以具备的常见IPv6地址]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>ICT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程的挑战]]></title>
    <url>%2F2019%2F12%2F07%2Fconcurrent1%2F</url>
    <content type="text"><![CDATA[开坑！从今天起开始看《并发编程的艺术》，这本书是经过学长的推荐才得知的，正好能让我对并发编程有更深入的了解。其实项目中已经开始遇到并发问题了，但是这方面一直没有得到系统的学习。现在忙里偷闲，赶紧补上。PS: 特意找了一张和锁相关的图片，同时也寓意着精诚所至，金锁为开🔒 一、上下文切换即使是单核处理器也支持多线程执行代码，CPU通过给每个线程分配CPU时间片（一般为几十毫秒）实现这个机制。 当前任务执行一个时间片后会切换到下一个任务。在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 如何减少上下文切换？ 无锁并发编程。多线程竞争锁时，会引起上下文切换，可以用一些办法避免使用锁，如将数据的id取模分段，不同的线程处理不同段的数据 CAS算法：Java的Atomic包使用CAS算法来更新数据，不需要加锁 使用最少线程，避免创建不需要的线程 在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 二、死锁避免死锁的几个常见方法： 避免一个线程获取多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源 尝试使用定时锁，使用lock.tryLock(timeout)来替代使用内部锁机制 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况 三、资源限制的挑战问题：如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 如何解决： 对于硬件资源的限制，使用机群并行执行程序，通过搭建服务器集群，不同的机器处理不同的数据 对于软件资源的限制，可以考虑使用资源池将资源复用 根据不同的资源限制调整程序的并发度]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[就业中心凉面复盘]]></title>
    <url>%2F2019%2F10%2F27%2Fcenter-of-job%2F</url>
    <content type="text"><![CDATA[console.error("Error: [hexo-tag-aplayer] Unrecognized tag argument(2): autoplay=false"); 今天结束了学校就业指导服务中心的第三次面试，算是体验了一把比较正式的面试过程，虽然最后铁定是进不了了，还是收获颇丰，也算是一段十分难得的经历了。这是做一下总结，希望对未来的我有所帮助。 宣讲会宣讲会的布置非常专业，中途没有出现较大的差错，整场气氛的把握非常好。 入场有专门的接待人员，同时会发放一些小礼品。对于相对重要的抽奖pass绿卡采用的也是专门的抽奖券，保证了结果相对公平。 入场后按音响、物资等均已调制完善，配备有自带的屏幕（校级组织的土豪气息~） 流程大同小异，先是就业中心的视频介绍，然后是学长学姐介绍，然后是老师介绍，接着是机构功能介绍和老师总结，最后公布招新流程，中间穿插了几次抽奖，最后的问答 也是恰到好处。主持人也算是有水平。 考试按 编号，无特别之处 考试题目也是比较常规，从他们一晚上改完也能知道肯定没仔细看~~ 一面（群面） 题目：《中国机长》背景，措施重要性排序，集体讨论解决 题目还算有新意，属于集体讨论解决问题，而且无法移动位置，只能通过语言说服对方 面试选择是十二人讨论形式 好处是讨论人数增加，同时讨论的位置相对固定，更考验说服能力 坏处是无法使用肢体语言丰富描述 面试官提的问题很常规，而且只争对可能选出的人，没有面面俱到。 最后给出了小组的整体评价，并给出了人性化建议 收获 开场还是惯性思维了，没有把领导到点上。当leader之前要做好准备 ，面对题目变化要有应对措施 中途没有把局势掌握住，还是不敢打断对方，就像学长说的，这样的话容易产生内耗 最后没有去抢机会，其实自己还是风险很大的，最好还是要去争取一下，把自己在群面的感想说出来 二面（单面）无特别之处，全在意料之中。（居然真的要我做英语面试。。Orz） 三面（即兴演讲）重点，这也是我这次面试最期待的地方！！因为习惯了群面单面，其实这些面试几乎对我已经造成不了影响，加上我的简历在校内还是能打，所以可以说是畅通无阻。但是三面不一样，即兴演讲让所有的包装都消失无形，你只有通过自己的真实实力展现自己。 好在这次确实不虚此行，我既看到了即兴演讲的基本操作过程，也确实从演讲中发现了自己的不足。可以说这是一次非常有意义的自我剖析过程。 面试流程首先是场地，即兴演讲讲究高压环境下锻炼临场应变能力，因此这次大约70个人全部坐在会议厅，加上面试官，大概有100多人，偌大的会议厅瞬间被挤得满满当当，可以说是相当压抑了。 面试官分别位于后排和两边，并由一个 主面试官进行随机抽人，一个计时面试官记录时间。人分三列依次带入防止出现拥挤现象。演讲流程是一个人准备在台下准备三分钟，然后上台进行三分钟演讲，演讲同时下一个人在台下进行三分钟的准备，另外提前叫了下下个人进行准备。演讲最后30s会有提醒，当时间还剩超过30s但演讲结束后，面试官会提醒演讲继续，超时打断。另外还有几个学长负责唱黑脸，禁止过程交头接耳、看手机或者鼓掌。由于人数众多，面试时长大致为5~6小时，中间休息了两次，时长为5min（其实不止，大致为10min左右） 面试感受 我抽到的题目是——“你最大的责任是把你铸造成器” 面试过程崩了，其实心态调整的还好，但主要还是硬实力不足，没有办法组织语言。其实我抽到的题还算比较常规，还是有很多素材可以说。我也确实在前期列好了提纲。但是，上场后明显感觉到语言表达的匮乏，无法用丰富的语言去展现自己的想法，导致出现了很多重复的语句，思路也没有完全打开，导致在第一个论点上兜兜转转了很久而且忘记了素材引用。直到提示30s时我还在说第一个论点，导致后面完全没有时间。尽管我主动放弃了第二个论点，但还是无法在30s的时间进行结题，导致全程崩盘。 问题 语言逻辑不清 表达词语匮乏 时间掌握不足 没有结题 收获 需要锻炼较长时间的演讲能力，主要是锻炼逻辑组织能力，从今天的面试来看，其实生活处处都可以作为演讲的主题，应该在平时就注意积累这些观点的积累和表达 时间掌握问题，还是最后选择的问题，如果我能直接结题，或许还能有所挽回，可惜没有做好选择，导致还是超时了，反而会因为语速加快而显示我的焦虑 其实这些问题单面的时候我都有所提及，看来这次确实是一语成谶。😓 即兴演讲的题目 接下来分享一下即兴演讲的题目，这也是我收获最大的地方，可以说出的确实是相当有趣： 万恶之源。可以说是相当有趣的题目 人类的本质是鸽子还是复读机 真香 百因必有果 衬衫的价格是九镑十五便士 （无题） 精神小伙 风华绝代乔碧萝 窝窝头，一块钱四个 苦练七十二变，笑对八十一难 awsl 这盛世，如你所愿 辩题小改，还是能反映一定的思维能力 个人觉得还是好讲的 贪婪是导致贫穷的原因吗 知易行难 分手需要说出来吗 00后和90后之间有代沟吗 如何看待电子竞技 微信和QQ有什么区别 如何看待TFBoys 咪蒙的伪心灵鸡汤 为什么鲁迅的语句这么出名 你觉得同性恋是自愿的还是跟风 朋友和恋人哪个更重要 网上原题，经典 你最大的责任是把你铸造成器 大海 云 鲜花 枷锁 如何看待校园暴力 没有比人更高的山 人生处处是考场 时间的重量 以貌取人是公平的吗 你曾经最绝境的时刻 每一种创伤，都是一 种成熟 自己出的，还是有点想法的 我和我的祖国 我爱重邮 大二的你如何过大三的生活 你在就业中心面试的XX小时XX分里你的感受 兼职和实习]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA知识点总结]]></title>
    <url>%2F2019%2F10%2F15%2Fjava-note%2F</url>
    <content type="text"><![CDATA[感觉学了一段时间java，总是顾着前就忘了后，是时候将常见的知识点做一下整理，也可以加深一下印象。 就这么愉快的决定了。 变量一、一个整数字面值是long类型，否则就是int类型。 建议使用大写的L 二、静态变量和实例变量区别？ 静态变量存在方法区，属于类所有，实例变量存储在堆中，引用存在当前线程栈 三、java 创建对象的几种方式 采用new 通过反射 采用clone(实现clonable接口然后重写clone方法) 通过序列化机制(实现序列化接口然后流式输出) 四、字符串常量池设计思想 为字符串开辟一个字符串常量池，类似于缓存区 创建字符串常量时，首先坚持字符串常量池是否存在该字符串 存在该字符串，返回引用实例，不存在，实例化该字符串并放入池中 实现基础 字符串是不变的，不用担心数据冲突进行共享 实例创建的全局字符串常量池中有一个表，为池中唯一的字符串对象维护一个引用，因此不会被垃圾回收 常量池存放在方法去，和堆区都属于线程共享的 创建对象过程：String str4 = new String(“abc”) 在常量池中查找是否有“abc”对象 有则返回对应的引用实例 没有则创建对应的实例对象 在堆中 new 一个 String(“abc”) 对象 将对象地址赋值给str4,创建一个引用 例： 12345678910111213141516String str1 = new String("A"+"B") ;// 会创建多少个对象? String str2 = new String("ABC") + "ABC" ;// 会创建多少个对象?/** str1：字符串常量池："A","B","AB" : 3个堆：new String("AB") ：1个引用： str1 ：1个总共 ： 5个*//**str2 ：字符串常量池："ABC" : 1个堆：new String("ABC") ：1个引用： str2 ：1个总共 ： 3个*/ String.intern() intern()方法会首先从常量池中查找是否存在该常量值,如果常量池中不存在则现在常量池中创建,如果已经存在则直接返回. 五、各类型字节数 六、String,StringBuffer和StringBuilder区别 String是字符串常量,final修饰; StringBuffer字符串变量(线程安全); StringBuilder 字符串变量(线程不安全). StringBuffer是对对象本身操作,而不是产生新的对象,因此在有大量拼接的情况下,我们建议使用StringBuffer. StringBuffer是线程安全的可变字符串,其内部实现是可变数组. StringBuilder是jdk 1.5新增的,其功能和StringBuffer类似,但是非线程安全.因此,在没有多线程问题的前提下,使用StringBuilder会取得更好的性能. 七、什么是编译器常量?使用它有什么风险?公共静态不可变（public static final ）变量也就是我们所说的编译期常量，这里的 public 可选的。实际上这些变量在编译时会被替换掉，因为编译器知道这些变量的值，并且知道这些变量在运行时不能改变。 这种方式存在的一个问题是你使用了一个内部的或第三方库中的公有编译时常量，但是这个值后面被其他人改变了，但是你的客户端仍然在使用老的值，甚至你已经部署了一个新的jar。为了避免这种情况，当你在更新依赖 JAR 文件时，确保重新编译你的程序。 八、byte[] 转String 可以使用String的构造器，但是注意使用正确编码 基本特性面向对象三特性 封装,继承,多态 多态的好处 可替换性 可扩充性：增加新的子类不影响已经存在的类结构 接口性：多态是超累通过方法签名，向子类提供一个公共接口 灵活性 简化性 如何实现多态 接口实现 继承父类重写方法 抽象类意义 为其他子类提供一个公共的类型 封装子类中重复定义的内容 定义抽象方法,子类虽然有不同的实现,但是定义是一致的 一、抽象类和接口的区别抽象类和接口区别 一个类只能继承一个类，但是可以实现多个接口 接口类只能做方法申明，抽象类可以做方法申明也可以做方法实现 接口类定义的变量是公共的静态常量，抽象类中的变量是普通变量 抽象类的抽象方法必须全部被子类实现，如果没有实现，子类只能是抽象类；同样实现一个接口时不实现全部方法，该类只能是抽象类 抽象方法只能申明，不能实现，接口是设计的结果，抽象类是重构的结果 ==抽象类中可以没有抽象方法==，抽象方法要被实现，不能是静态的也不能是私有的 接口可继承接口，类只能单根继承 ==接口中的变量会被隐式地指定为public static final变量== 语法层级区别 抽象类提供给成员方法的实现细节，接口中只存publi abstract方法 抽象类中的成员变量可以是各种类型的，接口中的成员变量只能是public static final 接口中不能有静态代码块，抽象类中可以有 一个类只能继承一个抽象类，但是可以实现多个接口 设计层次区别 抽象列是对类整体抽象，接口事对局部的行为进行抽象 抽象类是模板式设计，接口是行为规范 二、short类型在进行运算时会自动提升为int类型 三、final,finalize和finally的不同之处 final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变。 finalize 方法是在对象被回收之前调用的方法，给对象自己最后一个复活的机会，但是什么时候调用 finalize 没有保证。 finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常。 final的用法 1.被final修饰的类不可以被继承2.被final修饰的方法不可以被重写3.被final修饰的变量不可以被改变.如果修饰引用,那么表示引用不可变,引用指向的内容可变.4.被final修饰的方法,JVM会尝试将其内联,以提高运行效率5.被final修饰的常量,在编译阶段会存入常量池中. 编译器对final域要遵守的两个重排序规则： 1.在构造函数内对一个final域的写入,与随后把这个被构造对象的引用赋值给一个引用变量,这两个操作之间不能重排序.2.初次读一个包含final域的对象的引用,与随后初次读这个final域,这两个操作之间不能重排序. 四、如何正确的退出多层嵌套循环. 使用标号label和break; 通过在外层循环中添加标识符 五、深拷贝和浅拷贝的区别是什么? 浅拷贝：被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用仍然指向原来的对象。换言之，浅拷贝仅仅复制所考虑的对象，而不复制它所引用的对象。 深拷贝：被复制对象的所有变量都含有与原来的对象相同的值，而那些引用其他对象的变量将指向被复制过的新对象，而不再是原有的那些被引用的对象。换言之，深拷贝把要复制的对象所引用的对象都复制了一遍。 六、static都有哪些用法? 静态变量和静态方法：也就是被static所修饰的变量/方法都属于类的静态资源,类实例所共享. 初始化操作，静态块： 123456public calss PreCache&#123; static&#123; //执行相关操作 &#125;&#125; static也多用于修饰内部类 静态导包：import static是在JDK 1.5之后引入的新特性,可以用来指定导入某个类中的静态资源,并且不需要使用类名.资源名,可以直接使用资源名 七、进程、线程相关进程,线程,协程之间的区别 进程是==程序运行和资源分配==的基本单位,一个程序至少有一个进程,一个进程至少有一个线程。进程在执行过程中拥有==独立的内存单元==,而多个线程共享内存资源,==减少切换次数,从而效率更高.== 线程是进程的一个实体,是==cpu调度和分派的基本单位==,是比程序更小的能独立运行的基本单位.同一进程中的多个线程之间可以并发执行. 协程，是一种比线程更加轻量级的存在，协程不==是被操作系统内核所管理==，而==完全是由程序所控制==（也就是在用户态执行） 协程在子程序内部是可中断的，然后转而执行别的子程序，在适当的时候再返回来接着执行。由程序自身控制，没有线程切换，执行效率高。 因为只有一个线程，也不存在同时写变量冲突，因此在协程中控制共享资源不加锁 java为什么坚持用多线程不用协程? 一个tomcat上的woker线程池的最大线程数一般会配置为50～500之间（目前springboot的默认值给的200）,实际内存增幅对整体性能影响不大 使用netty，NIO+worker thread可以大致等于一套协程 通过线程池可以很好创建销毁线程开销 线程的切换实际上只会发生在那些“活跃”的线程上。java web中大量存在的是IO请求挂起的线程，不会参与OS的线程切换 守护线程和非守护线程区别 程序运行完毕,jvm会等待非守护线程完成后关闭,但是jvm不会等待守护线程.守护线程最典型的例子就是GC线程 多线程上下文切换 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 java.lang.Runnable比java.lang.Thread优势？ Java不支持多继承.因此继承Thread类就代表这个子类不能扩展其他类.而实现Runnable接口的类还可能扩展另一个类. 类可能只要求可执行即可,因此继承整个Thread类的开销过大. Thread类中的start()和run()方法有什么区别? start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。==当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动==，start()方法才会启动新线程。 怎么检测一个线程是否持有对象监视器 Thread类提供了一个·holdsLock(Object obj)方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着”某条线程”指的是当前线程。 对象监视器 监视器是==一种同步结构，它基于互斥锁==，允许线程同时互斥（使用锁）和协作，· 当一个线程需要数据在某一个状态下它才能执行，那么另一个线程负责将数据改变到此状态， 常见的如生产者/消费者的问题，当读线程需要缓冲区处于“不空”的状态它才可以从缓冲区中读取任何数据，如果它发现缓冲区为空，则进入wait-set等待。待写线程用数据填充缓冲区，再通知读线程进行读取。这种机制被称为“Wait and Notify”或“Signal and Continue” Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 什么导致线程阻塞 阻塞指的是暂停一个线程的执行以等待某个条件发生（如某资源就绪） sleep()：被用在等待某个资源就绪的情形：测试发现条件不满足后，让线程阻塞一段时间后重新测试，直到条件满足为止 suspend() 和 resume()：suspend()使得线程进入阻塞状态，并且不会自动恢复，必须其对应的resume() 被调用，才能使得线程重新进入可执行状态。 yield()：使当前线程放弃当前已经分得的CPU 时间，==但不使当前线程阻塞==，即线程仍处于可执行状态，随时可能再次分得 CPU 时间 wait() 和 notify()：wait() 使得线程进入阻塞状态，它有两种形式，一种允许指定以==毫秒==为单位的一段时间作为参数，另一种没有参数，前者当对应的 notify() 被调用或者超出指定时间时线程重新进入可执行状态，后者则必须对应的 notify() 被调用. wait(),notify()和suspend(),resume()之间的区别 wait(),notify()属于Object类，所有对象都拥有这一对方法；（因为锁是任何对象具有的）其他方法属于thread类。其他方法阻塞时都不会释放占用的锁（如果占用了的话），这一对会释放占用锁 wait(),notify()必须在 synchronized方法或块中调用，其他所有方法可在任何位置调用。（因为在synchronized方法或块中当前线程才占有锁，才有锁可以释放。同样的道理，调用这一对方法的对象上的锁必须为当前线程所拥有，这样才有锁可以释放）如果没有放在同步方法或同步块中，会报IllegalMonitorStateException 关于 wait() 和 notify() 方法最后再说明两点：第一：调用notify() 方法导致解除阻塞的线程是从因调用该对象的 wait() 方法而阻塞的线程中==随机选取==的，我们无法预料哪一个线程将会被选择，所以编程时要特别小心，避免因这种不确定性而产生问题。 第二：除了 notify()，还有一个方法 notifyAll()也可起到类似作用，唯一的区别在于，调用 notifyAll()方法将把因调用该对象的 wait()方法而阻塞的所有线程一次性全部解除阻塞。当然，只有获得锁的那一个线程才能进入可执行状态。 特别注意：uspend() 方法和不指定超时期限的 wait() 方法的调用都可能产生死锁 wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别 wait()方法==立即释放对象监视器==，notify()/notifyAll()方法则会==等待线程剩余代码执行完==毕才会放弃对象监视器。 标准使用wait示例 12345synchronized (obj) &#123;while (condition does not hold) obj.wait(); // (Releases lock, and reacquires on wakeup)... // Perform action appropriate to condition&#125; 八、产生死锁的条件 ==互斥条件==：一个资源每次只能被一个进程使用。 ==请求与保持条件：==一个进程因请求资源而阻塞时，对已获得的资源保持不放。 ==不剥夺条件:进程已获得的资源==，在末使用完之前，不能强行剥夺。 ==循环等待条件==:若干进程之间形成一种头尾相接的循环等待资源关系。 synchronized和ReentrantLock的区别· synchronized是和if、else、for、while一样的==关键字==，ReentrantLock是==类==，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上：（1）ReentrantLock可以对获取锁的等待时间进行设置，这样就==避免了死锁==（2）ReentrantLock可以获取各种锁的信息（3）ReentrantLock可以灵活地实现==多路通知==另外，二者的锁机制其实也是不一样的:ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark ``word. Java对象结构与锁实现原理及MarkWord详解 一个线程如果出现了运行时异常怎么办? 如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放 线程共享数据方法 通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 九、java中锁种类锁提供了两种主要特性：互斥（mutual exclusion） 和可见性（visibility） 锁的状态 自旋锁 ==共享数据的锁定状态==只会持续很短的时间，为了这一小段时间而去挂起和恢复线程有点浪费，所以这里就做了一个处理，让后面请求锁的那个线程在稍等一会，但是不放弃处理器的执行时间，看看持有锁的线程能否快速释放 ==为了让线程等待，所以需要让线程执行一个忙循环也就是自旋操作== 在jdk6之后，引入了自适应的自旋锁，也就是等待的时间不再固定了，而是由上一次在同一个锁上的自旋时间及锁的拥有者状态来决定 ==好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。== 偏向锁 目的是消除数据在无竞争情况下的同步原语。进一步提升程序的运行性能。 ==这个锁会偏向第一个获得他的线程，如果接下来的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程将永远不需要再进行同步。== 偏向锁可以提高带有同步但无竞争的程序性能，也就是说他并不一定总是对程序运行有利，如果程序中大多数的锁都是被多个不同的线程访问，那偏向模式就是多余的，在具体问题具体分析的前提下，可以考虑是否使用偏向锁。 轻量级锁/重量级锁 为了减少获得锁和释放锁带来的性能消耗 在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。==锁可以升级但不能降级==，意味着偏向锁升级成轻量级锁后不能降级成偏向锁 四种锁的状态是通过对象监视器在对象头中的字段来表明的。 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。==重量级锁会让他申请的线程进入阻塞，性能降低。== 锁的种类 独享锁/共享锁：独享锁是指==该锁一次只能被一个线程所持有==；共享锁是指==该锁可被多个线程所持有==。 对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。 对于Synchronized而言，当然是独享锁。 读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。 独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。 互斥锁/读写锁:即独享锁和共享锁的具体实现 可重入锁：又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁 对于Java ReetrantLock而言，从名字就可以看出是一个重入锁，其名字是Re entrant Lock 重新进入锁。 对于Synchronized而言，也是一个可重入锁。可重入锁的一个好处是==可一定程度避免死锁。== 12345678synchronized void setA() throws Exception&#123; Thread.sleep(1000); setB();&#125;//如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。synchronized void setB() throws Exception&#123; Thread.sleep(1000);&#125; 公平锁和非公平锁： 公平锁是指多个线程按照==申请锁的顺序==来获取锁。 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。 对于Java ReetrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于==吞吐量比公平锁大==。 对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以==并没有任何办法使其变成公平锁==。 锁的设计 乐观锁/悲观锁：主要是指看待==并发同步==的角度 ==悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。== 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，==但是在更新的时候会判断一下在此期间别人有没有去更新这个数据==，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，在Java中java.util.concurrent.atomic包下面的==原子变量类==就是使用了乐观锁的一种实现方式==CAS(Compare and Swap)== 比较并交换)实现的。 乐观锁在Java中的使用，是==无锁编程==，常常采用的是==CAS算法==，典型的例子就是原子类，通过==CAS自旋==实现原子操作的更新。 数据版本机制 实现数据版本一般有两种，第一种是使用版本号，第二种是使用时间戳。以版本号方式为例。 版本号方式：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 核心SQL代码： 1update table set xxx=#&#123;xxx&#125;, version=version+1 where id=#&#123;id&#125; and version=#&#123;version&#125;; CAS操作 CAS（Compare and Swap 比较并交换），当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，==失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。== CAS操作中包含三个操作数——==需要读写的内存位置(V)==、==进行比较的预期原值(A)==和==拟写入的新值(B)==。如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B，否则处理器不做任何操作。 以java.util.concurrent包中的AtomicInteger为例，看一下在不使用锁的情况下是如何保证线程安全的。主要理解getAndIncrement方法，该方法的作用相当于++i操作 12345678910111213141516171819public class AtomicInteger extends Number implements java.io.Serializable&#123; private volatile int value; //CAS中必须使用volatile变量，保证拿到的变量时主内存中最新值 public final int get()&#123; return value; &#125; public final int getAndIncrement()&#123; for (;;)&#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) //获取值后查看值是否更新 return current; &#125; &#125; public final boolean compareAndSet(int expect, int update)&#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125;&#125; 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。比如Java里面的同步原语synchronized关键字的实现就是悲观锁。 悲观锁在Java中的使用，就是利用各种锁 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要==等待==或者==抛出异常==。具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 期间如果有其他对该记录做修改或加排他锁的操作，都会==等待==我们解锁或直接抛出异常。 分段锁：对于ConcurrentHashMap而言，其并发的实现就是==通过分段锁的形式==来实现高效的并发操作 以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7和JDK8中HashMap的实现）的结构，即==内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock）==。 当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在哪一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入 但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。 分段锁的设计目的是==细化锁的粒度==，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 锁的使用java锁的使用和种类 预备知识 AQS：AbstractQueuedSynchronized 抽象队列式的同步器，AQS定义了一套==多线程访问共享资源的同步器框架==，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch… AQS维护了一个volatile int state(代表==共享资源)==和一个FIFO线程等待队列（==多线程争用资源被阻塞时会进入此队列==）。 state的访问方式： 123getState();setState();compareAndSetState(); AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 自定义同步器实现时主要实现以下几种方法 12345isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。tryAquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。tryReleaseShared(int)//共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0（即释放锁）为止，其他线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多少次，这样才能保证state是能回到零态的。 再以CountDownLatch为例，任务分为N个子线程去执行，state为初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS减1。等到所有子线程都执行完后（即state=0），会unpark()主调用线程，然后主调用线程就会await()函数返回，继续后余动作。 注 ：AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 十、 ThreadLocal线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享，Java提供ThreadLocal类来支持线程局部变量，是一种实现线程安全的方式。 但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。==任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。== 作用：简单说ThreadLocal就是一种以==空间换时间==的做法在每个Thread里面维护了一个ThreadLocal.ThreadLocalMap把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了. 十一、生产者消费者模型 作用： （1）通过==平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率==，这是生产者消费者模型最重要的作用（2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 写一个生产者-消费者队列方法 可以通过阻塞队列实现,也可以通过wait-notify来实现. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//阻塞队列实现生产者消费者模型//消费者public class Producer implements Runnable&#123; private final BlockingQueue&lt;Integer&gt; queue; public Producer(BlockingQueue q)&#123; this.queue=q; &#125; @Override public void run() &#123; try &#123; while (true)&#123; Thread.sleep(1000);//模拟耗时 queue.put(produce()); &#125; &#125;catch (InterruptedException e)&#123; &#125; &#125; private int produce() &#123; int n=new Random().nextInt(10000); System.out.println("Thread:" + Thread.currentThread().getId() + " produce:" + n); return n; &#125;&#125;//消费者public class Consumer implements Runnable &#123; private final BlockingQueue&lt;Integer&gt; queue; public Consumer(BlockingQueue q)&#123; this.queue=q; &#125; @Override public void run() &#123; while (true)&#123; try &#123; Thread.sleep(2000);//模拟耗时 consume(queue.take()); &#125;catch (InterruptedException e)&#123; &#125; &#125; &#125; private void consume(Integer n) &#123; System.out.println("Thread:" + Thread.currentThread().getId() + " consume:" + n); &#125;&#125;//测试public class Main &#123; public static void main(String[] args) &#123; BlockingQueue&lt;Integer&gt; queue=new ArrayBlockingQueue&lt;Integer&gt;(100); Producer p=new Producer(queue); Consumer c1=new Consumer(queue); Consumer c2=new Consumer(queue); new Thread(p).start(); new Thread(c1).start(); new Thread(c2).start(); &#125;&#125; 十二、java中的线程调度算法 抢占式。一个线程用完CPU之后，操作系统会根据==线程优先级、线程饥饿情况==等数据算出一个==总的优先级==并分配下一个时间片给某个线程执行。 十三、Thread.sleep(0)的作用是什么 由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用==Thread.sleep(0)手动触发一次操作系统分配时间片的操作==，这也是平衡CPU控制权的一种操作。 十四、ConcurrentHashMapConcurrentHashMap的并发度是什么? ConcurrentHashMap的并发度就是segment的大小，默认为==16==，这意味着最多同时可以有16条线程操作ConcurrentHashMap，这也是ConcurrentHashMap对Hashtable的最大优势，任何情况下，Hashtable能同时有两条线程获取Hashtable中的数据吗？ ConcurrentHashMap的工作原理 jdk 1.6: ConcurrentHashMap是==线程安全==的，但是与Hashtablea相比，实现线程安全的方式不同。 Hashtable是通过对hash表结构进行锁定，是==阻塞式==的，当一个线程占有这个锁时，其他线程必须阻塞等待其释放锁。 ConcurrentHashMap是采用==分离锁==的方式，它并没有对整个hash表进行锁定，而是局部锁定，也就是说当一个线程占有这个局部锁时，不影响其他线程对hash表其他地方的访问。 jdk1.7 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成 jdk 1.8 在jdk 8中，ConcurrentHashMap不再使用Segment分离锁，而是采用一种乐观锁CAS算法来实现同步问题，但其底层还是“==数组+链表-&gt;红黑树==”的实现,桶中的结构可能是链表，也可能是红黑树，红黑树是为了提高查找效率。 总结： 相对而言，ConcurrentHashMap只是增加了同步的操作来控制并发，从JDK1.7版本的==ReentrantLock+Segment+HashEntry==，到JDK1.8版本中==synchronized+CAS+HashEntry+红黑树==,相对而言，总结如下思考: JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点） JDK1.8版本的数据结构变得更加简单，使得操作也更加清晰流畅，因为已经使用synchronized来进行同步，所以不需要分段锁的概念，也就不需要Segment这种数据结构了，由于粒度的降低，实现的复杂度也增加了 JDK1.8使用红黑树来优化链表，基于长度很长的链表的遍历是一个很漫长的过程，而红黑树的遍历效率是很快的，代替一定阈值的链表，这样形成一个最佳拍档 JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock，我觉得有以下几点 因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在==粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了== JVM的开发团队从来都没有放弃synchronized，而且==基于JVM的synchronized优化空间更大==，使用内嵌的关键字比使用API更加自然 在大量的数据操作下，对于JVM的内存压力==，基于API的ReentrantLock会开销更多的内存==，虽然不是瓶颈，但是也是一个选择依据 十五、CyclicBarrier和CountDownLatch区别 这两个类非常类似，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： CyclicBarrier的某个线程运行到某个点上之后，==该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行==；CountDownLatch则不是，某线程运行到某个点上之后，==只是给某个数值-1而已，该线程继续运行== CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务 CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了 十六、java中的++操作符线程安全么? 不是线程安全的操作。它涉及到多个指令，如==读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差== 十七、多线程开发良好习惯 给线程命名 最小化同步范围 优先使用volatile 尽可能使用更高层次的并发工具而非wait和notify()来实现线程通信,如BlockingQueue,Semeaphore 优先使用并发容器而非同步容器. 考虑使用线程池 十八、volatile关键字指令重排序和内存可见性，volatile 类型变量即使在没有同步块的情况下赋值也不会与其他语句重排序。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。 ==Volatile 变量具有 synchronized 的可见性特性，但是不具备原子特性== 可以创建Volatile数组吗? Java 中可以创建 volatile类型数组，不过只是一个指向数组的引用，而不是整个数组。如果改变引用指向的数组，将会受到volatile 的保护，但是如果多个线程同时改变数组的元素，volatile标示符就不能起到之前的保护作用了 如何使非原子操作变成原子操作 典型案例： double 和 long 都是64位宽，因此对这两种类型的读是分为两部分的，第一次读取第一个 32 位，然后再读剩下的 32 位，这个过程不是原子的。如果知道要被多线程访问，应该加volatile关键字 提供内存屏障（memory barrier） 在写一个 volatile 变量之前，Java 内存模型会插入一个写屏障（write barrier），读一个 volatile 变量之前，会插入一个读屏障（read barrier）即在==你写一个 volatile 域时，能保证任何线程都能看到你写的值==，同时，在==写之前，也能保证任何数值的更新对所有线程是可见的==，因为内存屏障会将其他所有写的值更新到缓存 使用条件 对变量的写操作不依赖于当前值。 该变量没有包含在具有其他变量的不变式中。 十九、异常白话异常机制 throw和throws的区别 throw用于主动抛出java.lang.Throwable 类的一个实例化对象，意思是说你可以通过关键字 throw 抛出一个 Error 或者 一个Exception，如： throw new IllegalArgumentException(“size must be multiple of 2″) 而throws 的作用是作为方法声明和签名的一部分，方法被抛出相应的异常以便调用者能处理。Java 中，任何未处理的受检查异常强制在 throws 子句中声明。 二十、Java 中，Serializable 与 Externalizable 的区别 Serializable接口是一个序列化 Java 类的接口，以便于它们可以在网络上传输或者可以将它们的状态保存在磁盘上，是==JVM 内嵌的默认序列化方式==，成本高、脆弱而且不安全。Externalizable允许你控制整个序列化过程，指定特定的二进制格式，增加安全机制。 方法一、switch 在1.7后支持String类型，支持byte类型但是不支持long类型 二、a.hashCode()有什么用?与a.equals(b)有什么关系？ hashCode() 方法是相应对象整型的 hash 值。它常用于基于 hash 的集合类，如 Hashtable、HashMap、LinkedHashMap等等。它与 equals() 方法关系特别紧密。根据 Java 规范，使用 equal() 方法来判断两个相等的对象，必须具有相同的 hashcode。 三、a==b与a.equals(b)有什么区别 如果a 和b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。 四、+=操作符会进行隐式自动类型转换 五、位运算 12name!=null&amp;userName.equals("") //要报空指针异常 六、日期计算SimpleDateFormat是线程安全的吗? DateFormat的所有实现，包括 SimpleDateFormat都不是线程安全的，因此你不应该在多线程序中使用，除非是在对外线程安全的环境中使用，如将 SimpleDateFormat限制在 ThreadLocal中。如果你不这么做，在解析或者格式化日期的时候，可能会获取到一个不正确的结果。因此，从日期、时间处理的所有实践来说，我强力推荐 joda-time 库。 七、多态多态表示当同一个操作作用在不同对象时，会有不同的语义，从而产生不同的结果。3+4和“3”+“4” Java的多态性可以概括成”一个接口,两种方法”分为两种 编译时的多态 编译时的多态主要是指方法的重载（overload） 运行时的多态。 运行时的多态主要是指方法的覆盖（override），接口也是运行时的多态 运行时的多态的三种情况：1、父类有方法，子类有覆盖方法：编译通过，执行子类方法。2、父类有方法，子类没覆盖方法：编译通过，执行父类方法（子类继承）。3、父类没方法，子类有方法：编译失败，无法执行。==方法带final、static、private时是编译时多态，因为可以直接确定调用哪个方法。== 集合 一、ArrayList和LinkedList的区别? 最明显的区别是 ArrrayList底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构是双向循环链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。 二、ArrayList和Array有什么区别? Array可以容纳基本类型和对象，而ArrayList只能容纳对象。 Array是指定大小的，而ArrayList大小是固定的 三、Comparator和Comparable的区别? Comparable 接口用于定义对象的·自然顺序，而 comparator 通常用于定义用户定制的顺序。==Comparable 总是只有一个==，但是可以有多个 comparator 来定义对象的顺序。 四、如何打印数组内容 你可以使用Arrays.toString()和Arrays.deepToString() 方法来打印数组。由于数组没有实现 toString() 方法，所以如果将数组传递给System.out.println()方法，将无法打印出数组的内容，但是 Arrays.toString()可以打印每个元素。 数据库一、使用Integer和Long进行数据库数据存值，因为这些是对象，如果使用int或者long会获取不到值 注意不要和mysql的关键字冲突了！！ 二、三范式第一范式(1NF): 指的是数据库表的中的每一列都是不可分割的基本数据项,同一列中不能有多个值。第一范式要求属性值是不可再分割成的更小的部分。第一范式简而言之就是强调的是列的原子性，即列不能够再分成其他几列。例如有一个列是电话号码一个人可能有一个办公电话一个移动电话。第一范式就需要拆开成两个属性。 第二范式（2NF）： 第二范式首先是第一范式，同时还需要包含两个方面的内容，一是表必须要有一个主键；二是没有包含主键中的列必须完全依赖主键，而不能只是依赖于主键的一部分。例如在一个订单中可以订购多种产品，所以单单一个 OrderID 是不足以成为主键的，主键应该是（OrderID，ProductID）。显而易见 Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID。所以 OrderDetail 表不符合 2NF。 不符合 2NF 的设计容易产生冗余数据。 可以把【OrderDetail】表拆分为【OrderDetail】（OrderID，ProductID，Discount，Quantity）和【Product】（ProductID，UnitPrice，ProductName）来消除原订单表中UnitPrice，ProductName多次重复的情况。 第三范式（3NF）： 首先是第二范式，例外非主键列必须依赖于主键，不能存在传递。也就是说不能存在非主键列A依赖于非主键列B，然后B依赖于主键列考虑一个订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID）。其中 OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity 等非主键列都完全依赖于主键（OrderID），所以符合 2NF。不过问题是 CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF。通过拆分【Order】为【Order】（OrderID，OrderDate，CustomerID）和【Customer】（CustomerID，CustomerName，CustomerAddr，CustomerCity）从而达到 3NF。==二范式（2NF）和第三范式（3NF）的概念很容易混淆，区分它们的关键点在于，2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分；3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。== 三、内外连接 四、事务 原子性:即事务是一个不可分割的整体,数据修改时要么都操作一遍要么都不操作 一致性:一个事务执行前后数据库的数据必须保持一致性状态 隔离性:当两个或者以上的事务并发执行时,为了保证数据的安全性,将一个事务的内部的操作与事务操作隔离起来不被其他事务看到 持久性:更改是永远存在的 隔离级别 读未提交：事务中的修改，即使没有提交，其他事务也可以看得到，脏读。如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。一个在写事务另一个虽然不能写但是能读到还没有提交的数据 读已提交：可以避免脏读但是可能出现不可重复读。允许写事务，读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。事务T1读取数据，T2紧接着更新数据并提交数据，事务T1再次读取数据的时候，和第一次读的不一样。即虚读 可重复读：禁止写事务，读事务会禁止所有的写事务，但是允许读事务，避免了不可重复读和脏读，但是会出现幻读，即第二次查询数据时会包含第一次查询中未出现的数据 序列化：禁止任何事务，一个一个进行；提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。如果仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。 索引索引 7.6.1优缺点： 优点： 可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序缺点： 索引本省也是表会占用内存，索引表占用的空间是数据表的1.5倍；索引表的创建和维护需要时间成本，这个成本随着数据量的增大而增大。 7.6.2索引的底层实现原理： 哈希索引： 只有memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的hashCode，然后在hashCode相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。 Btree索引： B树是一个平衡多叉树，设树的度为2d，高度为h，那么B树需要满足每个叶子节点的高度都一样等于h，每个非叶子节点由n-1个key和n个point组成，d&lt; = n&lt;=2d 。所有叶子节点指针均为空，非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据。 B+Tree索引 B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：B+Tree中的非叶子结点不存储数据，只存储键值；B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址；B+Tree的每个非叶子节点由n个键值key和n个指针point组成；优点：查询速度更加稳定，磁盘的读写代价更低 聚簇索引与非聚簇索引 聚簇索引的解释是:聚簇索引的顺序就是数据的物理存储顺序非聚簇索引的解释是:索引顺序与数据物理排列顺序无关 MyISAM——非聚簇索引 MyISAM存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的key都存储指向键值对应的数据的物理地址。非聚簇索引的数据表和索引表是分开存储的。 innoDB——聚簇索引 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。聚簇索引的数据和主键索引存储在一起。 7.6.3 联合索引(顺丰) 利用最左前缀原则 7.7.数据库锁 锁是计算机协调多个进程或者纯线程并发访问某一资源的机制 7.7.1Mysql的锁种类 Mysql的锁机制比较简单，不同的搜索引擎支持不同的锁机制表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率高，并发度最低行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突概率最低，并发度也最高页面锁：开销和加锁速度位于表锁和行锁之间，会出现死锁，锁定粒度也位于表锁和行锁之间，并发度一般 7.7.2Mysql表级锁的锁模式（MyISAM） Mysql表级锁有两种模式：表共享锁（Table Read Lock）和表独占锁（Table Write Lock） 7.8.having 和group by四、不可重复读和幻读一个事务A开启后，第一次读取到一些数据之后，就对这些数据进行加行锁，导致其他事务B无法修改（更新或者删除）数据，于是A事务不管怎么读，返回的都是一样的数据，这就实现了“可重复读”这个隔离级别 “其他事务B无法修改这些数据（更新或删除）”，不代表其他事务B不能insert一些记录并提交。这样一来事务A还是可以读取到一条之前没有出现的数据，这就产生了“幻读”。 行级锁是无法解决幻读问题的。要想解决这个问题必须实现Serializable隔离级别。 使用间隙锁可以解决插入导致的幻读 五、分布式ID生成方案总结分布式ID生成方案总结 生成全局 id 有下面这几种方式： UUID：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。 数据库自增 id : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。 利用 redis 生成 id : 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。 Twitter的snowflake算法 ：Github 地址：https://github.com/twitter-archive/snowflake。 美团的Leaf分布式ID生成系统 ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。 。 六、常用命令MySQL常用操作命令 MySQL查看数据库性能常用命令 七、把子查询优化为 join 操作通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 八、临时表使用临时表的场景 1)ORDER BY子句和GROUP BY子句不·同， 例如：ORDERY BY price GROUP BY name； 2)在JOIN查询中，ORDER BY或者GROUP BY使用了不是第一个表的列 例如： 1SELECT * from TableA, TableB ORDER BY TableA.price GROUP by TableB.name 3)ORDER BY中使用了DISTINCT关键字 ORDERY BY DISTINCT(price) 4)SELECT语句中指定了SQL_SMALL_RESULT关键字 SQL_SMALL_RESULT的意思就是告诉MySQL，结果会很小，请直接使用内存临时表，不需要使用索引排序 SQL_SMALL_RESULT必须和GROUP BY、DISTINCT或DISTINCTROW一起使用 一般情况下，我们没有必要使用这个选项，让MySQL服务器选择即可。 直接使用磁盘临时表的场景 1)表包含TEXT或者BLOB列； 2)GROUP BY或者 DISTINCT子句中包含长度大于512字节的列； 3)使用UNION或者UNION ALL时，SELECT子句中包含大于512字节的列； JVM一、四种引用引用详解 强引用：如果一个对象具有强引用，==它就不会被垃圾回收器回收==。即使当前内存空间不足，JVM也不会回收它，而是==抛出 OutOfMemoryError 错误==，使程序异常终止。如果想中断强引用和某个对象之间的关联，可以==显式地将引用赋值为nul==l，这样一来的话，JVM在合适的时间就会回收该对象 1Person person=new Person(); 软引用：在使用软引用时，如果内存的空间足够，软引用就能继续被使用，而不会被垃圾回收器回收，==只有在内存不足时，软引用才会被垃圾回收器回收==。 12Person person=new Person(); SoftReference sr=new SoftReference(person); 弱引用：具有弱引用的对象拥有的生命周期更短暂。因为当 JVM 进行垃圾回收，一旦发现弱引用对象，==无论当前内存空间是否充足，都会将弱引用回收==。不过由于垃圾回收器是一个优先级较低的线程，所以并不一定能迅速发现弱引用对象 12Person person=new Person(); WeakReference wr=new WeakReference(person); 虚引用：顾名思义，就是形同虚设，如果一个对象仅持有虚引用，那么它相当于没有引用，在任何时候都可能被垃圾回收器回收。 设置虚引用的目的是为了==被虚引用关联的对象在被垃圾回收器回收时，能够收到一个系统通知== 1234ReferenceQueue queue=new ReferenceQueue();PhantomReference pr=new PhantomReference(object.queue);//GC在回收一个对象时，如果发现该对象具有虚引用，那么在回收之前会首先该对象的虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入虚引用来了解被引用的对象是否被GC回收。 引用顺序 单条引用链的可达性以最弱的一个引用类型来决定；多条引用链的可达性以最强的一个引用类型来决定； 应用场景 利用软引用和弱引用解决OOM问题： 例：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了OOM的问题. 通过软引用实现Java对象的高速缓存: 例：比如我们创建了一Person的类，如果每次需要查询一个人的信息,哪怕是几秒中之前刚刚查询过的，都要重新构建一个实例，这将引起大量Person对象的消耗,并且由于这些对象的生命周期相对较短,会引起多次GC影响性能。此时,通过软引用和 HashMap 的结合可以构建高速缓存,提供性能. 二、ReferenceQueue和ReferenceReferenceQueue 其作用在于Reference对象所引用的对象被GC回收时，该Reference对象将会被加入引用队列中（ReferenceQueue）的队列末尾,这相当于是一种通知机制.当关联的引用队列中有数据的时候，意味着引用指向的堆内存中的对象被回收。通过这种方式，JVM允许我们在对象被销毁后，做一些我们自己想做的事情 123ReferenceQueue&lt; Person&gt; rq=new ReferenceQueue&lt;Person&gt;();Person person=new Person();SoftReference sr=new SoftReference(person,rq); Reference Reference是SoftReference，WeakReference,PhantomReference类的父类，其内部通过一个next字段来构建了一个Reference类型的单向列表，而queue字段存放了引用对象对应的引用队列，若在Reference的子类构造函数中没有指定，则默认关联一个ReferenceQueue.NULL队列。 三、垃圾回收算法 GC主要完成三项任务：分配内存，确保被引用的对象的内存不被错误的回收以及回收不再被引用的对象的内存空间 标记-清除 标记-复制 标记-整理 分代回收 增量收集 不用stop the world 判断对象存活：1.引用计数法;2:对象可达性分析 简单的解释一下垃圾回收 Java 垃圾回收机制最基本的做法是分代回收。 内存中的区域被划分成不同的世代，对象根据其存活的时间被保存在对应世代的区域中。一般的实现是划分成3个世代：年轻、年老和永久。内存的分配是发生在年轻世代中的。当一个对象存活时间足够长的时候，它就会被复制到年老世代中。对于不同的世代可以使用不同的垃圾回收算法。 进行世代划分的出发点是对应用中对象存活时间进行研究之后得出的统计规律。一般来说，一个应用中的大部分对象的存活时间都很短。比如局部变量的存活时间就只在方法的执行过程中。基于这一点，对于年轻世代的垃圾回收算法就可以很有针对性. 四、System.gc()：通知GC开始工作,但是GC真正开始的时间不确定. 五、JVM的平台五无关性 Java语言的一个非常重要的特点就是与平台的无关性。而使用Java虚拟机是实现这一特点的关键。一般的高级语言如果要在不同的平台上运行，至少需要编译成不同的目标代码。而引入Java语言虚拟机后，Java语言在不同平台上运行时不需要重新编译。Java语言使用模式Java虚拟机屏蔽了与具体平台相关的信息，使得Java语言编译程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。Java虚拟机在执行字节码时，把字节码解释成具体平台上的机器指令执行。 六、类加载机制 深入理解JVM加载器。 初始化阶段 以下情况才会对类立即初始化： 使用new关键字实例化对象、访问或者设置一个类的静态字段（==被final修饰、编译器优化时已经放入常量池的例外==）、调用类方法，都会初始化该静态字段或者静态方法所在的类。 初始化类的时候，如果其父类没有被初始化过，则要先触发其父类初始化。 使用java.lang.reflect包的方法进行反射调用的时候，如果类没有被初始化，则要先初始化。· 虚拟机启动时，用户会先初始化要执行的主类（含有main） jdk 1.7后，如果java.lang.invoke.MethodHandle的实例最后对应的解析结果是 REF_getStatic、REF_putStatic、REF_invokeStatic方法句柄，并且这个方法所在类没有初始化，则先初始化。 其他一、XML解析的几种方式和特点 DOM,SAX,PULL三种解析方式: DOM:消耗内存：先把xml文档都读到内存中，然后再用DOM API来访问树形结构，并获取数据。这个写起来很简单，但是很消耗内存。要是数据过大，手机不够牛逼，可能手机直接死机 SAX:解析效率高，占用内存少，基于事件驱动的：更加简单地说就是对文档进行顺序扫描，当扫描到文档(document)开始与结束、元素(element)开始与结束、文档(document)结束等地方时通知事件处理函数，由事件处理函数做相应动作，然后继续同样的扫描，直至文档结束。 PULL:与 SAX 类似，也是基于事件驱动，我们可以调用它的next（）方法，来获取下一个解析事件（就是开始文档，结束文档，开始标签，结束标签），当处于某个元素时可以调用XmlPullParser的getAttributte()方法来获取属性的值，也可调用它的nextText()获取本节点的值。 二、版本特性 JDK 1.7特性 JDK 1.7 不像 JDK 5 和 8 一样的大版本，但是，还是有很多新的特性，如 try-with-resource语句，这样你在使用流或者资源的时候，就不需要手动关闭，Java 会自动关闭。 Fork-Join 池某种程度上实现 Java 版的 Map-reduce。 允许 Switch 中有 String 变量和文本。 菱形操作符(&lt;&gt;)用于类型推断，不再需要在变量声明的右边申明泛型，因此可以写出可读写更强、更简洁的代码 JDK 1.8特性 java 8 在 Java 历史上是一个开创新的版本，下面 JDK 8 中 5 个主要的特性： Lambda 表达式，允许像对象一样传递匿名函数 Stream API，充分利用现代多核 CPU，可以写出很简洁的代码 Date 与 Time API，最终，有一个稳定、简单的日期和时间库可供你使用 扩展方法，现在，接口中可以有静态、默认方法。 重复注解，现在你可以将相同的注解在同一类型上使用多次。 Spring动态代理 代理类在程序运行时创建的代理方式被称为动态代理。代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。方法实现前后加入对应的公共功能 基于接口· jdk的动态代理时基于Java的反射机制来实现的，是Java原生的一种代理方式。他的实现原理就是让代理类和被代理类实现同一接口，代理类持有目标对象来达到方法拦截的作用。 通过接口的方式有两个弊端: 一个就是必须保证被代理类有接口 另一个就是如果相对被代理类的方法进行代理拦截，那么就要保证这些方法都要在接口中声明。接口继承的是java.lang.reflect.InvocationHandler; 基于继承 cglib动态代理使用的ASM这个非常强大的Java字节码生成框架来生成class，比jdk动态代理ide效率高。基于继承的实现动态代理，可以直接通过super关键字来调用被代理类的方法. 子类可以调用父类的方法 AOP 面向切面编程。（Aspect-Oriented Programming） 。AOP可以说是对OOP的补充和完善。 面向对象编程将程序分解成各个层次的对象，面向切面编程将程序运行过程分解成各个切面。 AOP从程序运行角度考虑程序的结构，提取业务处理过程的切面，oop是静态的抽象，aop是动态的抽象， 是对应用执行过程中的步骤进行抽象，从而获得步骤之间的逻辑划分。 OOP引入封装、 继承和多态性等概念来建立一种对象层次结构，用以模拟公共行为的一个集合。 实现AOP的技术，主要分为两大类： 一是采用动态代理技术，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行； 二是采用静态织入 的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码，属于静态代理。 面向切面编程提供声明式事务管理 spring支持用户自定义的切面， AOP框架具有的两个特征： 各个步骤之间的良好隔离性 源代码无关性 springAOP的具体加载步骤： 1、当 spring 容器启动的时候，加载了 spring 的配置文件 2、为配置文件中的所有 bean创建对象 3、spring 容器会解析 aop:config的配置 解析切入点表达式，用切入点表达式和纳入 spring容器中的 bean做匹配 如果匹配成功，则会为该 bean创建代理对象，代理对象的方法=目标方法+通知 如果匹配不成功，不会创建代理对象 4、在客户端利用context.getBean()获取对象时，如果该对象有代理对象，则返回代理对象；如果没有，则返回目标对象说明：如果目标类没有实现接口，则 spring 容器会采用 cglib 的方式产生代理对象，如果实现了接口，则会采用 jdk 的方式 IOC控制反转也叫依赖注入。IOC利用java反射机制，AOP利用代理模式。 当某个角色需要另外一个角色协助的时候，在传统的程序设计过程中，通常由调用者来创建被调用者的实例。但在spring中创建被调用者的工作不再由调用者来完成，因此称为控制反转。创建被调用者的工作由spring来完成，然后注入调用者因此也称为依赖注入。spring以动态灵活的方式来管理对象 ， 注入的两种方式，设置注入和构造注入。 设置注入的优点：直观，自然构造注入的优点：可以在构造器中决定依赖关系的顺序。 IOC概念看似很抽象，但是很容易理解。 说简单点就是将对象交给容器管理，你只需要在spring配置文件中配置对应的bean以及设置相关的属性，让spring容器来生成类的实例对象以及管理对象。在spring容器启动的时候，spring会把你在配置文件中配置的bean都初始化好，然后在你需要调用的时候，就把它已经初始化好的那些bean分配给你需要调用这些bean的类 XML–—读取––-&gt; resoure—-解析——-&gt;BeanDefinition––—注入––––-&gt;BeanFactory Spring IOC 容器源码分析 Bean的生命周期可以简述为以下九步 实例化bean对象(通过构造方法或者工厂方法) 设置对象属性(setter等)（依赖注入） 如果Bean实现了BeanNameAware接口，工厂调用Bean的setBeanName()方法传递Bean的ID。（和下面的一条均属于检查Aware接口） 如果Bean实现了BeanFactoryAware接口，工厂调用setBeanFactory()方法传入工厂自身 将Bean实例传递给Bean的前置处理器的postProcessBeforeInitialization(Object bean, String beanname)方法 调用Bean的初始化方法 将Bean实例传递给Bean的后置处理器的postProcessAfterInitialization(Object bean, String beanname)方法 使用Bean容器关闭之前，调用Bean的销毁方法 Bean的单例和多例模式的使用条件spring生成的对象默认都是单例(singleton)的.可以通过scope改成多例. 对象在整个系统中只有一份，所有的请求都用一个对象来处理，如service和dao层的对象一般是单例的。 为什么使用单例：因为没有必要每个请求都新建一个对象的时候，因为这样会浪费CPU和内存。 prototype多例模式：对象在整个系统中可以有多个实例，每个请求用一个新的对象来处理，如action。 为什么使用多例：防止并发问题；即一个请求改变了对象的状态，此时对象又处理另一个请求，而之前请求对对象的状态改变导致了对象对另一个请求做了错误的处理； Spring MVC 的处理过程（1）客户端通过url发送请求（2-3）核心控制器Dispatcher Servlet接收到请求，通过系统或自定义的映射器配置找到对应的handler，并将url映射的控制器controller返回给核心控制器。（4）通过核心控制器找到系统或默认的适配器（5-7）由找到的适配器，调用实现对应接口的处理器，并将结果返回给适配器，结果中包含数据模型和视图对象，再由适配器返回给核心控制器（8-9）核心控制器将获取的数据和视图结合的对象传递给视图解析器，获取解析得到的结果，并由视图解析器响应给核心控制器（10）核心控制器将结果返回给客户端 spring面试真题 5.1SSM各层关系5.2 为什么注入的是接口(接口多继承)5.3 Spring的优点1.降低了组件之间的耦合性 ，实现了软件各层之间的解耦2.可以使用容易提供的众多服务，如事务管理，消息服务等3.容器提供单例模式支持4.容器提供了AOP技术，利用它很容易实现如权限拦截，运行期监控等功能5.容器提供了众多的辅助类，能加快应用的开发6.spring对于主流的应用框架提供了集成支持，如hibernate，JPA，Struts等7.spring属于低侵入式设计，代码的污染极低8.独立于各种应用服务器9.spring的DI机制降低了业务对象替换的复杂性10.Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可以自由选择spring的部分或全部 redisredi知识集合 Redis 中只包含“键”和“值”两部分，只能通过“键”来查询“值”。正是因为这样简单的存储结构，也让 Redis 的读写效率非常高 数据是存储在内存中的。尽管它经常被用作内存数据库，但是，它也支持将数据存储在硬盘中 解决高并发和高性能的问题 高性能：直接处理缓存也就是处理内存很快 高并发：直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中 去，这样用户的一部分请求会直接到缓存这里而不用经过数据库 数据结构建议看数据结构部分：redis设计与实现 String常用命令: set,get,decr,incr,mget等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。 常规key-value缓存应用； 常规计数：微博数，粉丝数等。 Hash常用命令： hget,hset,hgetall等。Hash是一个 string类型的 ﬁeld和 value的映射表，hash特别适合用于存储对象，后续操作的时候，你可以直接仅 仅修改这个对象中的某个字段的值。 比如我们可以Hash数据结构来存储用户信息，商品信息等等。比如下面我就用 hash类型存放了我本人的一些信息： 1key=JavaUser293847 value=&#123; “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei” &#125; List常用命令: lpush,rpush,lpop,rpop,lrange等list 就是链表，Redis list 的应用场景非常多，也是Redis重要的数据结构之一，比如微博的关注列表，粉丝列表， 消息列表等功能都可以用Redis的 list结构来实现。Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。另外可以通过 lrange命令，就是从某个元素开始读取多少个元素，可以基于 list实现分页查询，这个很棒的一个功 能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 Set常用命令： sadd,spop,smembers,sunion等 set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在 一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常 方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下： 1sinterstore key1 key2 key3 将交集存在key1内 Sorted Set常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维 度的消息排行榜）等信息，适合使用 Redis 中的 SortedSet 结构进行存储。 底层结构string在Redis内部，String类型通过int、SDS作为结构存储,int用来存放整型数据，sds存放字 节/字符串和浮点型数据。 list压缩列表和双向循环链表 Redis3.2之后，采用的一种叫quicklist的数据结构来存储list，列表的底层都由quicklist实现。 当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件： 列表中保存的单个数据（有可能是字符串类型的）小于 64 字节； 列表中数据个数少于 512 个。 Redis 实现了自己的双端链表结构。 双端链表主要有两个作用： 作为 Redis 列表类型的底层实现之一； 作为通用数据结构，被其他功能模块所使用；（事务模块保存命令、服务器模块、订阅发送模块保存客户端、事件模块） 双端链表及其节点的性能特性如下： 节点带有前驱和后继指针，访问前驱节点和后继节点的复杂度为 O(1) ，并且对链表的迭代可以在从表头到表尾和从表尾到表头两个方向进行； 链表带有指向表头和表尾的指针，因此对表头和表尾进行处理的复杂度为 O(1)O(1) ； 链表带有记录节点数量的属性，所以可以在 O(1)O(1) 复杂度内返回链表的节点数量（长度）； quicklist仍然是一个双向链表，只是列表的每个节点都是一个ziplist，其实就是linkedlist和ziplist的结合，quicklist 中每个节点ziplist都能够存储多个数据元素 hash压缩列表和字典（散列表）（渐进式扩容缩容策略、链地址法） 同样，只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件： 字典中保存的键和值的大小都要小于 64 字节； 字典中键值对的个数要小于 512 个。 注意 dict 类型使用了两个指针，分别指向两个哈希表。 其中， 0 号哈希表（ht[0]）是字典主要使用的哈希表， 而 1 号哈希表（ht[1]）则只有在程序对 0 号哈希表进行 rehash 时才使用。 为了在字典的键值对不断增多的情况下保持良好的性能， 字典需要对所使用的哈希表（ht[0]）进行 rehash 操作： 在不修改任何键值对的情况下，对哈希表进行扩容， 尽量将比率维持在 1:1 左右。 dictAdd 在每次向字典添加新键值对之前， 都会对哈希表 ht[0] 进行检查， 对于 ht[0] 的 size 和 used 属性， 如果它们之间的比率 ratio = used / size 满足以下任何一个条件的话，rehash 过程就会被激活： 自然 rehash ： ratio &gt;= 1 ，且变量 dict_can_resize 为真。（后台持久化时为false） 强制 rehash ： ratio 大于变量 dict_force_resize_ratio （目前版本中， dict_force_resize_ratio 的值为 5 ）。 rehash执行过程（rehash后的大小至少为原来的两倍） 创建一个比 ht[0]-&gt;table 更大的 ht[1]-&gt;table ； 将 ht[0]-&gt;table 中的所有键值对迁移到 ht[1]-&gt;table ； 将原有 ht[0] 的数据清空，并将 ht[1] 替换为新的 ht[0] ； 字典的缩容 在默认情况下， REDIS_HT_MINFILL 的值为 10 ， 也即是说， 当字典的填充率低于 10% 时， 程序就可以对这个字典进行收缩操作了。 字典收缩和字典扩展的一个区别是： 字典的扩展操作是自动触发的（不管是自动扩展还是强制扩展）； 而字典的收缩操作则是由程序手动执行。 set一种是基于有序数组(整数集合)和散列表。 当要存储的数据，同时满足下面这样两个条件的时候，Redis 就采用有序数组，来实现集合这种数据类型。 存储的数据都是整数； 存储的数据元素个数不超过 512 个。 添加新元素时，如果 intsetAdd 发现新元素，不能用现有的编码方式来保存，便会将升级集合和添加新元素的任务转交给 intsetUpgradeAndAdd 来完成： intsetUpgradeAndAdd 需要完成以下几个任务： 对新元素进行检测，看保存这个新元素需要什么类型的编码； 将集合 encoding 属性的值设置为新编码类型，并根据新编码类型，对整个 contents 数组进行内存重分配。 调整 contents 数组内原有元素在内存中的排列方式，从旧编码调整为新编码。 将新元素添加到集合中。 升级 第一，从较短整数到较长整数的转换，并不会更改元素里面的值。 第二，集合编码元素的方式，由元素中长度最大的那个值来决定 inset(有序数组)，set本身是无序的 Intset 用于有序、无重复地保存多个整数值，会根据元素的值，自动选择该用什么长度的整数类型来保存元素。 当一个位长度更长的整数值添加到 intset 时，需要对 intset 进行升级，新 intset 中每个元素的位长度，会等于新添加值的位长度，但原有元素的值不变。 升级会引起整个 intset 进行内存重分配，并移动集合中的所有元素，这个操作的复杂度为 O(N） 。 Intset 只支持升级，不支持降级。 Intset 是有序的，程序使用二分查找算法来实现查找操作，复杂度为 O(lgN)O(lg⁡N) 。 zset跳表和压缩列表 当数据量比较小的时候，Redis 会用压缩列表来实现有序集合。 所有数据的大小都要小于 64字节； 元素个数要小于 128个。 跳跃表是一种随机化数据结构，查找、添加、删除操作都可以在对数期望时间下完成。 跳跃表目前在 Redis 的唯一作用，就是作为有序集类型的底层数据结构（之一，另一个构成有序集的结构是字典）。 为了满足自身的需求，Redis 基于 William Pugh 论文中描述的跳跃表进行了修改，包括： score 值可重复。 对比一个元素需要同时检查它的 score 和 memeber 。在Redis的skiplist实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。 每个节点带有高度为 1 层的后退指针，用于从表尾方向向表头方向迭代。· 压缩列表 域 长度/类型 域的值 zlbytes uint32_t 整个 ziplist 占用的内存字节数，对 ziplist 进行内存重分配，或者计算末端时使用。 zltail uint32_t 到达 ziplist 表尾节点的偏移量。 通过这个偏移量，可以在不遍历整个 ziplist 的前提下，弹出表尾节点。 zllen uint16_t ziplist 中节点的数量。 当这个值小于 UINT16_MAX （65535）时，这个值就是 ziplist 中节点的数量； 当这个值等于 UINT16_MAX 时，节点的数量需要遍历整个 ziplist 才能计算得出。 entryX ? ziplist 所保存的节点，各个节点的长度根据内容而定。 zlend uint8_t 255 的二进制值 1111 1111 （UINT8_MAX） ，用于标记 ziplist 的末端。 节点entry结构 12345area |&lt;------------------- entry --------------------&gt;| +------------------+----------+--------+---------+component | pre_entry_length | encoding | length | content | +------------------+----------+--------+---------+ pre_entry_length pre_entry_length 记录了前一个节点的长度，通过这个值，可以进行指针计算，从而跳转到上一个节点。 根据编码方式的不同， pre_entry_length 域可能占用 1 字节或者 5 字节： 1 字节：如果前一节点的长度小于 254 字节，便使用一个字节保存它的值。 5 字节：如果前一节点的长度大于等于 254 字节，那么将第 1 个字节的值设为 254 ，然后用接下来的 4 个字节保存实际长度。 encoding 和 length encoding 和 length 两部分一起决定了 content 部分所保存的数据的类型（以及长度）。 其中， encoding 域的长度为两个 bit ， 它的值可以是 00 、 01 、 10 和 11 ： 00 、 01 和 10 表示 content 部分保存着字符数组。 11 表示 content 部分保存着整数。 content content 部分保存着节点的内容，类型和长度由 encoding 和 length 决定。 添加和删除 ziplist 节点有可能会引起连锁更新，因此，添加和删除操作的最坏复杂度为 O(N2) ，不过，因为连锁更新的出现概率并不高，所以一般可以将添加和删除操作的复杂度视为 O(N)) 。 操作原理 在最后添加一个节点 在节点间添加一个节点 特殊数据结构HyperLogLog用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 Geo这个功能可以将用户给定的地理位置信息储存起来， 并对这些信息进行操作 Pub/Sub 一个Redis client发布消息,其他多个redis client订阅消息,发布的消息“即发即失”, redis不会持久保存发布的消息; 消息订阅者也将只能得到订阅之后的消息,通道中此前的消息将无从获得。 例如：哨兵机制推送活跃消息 注意：在消费者下线的情况下，生产的消息会丢失 redis过期时间 有些数据是有时间限制的例如一些登陆信息，尤其是短信验证码都是有时间限制的。定期删除+惰性删除定期删除要点：默认每隔1000ms就==随机抽取==一些设置了过期时间的key。惰性删除：定期删除会导致很多过期的key到了时间并没有被删除掉。假如过期的key靠定期删除没有删除掉，还停留在内存中，除非你的系统去查一下那个key，才会被redis删除 redis的持久化机制1.RDB BGSAVE、SAVE、 save 60 10000 、SHUTDOWN 、SYNC 快照时间：1.配置、2.用户调用save/BGSAVE、3.flushALL、4.主从复制初始化时 BGSAVE原理：fork and cow 也就是快照持久化,通过创建快照来获得存储在内存里面的数据在某个时间节点上的副本（生成dump.rdb文件）。redis创建快照后可以对快照进行备份，可以将快照复制到其他服务器从而创建出具有相同数据的服务器副本（redis主从结构，主要用来提高redis的性能），还可以将快照留在原地以便重启服务器的时候使用。注意：如果系统发生崩溃，会丢失最近快照之后的所有数据 场景 日志聚合计算 大数据 缺点：1 数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。2 备份时占用内存，因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍哦），最后再将临时文件替换之前的备份文件。 2.AOF只追加文件 appendonly yes appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速 appendfsync no #让操作系统决定何时进行同步 与快照相比AOF的实时性更好，开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件 AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 优化 在执行 BGREWRITEAOF命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作 设置 auto-aof-rewrite-percentage 和auto-aof-rewrite-min-size 恢复时redis-check-aof –fix appendonly.aof 校验文件完整性,修复破碎文件 缓存雪崩和缓存穿透缓存穿透：一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存 在，还是系统故障），我们仍然把这个空结果进行缓存（缓存过程上锁），但它的过期时间会很短，长不超过五分钟。 缓存雪崩：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 有哪些解决办法？ 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 其他方法 1，在设置Redis键的过期时间时，加上一个随机数，这样可以避免。2，部署分布式的Redis服务，当一个Redis服务器挂掉了之后，进行故障转移。 缓存击穿“: “就是说某个key非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库 解决方法 可以将热点数据设置为永远不过期； 基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据 热点key缓存中的某些Key(可能对应用与某个促销商品)对应的value存储在集群中一台机器，使得所有流量涌向同一机器，成为系统的瓶颈，该问题的挑战在于它无法通过增加机器容量来解决。 客户端热点key缓存：将热点key对应value并缓存在客户端本地，并且设置一个失效时间。 将热点key分散为多个子key，然后存储到缓存集群的不同机器上，这些子key对应的value都和热点key是一样的。 数据库与缓存数据一致性数据和缓存一致性问题 写完数据库后是否需要马上更新缓存还是直接删除缓存？ （1）、如果写数据库的值与更新到缓存值是一样的，不需要经过任何的计算，可以马上更新缓存，但是如果对于那种写数据频繁而读数据少的场景并不合适这种解决方案，因为也许还没有查询就被删除或修改了，这样会浪费时间和资源 （2）、如果写数据库的值与更新缓存的值不一致，写入缓存中的数据需要经过几个表的关联计算后得到的结果插入缓存中，那就没有必要马上更新缓存，只有删除缓存即可，等到查询的时候在去把计算后得到的结果插入到缓存中即可。 所以一般的策略是当更新数据时，先删除缓存数据，然后更新数据库，而不是更新缓存，等要查询的时候才把最新的数据更新到缓存 线程模型Redis线程模型 redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，根据 socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） I/O 多路复用程序 I/O 多路复用程序可以监听多个套接字的 ae.h/AE_READABLE 事件和 ae.h/AE_WRITABLE 事件， 这两类事件和套接字操作之间的对应关系如下： 当套接字变得可读时（客户端对套接字执行 write 操作，或者执行 close 操作）， 或者有新的可应答（acceptable）套接字出现时（客户端对服务器的监听套接字执行 connect 操作）， 套接字产生 AE_READABLE 事件。 当套接字变得可写时（客户端对套接字执行 read 操作）， 套接字产生 AE_WRITABLE 事件。 I/O 多路复用程序允许服务器同时监听套接字的 AE_READABLE 事件和 AE_WRITABLE 事件， 如果一个套接字同时产生了这两种事件， 那么文件事件分派器会优先处理 AE_READABLE 事件， 等到 AE_READABLE 事件处理完之后， 才处理 AE_WRITABLE 事件。 这也就是说， 如果一个套接字又可读又可写的话， 那么服务器将先读套接字， 后写套接字。 Redis是单线程模型为什么效率还这么高？ 纯内存访问：数据存放在内存中，内存的响应时间大约是100纳秒，这是Redis每秒万亿级别访问的重要基础。 非阻塞I/O：Redis采用epoll做为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接，读写，关闭都转换为了时间，不在I/O上浪费过多的时间。 单线程避免了线程切换和竞态产生的消耗。 Redis采用单线程模型，每条命令执行如果占用大量时间，会造成其他线程阻塞，对于Redis这种高性能服务是致命的，所以Redis是面向高速执行的数据库 redis事务Redis事务有如下一些特点: 事务中的命令序列执行的时候是原子性的,也就是说,其不会被其他客户端的命令中断. 这和传统的数据库的事务的属性是类似的. 尽管Redis事务中的命令序列是原子执行的, 但是事务中的命令序列执行可以部分成功,这种情况下,Redis事务不会执行回滚操作. 这和传统关系型数据库的事务是有区别的. redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 尽管Redis有RDB和AOF两种数据持久化机制, 但是其设计目标是高效率的cache系统. Redis事务只保证将其命令序列中的操作结果提交到内存中,不保证持久化到磁盘文件. 更进一步的, Redis事务和RDB持久化机制没有任何关系, 因为RDB机制是对内存数据结构的全量的快照.由于AOF机制是一种增量持久化,所以事务中的命令序列会提交到AOF的缓存中.但是AOF机制将其缓存写入磁盘文件是由其配置的实现策略决定的,和Redis事务没有关系. Redis事务涉及到MULTI, EXEC, DISCARD, WATCH和UNWATCH这五个命令:· 事务开始的命令是MULTI, 该命令返回OK提示信息. Redis不支持事务嵌套,执行多次MULTI命令和执行一次是相同的效果.嵌套执行MULTI命令时,Redis只是返回错误提示信息. EXEC是事务的提交命令,事务中的命令序列将被执行(或者不被执行,比如乐观锁失败等).该命令将返回响应数组,其内容对应事务中的命令执行结果. WATCH命令是开始执行乐观锁,该命令的参数是key(可以有多个), Redis将执行WATCH命令的客户端对象和key进行关联,如果其他客户端修改了这些key,则执行WATCH命令的客户端将被设置乐观锁失败的标志.该命令必须在事务开始前执行,即在执行MULTI命令前执行WATCH命令,否则执行无效,并返回错误提示信息. UNWATCH命令将取消当前客户端对象的乐观锁key,该客户端对象的事务提交将变成无条件执行. DISCARD命令将结束事务,并且会丢弃全部的命令序列. 需要注意的是,EXEC命令和DISCARD命令结束事务时,会调用UNWATCH命令,取消该客户端对象上所有的乐观锁key. 事务的错误处理 事务提交命令EXEC有可能会失败, 有三种类型的失败场景: 在事务提交之前,客户端执行的命令缓存失败.比如命令的语法错误(命令参数个数错误, 不支持的命令等等).如果发生这种类型的错误,Redis将向客户端返回包含错误提示信息的响应. 事务提交时,之前缓存的命令有可能执行失败.（==但Redis不会对事务做任何回滚补救操作==） 由于乐观锁失败,事务提交时,将丢弃之前缓存的所有命令序列. 实际上，这就意味着只有程序错误才会导致Redis命令执行失败，这种错误很有可能在程序开发期间发现，一般很少在生产环境发现。Redis已经在系统内部进行功能简化，这样可以确保更快的运行速度，因为Redis不需要事务回滚的能力。 乐观锁机制· 关于乐观锁,需要注意的是: WATCH命令必须在MULTI命令之前执行. WATCH命令可以执行多次. WATCH命令可以指定乐观锁的多个key,如果在事务过程中,任何一个key被其他客户端改变,则当前客户端的乐观锁失败,事务提交时,将丢弃所有命令序列. 多个客户端的WATCH命令可以指定相同的key. WATCH命令指定乐观锁后,可以接着执行MULTI命令进入事务上下文,也可以在WATCH命令和MULTI命令之间执行其他命令. 具体使用方式取决于场景需求,不在事务中的命令将立即被执行. 如果WATCH命令指定的乐观锁的key,被当前客户端改变,在事务提交时,乐观锁不会失败. 如果WATCH命令指定的乐观锁的key具有超时属性,并且该key在WATCH命令执行后, 在事务提交命令EXEC执行前超时, 则乐观锁不会失败.如果该key被其他客户端对象修改,则乐观锁失败. Redis事务其本质就是,以不可中断的方式依次执行缓存的命令序列,将结果保存到内存cache中 redis事务 无锁化编程一种实现思想 假如有一个上述的post请求的URI部分是个覆盖写操作，reqid=abc123789def，服务部署在多台机器，在大前端将流量转发到Nginx之后根据reqid进行哈希, 经过 Nginx负载均衡 相同reqid的请求将被转发到一台机器上，当然你可能会说如果集群的机器动态调整呢？我只能说不要考虑那么多那么充分， 工程化去设计 即可。 然而转发到一台机器仍然无法保证串行处理，因为单机仍然是多线程的，我们仍然需要将所有的reqid数据放到同一个线程处理，最终保证线程内串行，这个就需要借助于线程池的管理者Disper按照 reqid哈希取模 来进行多线程的负载均衡。 经过Nginx和线程内负载均衡，最终相同的reqid都将在线程内串行处理，有效避免了锁的使用，当然这种设计可能在reqid不均衡时造成 线程饥饿 ，不过高并发大量请求的情况下还是可以的。 分布式锁 在 分布式部署高并发场景 下，经常会遇到资源的互斥访问的问题，最有效最普遍的方法是给共享资源或者对共享资源的操作加一把锁 分布式锁是 控制分布式系统之间同步访问共享资源的一种方式 ，用于在分布式系统中协调他们之间的动作。 分布式锁一般有三种实现方式： 基于数据库在数据库中创建一张表，表里包含方法名等字段，并且在方法名字段上面创建唯一索引，执行某个方法需要使用此方法名向表中插入数据，成功插入则获取锁，执行结束则删除对应的行数据释放锁 基于缓存数据库Redis Redis性能好并且实现方便，但是单节点的分布式锁在故障迁移时产生安全问题 (在redis主从架构部署时，在redis-master实例宕机的时候，可能导致多个客户端同时完成加锁。极端情况下不能得到保证。作者都是这吗说的) Redlock是Redis的作者 Antirez 提出的集群模式分布式锁，基于N个完全独立的Redis节点实现分布式锁的高可用 基于ZooKeeper ZooKeeper是以 Paxos 算法为基础的分布式应用程序协调服务，为分布式应用提供一致性服务的开源组件 基于Redis的分布式锁和Redlock算法· 分布式锁正确实现 redisession实现 简单应用 先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放 主从+哨兵 主从模式很好的解决了数据备份问题，并且由于主从服务数据几乎是一致的，因而可以将写入数据的命令发送给主机执行，而读取数据的命令发送给不同的从机执行，从而达到读写分离的目的 Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 问题 同步故障 复制数据延迟(不一致) —-info replication 查看延迟偏移量—zookeeper监听回调机制实现客户端通知 读取过期数据(Slave 不能删除数据) —–惰性删除和定期删除—Redis3.2版本中已经解决了这个问题，在此版本中slave节点读取数据之前会检查键过期时间来决定是否返回数据的。 从节点故障 –哨兵机制 主节点故障 配置不一致 maxmemory不一致:丢失数据 优化参数不一致:内存不一致. 避免全量复制 选择小主节点(分片)、低峰期间操作.（防止第一次全量复制压力过大） 如果节点运行 id 不匹配(如主节点重启、运行 id 发送变化)，此时要执行全量复制，应该配合哨兵和集群解决. 主从复制挤压缓冲区不足产生的问题(网络中断，部分复制无法满足)，可增大复制缓冲区( rel_backlog_size参数).· 复制风暴 复制风暴是指大量从节点对同一主节点或者同一台机器的多个主节点，在短时间内发起全量复制的过程。此时将导致被发起的主节点或机器产生大量开销，如 ：CPU、内存、硬盘、带宽等 单节点复制风暴 —首先减少主节点挂在从节点的数量，或者采用树桩复制结构。 单机复制风暴 —集群和多节点部署—注意故障恢复机制，防止恢复时出现密集全量复制 主从故障如何故障转移· a)主节点(master)故障，从节点slave-1端执行slaveof no one后变成新主节点；b)其它的节点成为新主节点的从节点，并从新节点复制数据；c)需要人工干预，无法实现高可用。 配置 12345678#配置主从节点127.0.0.1:6380&gt; slaveof 127.0.0.1 6379#配置哨兵（配置sentinel.conf）#①每个sentinel的myid参数也要进行修改，因为sentinel之间是通过该属性来唯一区分其他sentinel节点的；#②参数中sentinel monitor mymaster 127.0.0.1 6379 2这里的端口号6379是不用更改的，因为sentinel是通过检测主节点的状态来得知当前主节点的从节点有哪些的，因而设置为主节点的端口号即可./src/redis-sentinel sentinel-26379.conf#查看状态info replication 哨兵监控机制 任务1：每个哨兵节点每10秒会向主节点和从节点发送info命令获取拓扑结构图 任务2：每个哨兵节点每隔2秒会向redis数据节点的指定频道上发送该哨兵节点对于主节点的判断以及当前哨兵节点的信息，同时每个哨兵节点也会订阅该频道，来了解其它哨兵节点的信息及对主节点的判断，其实就是通过消息publish和subscribe来完成的· 任务3：每隔1秒每个哨兵会向主节点、从节点及其余哨兵节点发送一次ping命令做一次心跳检测，这个也是哨兵用来判断节点是否正常的重要依据 领导者哨兵选举流程 a)每个在线的哨兵节点都可以成为领导者，当它确认（比如哨兵3）主节点下线时（主观下线），会向其它哨兵发is-master-down-by-addr命令，征求判断并要求将自己·设置为领导者，由领导者处理故障转移；b)当其它哨兵收到此命令时，可以同意或者拒绝它成为领导者；c)如果哨兵3发现自己在选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举…………(客观下线)\ 故障转移· 选择 slave-priority 最高的节点。 选择复制偏移量最大的节点(同步数据最多)。 选择 runId最小的节点。 部署建议 a，sentinel节点应部署在多台物理机（线上环境） b，至少三个且奇数个sentinel节点 c，通过以上我们知道，3个sentinel可同时监控一个主节点或多个主节点 监听N个主节点较多时，如果sentinel出现异常，会对多个主节点有影响，同时还会造成sentinel节点产生过多的网络连接， 一般线上建议还是， 3个sentinel监听一个主节点 数据同步 redis 2.8版本以上使用·命令完成同步，过程分“全量”与“部分”复制 全量复制：一般用于初次复制场景（第一次建立SLAVE后全量） 部分复制：网络出现问题，从节点再次连接主节点时，主节点补发缺少的数据，每次数据增量同步 心跳：主从有长连接心跳，主节点默认每10S向从节点发ping命令，repl-ping-slave-period控制发送频率· 集群 当遇到单机内存，并发和流量瓶颈等问题时，可采用Cluster方案达到负载均衡的目的。并且从另一方面讲，redis中sentinel有效的解决了故障转移的问题，也解决了主节点下线客户端无法识别新的可用节点的问题，但是如果是从节点下线了，sentinel是不会对其进行故障转移的，并且连接从节点的客户端也无法获取到新的可用从节点 redis集群中数据是和槽（slot）挂钩的，其总共定义了16384个槽，所有的数据根据一致哈希算法会被映射到这16384个槽中的某个槽中。 slot=CRC16（key）/16384 数据的存储只和槽有关，并且槽的数量是一定的，由于一致hash算法·是一定的，因而将这16384个槽分配给无论多少个redis实例，对于确认的数据其都将被分配到确定的槽位上。redis集群通过这种方式来达到redis的高效和高可用性目的。 一致性哈希和哈希槽的区别·一致性哈希是创建虚拟节点来实现节点宕机后的数据转移并保证数据的安全性和集群的可用性的。 redis cluster是采用master节点有多个slave节点机制来保证数据的完整性的,master节点写入数据，slave节点同步数据。当master节点挂机后，slave节点会通过选举机制选举出一个节点变成master节点，实现高可用。但是这里有一点需要考虑，如果master节点存在热点缓存，某一个时刻某个key的访问急剧增高，这时该mater节点可能操劳过度而死，随后从节点选举为主节点后，同样宕机，一次类推，造成缓存雪崩 配置 123456789#redis.conf配置文件设置port 6379cluster-enabled yescluster-node-timeout 15000cluster-config-file "nodes-6379.conf"pidfile /var/run/redis_6379.pidlogfile "cluster-6379.log"dbfilename dump-cluster-6379.rdbappendfilename "appendonly-cluster-6379.aof" 123#启动配置文件./src/redis-server cluster-6379.conf... 123456#设置槽位#先连接./src/redis-cli -p 6379#设置多个槽位127.0.0.1:6379&gt;cluster meet 127.0.0.1 6380... 12345678#查看状态127.0.0.1:6379&gt; cluster nodes4fa7eac4080f0b667ffeab9b87841da49b84a6e4 127.0.0.1:6384 master - 0 1468073975551 5 connectedcfb28ef1deee4e0fa78da86abe5d24566744411e 127.0.0.1:6379 myself,master - 0 0 0 connectedbe9485a6a729fc98c5151374bc30277e89a461d8 127.0.0.1:6383 master - 0 1468073978579 4 connected40622f9e7adc8ebd77fca0de9edfe691cb8a74fb 127.0.0.1:6382 master - 0 1468073980598 3 connected8e41673d59c9568aa9d29fb174ce733345b3e8f1 127.0.0.1:6380 master - 0 1468073974541 1 connected40b8d09d44294d2e23c7c768efc8fcd153446746 127.0.0.1:6381 master - 0 1468073979589 2 connected 123#添加虚拟槽(类似于一致性hash提供给虚拟节点)127.0.0.1:6379&gt;cluster addslots &#123;0...5461&#125;#...之后我们还可以分配主从节点，进一步提高可靠性 其他集群案例 Redis Sharding集群 采用一致性哈希算法(consistent hashing)，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。 为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。 ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。 特点：resharding·，即预先根据系统规模尽量部署好多个Redis实例，这些实例占用系统资源很小，一台物理机可部署多个，让他们都参与sharding，当需要扩容时，选中一个实例作为主节点，新加入的Redis节点作为从节点进行数据复制。 presharding是预先分配好足够的分片，扩容时只是将属于某一分片的原Redis实例替换成新的容量更大的Redis实例。参与sharding的分片没有改变，所以也就不存在key值从一个区转移到另一个分片区的现象，只是将属于同分片区的键值从原Redis实例同步到新Redis实例。 使用场景 热点数据缓存 会话维持session 分布式锁SETNX 表缓存 消息队列 list()提供阻塞方法 计数器 string 注意点关注最后的开发建议：不要用keys 大家知道 Redis 是单线程程序，是按照顺序执行指令的，如果说我们现在正在执行 keys 命令，那么其它指令必须等到当前的 keys 指令执行完了才可以继续，再加上 keys 操作是遍历算法，复杂度是 O (n)，乍一想就知道问题所在了，当实例中数据量过大的时候，Redis 服务可能会卡顿，其余指令可能会延时甚至超时报错…. 使用：scan - cursor [MATCH pattern] [COUNT count]· 复杂度虽然也是 O (n)，但是它是通过游标分步进行的，不会阻塞线程； scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 scan用法 IONIO浅析 分类java提供的API IO模型:IO NIO AIO Java中提供的IO有关的API，在文件处理的时候，其实依赖操作系统层面的IO操作实现的。比如在Linux 2.6以后，Java中NIO和AIO都是通过epoll来实现的，而在Windows上，AIO是通过IOCP来实现的。 可以把Java中的BIO、NIO和AIO理解为是Java语言对操作系统的各种IO模型的封装。程序员在使用这些API的时候，不需要关心操作系统层面的知识，也不需要根据不同操作系统编写不同的代码。只需要使用Java的API就可以了。 操作系统层面的IO模型: 阻塞IO模型、非阻塞IO模型、IO复用模型、信号驱动IO模型以及异步IO模型。 阻塞式IO 应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就会阻塞住，直到内核准备好数据报，recvfrom 完成数据报复制工作，应用进程才能结束阻塞状态。 非阻塞式IO 应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好，内核会返回error，应用进程在得到error后，过一段时间再发送recvfrom请求。在两次发送请求的时间段，进程可以先做别的事情。 信号驱动IO模型 应用进程预先向内核注册一个信号处理函数，然后用户进程返回，并且不阻塞，当内核数据准备就绪时会发送一个信号给进程，用户进程便在信号处理函数中开始把数据拷贝的用户空间中。 IO复用模型 IO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。 这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。 异步IO模型 上述IO模型的数据拷贝过程，都是同步进行的。(信号驱动IO模型数据准备阶段式异步的但是拷贝依然是同步的) 用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。 IO、NIO、AIO区别BIO: 用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接 BIO缺点 线程的创建和销毁成本很高，在Linux这样的操作系统中，线程本质上就是一个进程。创建和销毁都是重量级的系统函数。 线程本身占用较大内存，像Java的线程栈，一般至少分配512K～1M的空间，如果系统中的线程数过千，恐怕整个JVM的内存都会被吃掉一半。 线程的切换成本是很高的。操作系统发生线程切换的时候，需要保留线程的上下文，然后执行系统调用。如果线程数过高，可能执行线程切换的时间甚至会大于线程执行的时间，这时候带来的表现往往是系统load偏高、CPU sy使用率特别高（超过20%以上)，导致系统几乎陷入不可用的状态。 容易造成锯齿状的系统负载。因为系统负载是用活动线程数或CPU核心数，一旦线程数量高但外部网络环境不是很稳定，就很容易造成大量请求的结果同时返回，激活大量阻塞线程从而使系统负载压力过大。 NIO : NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。 NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 NIO特点 事件驱动模型 避免多线程 单线程处理多任务 非阻塞I/O，I/O读写不再阻塞，而是返回0（指channel操作的时候可以选择注册成非阻塞） 基于block的传输，通常比基于流的传输更高效 更高级的IO函数，zero-copy IO多路复用大大提高了Java网络应用的可伸缩性和实用性 NIO由原来的阻塞读写（占用线程）变成了单线程轮询事件，找到可以进行读写的网络描述符进行读写。除了事件的轮询是阻塞的（没有可干的事情必须要阻塞），剩余的I/O操作都是纯CPU操作，没有必要开启多线程。 AIO: 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。 在 Windows 中 JDK 直接采用了 IOCP 的支持 linux中使用的epoll NIO的三个主要组成部分 Channel（通道）、Buffer（缓冲区）、Selector（选择器） ChannelChannel（通道）：Channel是一个对象，可以通过它读取和写入数据。可以把它看做是IO中的流，不同的是： Channel是双向的，既可以读又可以写，而流是单向的 Channel可以进行异步的读写 对Channel的读写必须通过buffer对象 在Java NIO中的Channel主要有如下几种类型： FileChannel：从文件读取数据的（没有异步模式） DatagramChannel：读写UDP网络协议数据 SocketChannel：读写TCP网络协议数据 ServerSocketChannel：可以监听TCP连接 FileChannel读取文件内容： 123ByteBuffer buffer = ByteBuffer.allocate(1024);int num = fileChannel.read(buffer); 前面我们也说了，所有的 Channel 都是和 Buffer 打交道的。 写入文件内容： 12345678ByteBuffer buffer = ByteBuffer.allocate(1024);buffer.put("随机写入一些内容到 Buffer 中".getBytes());// Buffer 切换为读模式buffer.flip();while(buffer.hasRemaining()) &#123; // 将 Buffer 中的内容写入文件 fileChannel.write(buffer);&#125; SocketChannel打开一个 TCP 连接： 1SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress("https://www.javadoop.com", 80)); 当然了，上面的这行代码等价于下面的两行： 1234// 打开一个通道SocketChannel socketChannel = SocketChannel.open();// 发起连接socketChannel.connect(new InetSocketAddress("https://www.javadoop.com", 80)); SocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区。 1234567// 读取数据socketChannel.read(buffer);// 写入数据到网络连接中while(buffer.hasRemaining()) &#123; socketChannel.write(buffer); &#125; ServerSocketChannel之前说 SocketChannel 是 TCP 客户端，这里说的 ServerSocketChannel 就是对应的服务端。 ServerSocketChannel 用于监听机器端口，管理从这个端口进来的 TCP 连接。 123456789// 实例化ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 监听 8080 端口serverSocketChannel.socket().bind(new InetSocketAddress(8080));while (true) &#123; // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理 SocketChannel socketChannel = serverSocketChannel.accept();&#125; ServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接。 DatagramChannelUDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端。 监听端口： 12345DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9090));ByteBuffer buf = ByteBuffer.allocate(48);channel.receive(buf); 发送数据： 12345678String newData = "New String to write to file..." + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress("jenkov.com", 80)); BufferBuffer是一个对象，它包含一些要写入或者读到Stream对象的。应用程序不能直接对 Channel 进行读写操作，而必须通过 Buffer 来进行，即 Channel 是通过 Buffer 来读写数据的。 在NIO中，所有的数据都是用Buffer处理的，它是NIO读写数据的中转池。Buffer实质上是一个数组，通常是一个字节数据，但也可以是其他类型的数组。但一个缓冲区不仅仅是一个数组，重要的是它提供了对数据的结构化访问，而且还可以跟踪系统的读写进程。 使用 Buffer读写数据一zes般遵循以下四个步骤： 1.写入数据到 Buffer； 2.调用 flip()方法； 3.从 Buffer中读取数据； 4.调用 clear()方法或者compact() 方法。 当向 Buffer 写入数据时，Buffer 会记录下写了多少数据。一旦要读取数据，需要通过 flip()方法将 Buffer 从写模式切换到读模式。在读模式下，可以读取之前写入到 Buffer 的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用 clear() 或 compact() 方法。 clear() 方法会清空整个缓冲区。 compact() 方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 Buffer主要有如下几种 ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 重要属性 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。 position 的初始值是 0，每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置。读操作的时候也是类似的，每读一个值，position 就自动加 1。 从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。 Limit：写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了。 123456public final Buffer flip() &#123; limit = position; // 将 limit 设置为实际写入的数据数量 position = 0; // 重置 position 为 0 mark = -1; // mark 之后再说 return this;&#125; mark() &amp; reset() 除了 position、limit、capacity 这三个基本的属性外，还有一个常用的属性就是 mark。 mark 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用。reset用于返回保存值 rewind() &amp; clear() &amp; compact() rewind()：会重置 position 为 0，通常用于重新从头读写 Buffer。 clear()：有点重置 Buffer 的意思，相当于重新实例化了一样 compact()：和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用。但是不同在于，调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向原来数据的右边。 Selector（选择器对象）首先需要了解一件事情就是线程上下文切换开销会在高并发时变得很明显，这是同步阻塞方式的低扩展性劣势· Selector是一个对象，它可以注册到很多个Channel上，监听各个Channel上发生的事件，并且能够根据事件情况决定Channel读写。这样，通过一个线程管理多个Channel，就可以处理大量网络连接了。 有了Selector，我们就可以利用一个线程来处理所有的channels。线程之间的切换对操作系统来说代价是很高的，并且每个线程也会占用一定的系统资源。所以，对系统来说使用的线程越少越好。 1.如何创建一个Selector Selector 就是您注册对各种 I/O 事件兴趣的地方，而且当那些事件发生时，就是这个对象告诉您所发生的事件。 1Selector selector = Selector.open(); 2.注册Channel到Selector 为了能让Channel和Selector配合使用，我们需要把Channel注册到Selector上。通过调用 channel.register（）方法来实现注册： 12channel.configureBlocking(false);SelectionKey key =channel.register(selector,SelectionKey.OP_READ); 注意，注册的Channel 必须设置成非阻塞模式 才可以,否则异步IO就无法工作，这就意味着我们不能把一个FileChannel注册到Selector，因为FileChannel没有非阻塞模式，但是网络编程中的SocketChannel是可以的。 3.调用 select()方法获取通道信息。用于判断是否有我们感兴趣的事件已经发生了。 事件 SelectionKey.OP_READ 对应 00000001，通道中有数据可以进行读取 SelectionKey.OP_WRITE 对应 00000100，可以往通道中写入数据 SelectionKey.OP_CONNECT 对应 00001000，成功建立 TCP 连接 SelectionKey.OP_ACCEPT 对应 00010000，接受 TCP 连接 我们可以同时监听一个 Channel 中的发生的多个事件，比如我们要监听 ACCEPT 和 READ 事件，那么指定参数为二进制的 00010001 即十进制数值 17 即可。 注册方法返回值是 SelectionKey 实例，它包含了 Channel 和 Selector 信息，也包括了一个叫做 Interest Set 的信息，即我们设置的我们感兴趣的正在监听的事件集合。 SelectionKey 请注意对register()的调用的返回值是一个SelectionKey。 SelectionKey代表这个通道在此 Selector上注册。当某个 Selector通知您某个传入事件时，它是通过提供对应于该事件的 SelectionKey来进行的。SelectionKey还可以用于取消通道的注册。 SelectionKey中包含如下属性： The interest set The ready set The Channel The Selector An attached object (optional) 新技术NIO的特性 IO是面向流的，NIO是面向缓冲的； IO是阻塞的，NIO是非阻塞的； IO是单线程的，NIO 是通过选择器来模拟多线程的； 内存映射内存映射文件(memory-mappedfile)能让你创建和修改那些大到无法读入内存的文件。有了内存映射文件，你就可以认为文件已经全部读进了内存，然后把它当成一个非常大的数组来访问了。将文件的一段区域映射到内存中，比传统的文件处理速度要快很多。内存映射文件它虽然最终也是要从磁盘读取数据，但是它并不需要将数据读取到OS内核缓冲区，而是直接将进程的用户私有地址空间中的一部分区域与文件对象建立起映射关系，就好像直接从内存中读、写文件一样，速度当然快了。 NIO中内存映射主要用到以下两个类： java.nio.MappedByteBuffer java.nio.channels.FileChannel 支持三种模式:只读,只写,私有 内存映射文件的优点： 用户进程将文件数据视为内存，因此不需要发出read()或write()系统调用。 当用户进程触摸映射的内存空间时，将自动生成页面错误，以从磁盘引入文件数据。 如果用修改映射的内存空间，受影响的页面将自动标记为脏，并随后刷新到磁盘以更新文件。 操作系统的虚拟内存子系统将执行页面的智能缓存，根据系统负载自动管理内存。 数据始终是页面对齐的，不需要缓冲区复制。 可以映射非常大的文件，而不消耗大量内存来复制数据。 字符和编码大部分的操作系统在I/O与文件存储方面仍是以字节为导向的，所以无论使用何种编码，Unicode或其他编码，在字节序列和字符集编码之间仍需要进行转化。 在NIO中提供了两个类CharsetEncoder和CharsetDecoder来实现编码转换方案 CharsetEncoder类是一个状态编码引擎。实际上，编码器有状态意味着它们不是线程安全的 12345678910111213141516171819202122//nio字符集编码public class testCharacter &#123; public static void main(String[] a)&#123; //设置编码器 Charset charset = Charset.forName("GBK"); //获取缓冲器 CharBuffer charBuffer = CharBuffer.allocate(1024); charBuffer.put("skdfns史可法你00"); //编码 charBuffer.flip(); ByteBuffer byteBuffer = charset.encode(charBuffer); for(int i = 0;i &lt; byteBuffer.limit();i ++)&#123; System.out.println(byteBuffer.get()); &#125; //解码 byteBuffer.flip(); CharBuffer charBuffer1 = charset.decode(byteBuffer); for(int i = 0;i &lt; charBuffer1.limit();i ++)&#123; System.out.println(charBuffer1.get()); &#125; &#125;&#125; 非阻塞IONIO 的非阻塞 I/O 机制是围绕 选择器和 通道构建的。 Channel 类表示服务器和客户机之间的一种通信机制。Selector 类是 Channel 的多路复用器。 Selector 类将传入客户机请求多路分用并将它们分派到各自的请求处理程序。NIO 设计背后的基石是反应器(Reactor)设计模式。 Reactor负责IO事件的响应，一旦有事件发生，便广播发送给相应的handler去处理 在Reactor模式中，包含如下角色： Reactor 将I/O事件发派给对应的Handler Acceptor 处理客户端连接请求 Handlers 执行非阻塞读/写 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class NIOServer &#123; private static final Logger LOGGER = LoggerFactory.getLogger(NIOServer.class); public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.configureBlocking(false); serverSocketChannel.bind(new InetSocketAddress(1234)); serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; if(selector.selectNow() &lt; 0) &#123; continue; &#125; //获取注册的channel Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); //遍历所有的key Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while(iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); //如果通道上有事件发生 if (key.isAcceptable()) &#123; //获取该通道 ServerSocketChannel acceptServerSocketChannel = (ServerSocketChannel) key.channel(); SocketChannel socketChannel = acceptServerSocketChannel.accept(); socketChannel.configureBlocking(false); LOGGER.info("Accept request from &#123;&#125;", socketChannel.getRemoteAddress()); //同时将SelectionKey标记为可读，以便读取。 SelectionKey readKey = socketChannel.register(selector, SelectionKey.OP_READ); //利用SelectionKey的attache功能绑定Acceptor 如果有事情，触发Acceptor //Processor对象为自定义处理请求的类 readKey.attach(new Processor()); &#125; else if (key.isReadable()) &#123; Processor processor = (Processor) key.attachment(); processor.process(key); &#125; &#125; &#125; &#125;&#125;/** * Processor类中设置一个线程池来处理请求， * 这样就可以充分利用多线程的优势 */class Processor &#123; private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class); private static final ExecutorService service = Executors.newFixedThreadPool(16); public void process(final SelectionKey selectionKey) &#123; service.submit(new Runnable() &#123; @Override public void run() &#123; ByteBuffer buffer = null; SocketChannel socketChannel = null; try &#123; buffer = ByteBuffer.allocate(1024); socketChannel = (SocketChannel) selectionKey.channel(); int count = socketChannel.read(buffer); if (count &lt; 0) &#123; socketChannel.close(); selectionKey.cancel(); LOGGER.info("&#123;&#125;\t Read ended", socketChannel); &#125; else if(count == 0) &#123; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; LOGGER.info("&#123;&#125;\t Read message &#123;&#125;", socketChannel, new String(buffer.array())); &#125; &#125;); &#125;&#125; 多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态时通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。 文件锁定12345678910// 如果请求的锁定范围是有效的，阻塞直至获取锁 public final FileLock lock() // 尝试获取锁非阻塞，立刻返回结果 public final FileLock tryLock() // 第一个参数：要锁定区域的起始位置 // 第二个参数：要锁定区域的尺寸, // 第三个参数：true为共享锁，false为独占锁 public abstract FileLock lock (long position, long size, boolean shared) public abstract FileLock tryLock (long position, long size, boolean shared) NIO AsynchronousFileChannel异步文件通道AIOJava 异步 IO 提供了两种使用方式，分别是返回 Future 实例和使用回调函数。 返回Future实例 future.isDone(); 判断操作是否已经完成，包括了正常完成、异常抛出、取消 future.cancel(true); 取消操作，方式是中断。参数 true 说的是，即使这个任务正在执行，也会进行中断。 future.isCancelled(); 是否被取消，只有在任务正常结束之前被取消，这个方法才会返回 true future.get(); 这是我们的老朋友，获取执行结果，阻塞。 future.get(10, TimeUnit.SECONDS); 如果上面的 get() 方法的阻塞你不满意，那就设置个超时时间。 提供 CompletionHandler 回调函数 用法 注意，参数上有个 attachment，虽然不常用，我们可以在各个支持的方法中传递这个参数值 123456789101112AsynchronousServerSocketChannel listener = AsynchronousServerSocketChannel.open().bind(null);// accept 方法的第一个参数可以传递 attachmentlistener.accept(attachment, new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; public void completed( AsynchronousSocketChannel client, Object attachment) &#123; // &#125; public void failed(Throwable exc, Object attachment) &#123; // &#125;&#125;); ChannelAsynchronousFileChannelAIO 的读写主要也还是与 Buffer 打交道，这个与 NIO 是一脉相承的。 另外，还提供了用于将内存中的数据刷入到磁盘的方法： 1public abstract void force(boolean metaData) throws IOException; 因为我们对文件的写操作，操作系统并不会直接针对文件操作，系统会缓存，然后周期性地刷入到磁盘。如果希望将数据及时写入到磁盘中，以免断电引发部分数据丢失，可以调用此方法。参数如果设置为 true，意味着同时也将文件属性信息更新到磁盘。 还有，还提供了对文件的锁定功能，我们可以锁定文件的部分数据，这样可以进行排他性的操作。 1public abstract Future&lt;FileLock&gt; lock(long position, long size, boolean shared); position 是要锁定内容的开始位置，size 指示了要锁定的区域大小，shared 指示需要的是共享锁还是排他锁 注意: AsynchronousFileChannels 不属于 group。但是它们也是关联到一个线程池的，如果不指定，会使用系统默认的线程池，如果想要使用指定的线程池，可以在实例化的时候使用以下方法： 123456public static AsynchronousFileChannel open(Path file, Set&lt;? extends OpenOption&gt; options, ExecutorService executor, FileAttribute&lt;?&gt;... attrs) &#123; ...&#125; AsynchronousServerSocketChannelAsynchronousSocketChannelAsynchronous Channel Groups 异步 IO 一定存在一个线程池，这个线程池负责接收任务、处理 IO 事件、回调等。这个线程池就在 group 内部，group 一旦关闭，那么相应的线程池就会关闭。 AsynchronousServerSocketChannels 和 AsynchronousSocketChannels 是属于 group 的，当我们调用 AsynchronousServerSocketChannel 或 AsynchronousSocketChannel 的 open() 方法的时候，相应的 channel 就属于默认的 group，这个 group 由 JVM 自动构造并管理。 想要使用自己定义的 group，这样可以对其中的线程进行更多的控制，使用以下几个方法即可： AsynchronousChannelGroup.withCachedThreadPool(ExecutorService executor, int initialSize) AsynchronousChannelGroup.withFixedJava 非阻塞 IO 和异步 IOThreadPool(int nThreads, ThreadFactory threadFactory) AsynchronousChannelGroup.withThreadPool(ExecutorService executor) Java 非阻塞 IO 和异步 IO 事件驱动模型和消息驱动模型事件驱驱动架构由三个基本组件构成，事件、事件处理器、事件循环。事件产生后发送给事件循环，事件循环将每个事件分派给个各个事件处理器。事件A由处理器A处理，事件B将被处理器B处理。 select poll epoll ServletWeb容器servlet工作原理之tomcat篇 Web容器是一种服务程序，给处于其中的应用程序组件提供环境，使其直接跟容器中的环境变量交互，不必关注其它系统问题。主要由应用服务器来实现，如Tomcat、JBoss、Weblogic、WebSphere等。 Servlet容器的主要任务是管理Servlet的生命周期，而Web容器主要任务是管理Web应用程序。 一个web应用对应一个context容器，添加一个应用时将会创建一个StandardContext容器，并且给这个context容器设置必要的参数，url和path分别代表这个应用在tomcat中的访问路径和这个应用实际的物理路径。其中最重要的一个配置是ContextConfig，这个类将会负责整个web应用配置的解析工作，最后将这个context容器加到父容器host中。 servlet 抽象类HttpServlet继承抽象类GenericServlet，其有两个比较关键的方法，doGet()和doPost() GenericServlet实现接口Servlet,ServletConfig,Serializable MyServlet(用户自定义Servlet类)继承HttpServlet，重写抽象类HttpServlet的doGet()和doPost()方法 注：任何一个用户自定义Servlet，只需重写抽象类HttpServlet的doPost()和doGet()即可 容器中的执行过程 Servlet只有放在容器中，方可执行，且Servlet容器种类较多，如Tomcat,WebLogic等。下图为简单的 请求响应 模型。 分析： 1.浏览器向服务器发出GET请求(请求服务器ServletA) 2.服务器上的容器逻辑接收到该url,根据该url判断为Servlet请求，此时容器逻辑将产生两个对象：请求对象(HttpServletRequest)和响应对象(HttpServletResponce) 3.容器逻辑根据url找到目标Servlet(本示例目标Servlet为ServletA),且创建一个线程A 4.容器逻辑将刚才创建的请求对象和响应对象传递给线程A 5.容器逻辑调用Servlet的service()方法 6.service()方法根据请求类型(本示例为GET请求)调用doGet()(本示例调用doGet())或doPost()方法 7.doGet()执行完后，将结果返回给容器逻辑 8.线程A被销毁或被放在线程池中 注意： 1.在容器中的每个Servlet原则上只有一个实例 2.每个请求对应一个线程 3.多个线程可作用于同一个Servlet(这是造成Servlet线程不安全的根本原因) 4.每个线程一旦执行完任务，就被销毁或放在线程池中等待回收 在JavaWeb中扮演的角色 Servlet在JavaWeb中，扮演两个角色：页面角色和控制器角色(更多)。 生命周期 第一步：容器先加载Servlet类 第二步：容器实例化Servlet(Servlet无参构造函数执行) 第三步：执行init()方法（在Servlet生命周期中，只执行一次，且在service()方法执行前执行） 第四步：执行service()方法，处理客户请求，doPost()或doGet() 第五步：执行destroy()，销毁线程 BigDecimal 保留两位小数的方法 BigDecimal的setScal()方法 System.out.println(“%2f”,a); NumberFormat DecimalFormat的format方法 1234567891011121314151617181920212223public class Format&#123; double f = 111231.5585; public void m1()&#123; BigDecimal bg = new BigDecimal(f); double f1 = bg.setScale(2,BigDecimal.ROUND_HALF_UP).doubleValue(); &#125; public void m2()&#123; DecimalFormat df = new DecimalFormat("#.00"); Sytem.out.println(df.format(f)); &#125; public void m3()&#123; System.out.printlin(String.format()".%2f",f); &#125; public void m4() &#123; NumberFormat nf = NumberFormat.getNumberInstance(); nf.setMaximumFractionDigits(2); System.out.println(nf.format(f)); &#125;&#125; 当double必须用作BigDecimal的源时，请使用Double.toString(double)转成String，然后`使用String构造方法，或使用BigDecimal的静态方法valueOf，如下 123456789public static void main(String[] args) &#123; BigDecimal bDouble1 = BigDecimal.valueOf(2.3); BigDecimal bDouble2 = new BigDecimal(Double.toString(2.3)); System.out.println("bDouble1=" + bDouble1); System.out.println("bDouble2=" + bDouble2); &#125; 四则运算1public BigDecimal divide(BigDecimal divisor, int scale, int roundingMode) 123456789101112131415ROUND_CEILING //向正无穷方向舍入ROUND_DOWN //向零方向舍入ROUND_FLOOR //向负无穷方向舍入ROUND_HALF_DOWN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向下舍入, 例如1.55 保留一位小数结果为1.5ROUND_HALF_EVEN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，如果保留位数是奇数，使用ROUND_HALF_UP，如果是偶数，使用ROUND_HALF_DOWNROUND_HALF_UP //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向上舍入, 1.55保留一位小数结果为1.6ROUND_UNNECESSARY //计算结果是精确的，不需要舍入模式ROUND_UP //向远离0的方向舍入 (1)商业计算使用BigDecimal。 (2)尽量使用参数类型为String的构造函数。 (3) BigDecimal都是不可变的（immutable）的，在进行每一步运算时，都会产生一个新的对象，所以在做加减乘除运算时千万要保存操作后的值。 枚举EnumSet和EnumMapEnumMap是专门为枚举类型量身定做的Map实现。虽然使用其它的Map实现（如HashMap）也能完成枚举类型实例到值得映射，但是使用EnumMap会更加高效：它只能接收同一枚举类型的实例作为键值，并且由于枚举类型实例的数量相对固定并且有限，所以EnumMap使用数组来存放与枚举类型对应的值。这使得EnumMap的效率非常高。EnumMap在内部使用枚举类型的ordinal()得到当前实例的声明次序，并使用这个次序维护枚举类型实例对应值在数组的位置。 1、父类为AbstractMap，未实现Map接口，只实现了Cloneable和Serializable接口。 2、非线程安全，所有方法和操作都未加锁。 3、采用key数组和vals数组共同实现key和value的关联。 4、不允许null key，但允许null value。 5、null值会被转换为Object的NULL实例占位替换。 6、元素的存储顺序按照枚举值的声明次序存储。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建一个防火墙nat拓扑]]></title>
    <url>%2F2019%2F10%2F15%2Fnat-topo%2F</url>
    <content type="text"><![CDATA[ensp真是有毒。。昨天死活ping不通的路由今天重启就可以ping通了？？亏我昨天排查了一晚上。。 记录一次搭建大型的网络拓扑，先看看总的拓扑结构： 这是一个大型公司的组网方案，总部放在内网192.168.0.0， 这里以四个PC作为代替的四个分部门，四部门之间可以配置VLAN实现互通或者限制某一部门不通。 连接部门的是两台核心交换机，作为汇聚层。交换机上实现主备备份并在关键链路实现链路聚合。同时，核心交换机还承担着DHCP服务器的作用，向下面的四个部门分配IP地址并实现三层交换 防火墙添加了NAT策略并实现了双机热备+心跳检测，由于开启了web视图，因此配置起来变得更加简单 外网方面，AR作为核心路由器汇聚了外网服务器机群和分部门 其中一个子公司使用PPP 协议+CHAP链路加密实现和AR的通信；另外一个子公司使用帧中继于AR通信；路由协议使用的是ospf，为了防环设置了选择了子公司不同的area与核心路由器area0相连 防火墙NAT策略 踩坑 +]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>ICT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程、IO流与反射]]></title>
    <url>%2F2019%2F10%2F12%2Fmutithread%2F</url>
    <content type="text"><![CDATA[旧笔记补档 多线程/IO CPU是以时间片的方式为进程分配CPU处理时间的,为了提高CPU的使用率，采用多线程的方式去同时完成几件事情而互不干扰,从而提高CPU的使用效率 多线程的使用场景 多线程同时完成多个任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//多线程核心代码 public void run()&#123; FileInputStream in = null; FileOutputStream out = null; try &#123; in = new FileInputStream(src); out = new FileOutputStream(dest); int buf = 1024; byte[] b = new byte[buf]; int lenth = 0; long len = src.length();// //已复制文件的字节数 double temp = 0; //数字格式化，显示百分比 DecimalFormat df = new DecimalFormat("##.00%"); while ((lenth = in.read(b)) != -1) &#123; out.write(b,0,lenth); //获取已下载的大小，并且转换成百分比 temp += lenth; double d = temp/len; System.out.println(src.getName()+"以复制进度"+df.format(d)); //线程阻塞，实现同步// Thread.sleep(1); if (len - temp &lt; buf) &#123; buf = (int) (len - temp); if (buf == 0) &#123; break; &#125; b = new byte[buf]; &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(in !=null)&#123; in.close(); &#125; if(out != null)&#123; out.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 多线程协同完成单一任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//多线程核心代码public void run() &#123; //可以从任意位置进行读写文件 RandomAccessFile in =null; RandomAccessFile out = null; try &#123; in = new RandomAccessFile(src, "r"); out = new RandomAccessFile(dest, "rw"); in.seek(startPos); out.seek(endPos); int size = endPos-startPos; int lenth=0; double temp =0;// 判断剩余长度是否大于1024 int bufLen = (int) (size &lt; 1024 ? src.length() : 1024);// int bufLen = 1024; byte[] b = new byte[bufLen]; //数字格式化，显示百分比 DecimalFormat df = new DecimalFormat("##.00%"); while ((lenth = in.read(b)) != -1) &#123; out.write(b, 0, lenth); temp += lenth; double d = temp / size; System.out.println("线程" + j + "复制进度" + df.format(d)); if (size - temp &lt; bufLen) &#123; bufLen = (int) (size - temp); if (bufLen == 0) &#123; break; &#125; b = new byte[bufLen]; &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; in.close(); out.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println("运行到这里"); countDownLatch.countDown(); &#125; &#125;//线程调用方法private static Executor executor = Executors.newCachedThreadPool(); private static void MutiCopy(String src,String dest,int count) &#123; File file = new File(src); long len= file.length(); int oneNum = (int) (len/count); System.out.println(oneNum); int num = 0; // CountDownLatch latch = null; if((num= (int) (len%count))!=0)&#123; System.out.println("剩余："+(len%count)); latch = new CountDownLatch(count+1); executor.execute(new FileCopy2(src,dest,(int)(len-len%count),(int)len,num,latch)); &#125;else&#123; System.out.println("整除"); latch = new CountDownLatch(count); &#125; for(int i =0;i &lt;count;i++)&#123; executor.execute(new FileCopy2(src,dest,oneNum*i,oneNum*(i+1),i,latch)); System.out.println("线程"+i+"启动"); &#125; try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; sleep()、join()、CountDownLatch类之间的区别 Sleep方法是个静态方法，由thread类来调用。它只能暂停当前线程，不能暂停其他线程。它接受的参数指名这个线程需要休眠多少时间。 Join方法是非静态方法，他使得在系统调用此方法时只有这个线程执行完后 ，才能执行其他线程，包括主线程的终止；或者给它制定时间，即最多过了这么多时间后，如果还是没有执行完，下面的线程可以继续执行而不必再理会当前线程是否执行完。 CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完后再执行。调用join方法需要等待thread执行完毕才能继续向下执行,而CountDownLatch只需要检查计数器的值为零就可以继续向下执行 Thread和Runnable之间的关系 实现Runnable更加适合资源共享 实现Runnable接口避免单点继承的局限，一个类可以继承多个接口 备注： Runnable始终使用一个对象的计数器，Thread则每次创建一个计数器 Thread通过static属性同样可以实现资源共享 注意添加同步锁 线程池 更好调度和管理线程 种类 FixedThreadPool SingleThreadExecutor CachedThreadPool SingleThreadSchduledExecutor ScheduledThreadPool 使用线程池的好处是减少在创建和销毁线程上所花的时间以及系统资源的开销，解决系统资源不足的问题。如果不适用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者”过度切换的问题”。 单线程和多线程 进程只有一个，所以分配的CPU资源是一定的，多线程只不过是轮流抢占CPU而已，并不会真正提高处理速度。这意味着，多线程的作用主要在于提高了并发数量，比如http请求，如果是单线程，一次只能接收一个请求，多线程则可以同时接收多个请求 但是多线程由于轮换使用CPU，会造成单个线程的执行速度变慢（以前CPU供一个线程使用，现在要供多个线程轮流使用了）。但是在多CPU的服务器上，多线程就很有优势了，它不但能提高并发数量，而且能提高处理速度。因为在多CPU的服务器上，CPU调度很灵活，当一个线程占用着一个CPU的时候，其他线程可以被分配给其他CPU去处理，从而实现了“真正意义上地并行” 反射反射的概念 虚拟机在运行状态中，可以动态加载一个只有名称的类，加载完类之后，在堆内存中，就产生了一个 Class 类型的对象，这个对象就包含了完整的结构信息，通过这个对象我们可以看到类的结构。 反射的用处 动态加载类，在运行期间可以打开和检查.class文件，动态获取类、方法、属性等 广泛应用于各类框架，如spring Class类 在Java中每个类都有一个Class对象，每当我们编写并且编译一个新创建的类就会产生一个对应Class对象并且这个Class对象会被保存在同名.class文件里(编译后的字节码文件保存的就是Class对象) __内存中class对象只有一个，只能被JVM创建并加载* 类的生成过程 加载 连接 初始化 获取 class类的三种方式 通过类名获取 : 类名.class 通过对象获取 : 对象名.getClass() 获取字面常量的Class引用时，不会触发初始化 通过全类名获取 : Class.ForName(全类名) 类加载器 负责将.class文件加载到内存中，并为之生成对应的Class对象 组成 Bootstrap ClassLoader 根类加载器 负责Java核心类的加载，比如System类，在JDK中JRE的lib目录下rt.jar文件中的类 Extension ClassLoader 扩展类加载器 负责JRE的扩展目录中jar包的加载，在JDK中JRE的lib目录下ext目录 System ClassLoader 系统类加载器 负责在JVM启动时加载来自java命令的class文件，以及classpath环境变量所指定的jar包和类路径 123456public void testClassLoader() throws IOException &#123; InputStream in1 = null; in1 = this.getClass().getClassLoader().getResourceAsStream("reflect/1.txt"); System.out.println(in1.read()); &#125; 基本操作类 获取类的方法 获取类名以及其它信息 1234567891011private static void test1() throws ClassNotFoundException &#123; String path = &quot;team.redrock.reflect.Student&quot;; Class cls1 = Class.forName(path);// Class cls2 = Student.class; System.out.println(cls1.getName()); System.out.println(cls1.getSimpleName()); // Student student = new Student(); // Class cls3 = student.getClass(); // System.out.println(cls3.getName()); &#125; 属性 无参获取公有属性 无参获取私有属性 获取公有属性值 获取私有属性值 12345678910111213141516171819202122232425private static void test2(Class cls)&#123; //获取公有属性 Field[] fields = cls.getFields(); for(Field f : fields) &#123;// System.out.println(f.getName()); &#125; //获取全部属性 Field[] fieldss = cls.getDeclaredFields(); for(Field f : fieldss)&#123; //如果字段是私有的，不管是读值还是写值，都必须先调用setAccessible（true）方法 f.setAccessible(true); try &#123; System.out.println(f.get(cls.getDeclaredConstructor().newInstance())); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; &#125; &#125; new和newInstance的区别 new=初始化+实例化 newInstance=实例化 必须根据实例获取属性和方法 方法 获取公有方法 获取私有方法 获取指定方法 调用方法 调用父类的私有方法 1234567891011121314151617181920212223242526private static void test3(Class cls)&#123;// Method[] methods = cls.getMethods(); //不能获取私有方法// for(Method method:methods)&#123;// System.out.println(method.getName());// &#125;// Method[] methods1 = cls.getDeclaredMethods(); //不能获取父类方法// for(Method method:methods1)&#123;// System.out.println(method.getName());// &#125; Method method = null; try &#123;// method = cls.getMethod("studySecretly",null); //和无参获取method一样 method = cls.getDeclaredMethod("studySecretly"); Object instance = cls.getDeclaredConstructor().newInstance(); method.setAccessible(true); //调用位置注意一下 method.invoke(instance); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; 注解 利用反射实现注解功能 123456789101112131415161718192021private static void test4(Class cls)&#123; try &#123; Object object = cls.getDeclaredConstructor().newInstance(); Annotation annotation = (Annotation) cls.getAnnotation(GoodBoy.class); if(annotation !=null)&#123; if(annotation instanceof GoodBoy)&#123; //该注解是否是定义注解类的一个实例 GoodBoy goodBoy = (GoodBoy) annotation; System.out.println(goodBoy.value());//实现注解逻辑的地方 &#125; &#125; &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125;&#125; 如果在程序中要获取注解，然后获取注解的值进而判断我们赋值是否合法，那么类对象的创建和方法的创建必须是通过反射而来的 构造器 利用构造器创建对象 12345678910111213141516private static void test5(Class cls)&#123; Constructor&lt;Student&gt;[] constructors = cls.getConstructors(); for (Constructor&lt;Student&gt; constructor1 :constructors)&#123; System.out.println(constructor1); try &#123; Student student = constructor1.newInstance(); System.out.println(student.play("dota")); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; &#125; &#125;]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[踩坑记录-SSL验证报错]]></title>
    <url>%2F2019%2F10%2F11%2FSSLSL-Problem%2F</url>
    <content type="text"><![CDATA[今天不知道怎么回事诸事不顺，居然连创建一个新项目都会报错。。今天一如既往打开IDEA，满怀欣喜地创建一个springboot地新项目，想着怎么去慢慢填充羽翼，给项目注入新的生命。。突然，一排红线显示在我面前—— 1sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target son of bitch ! SSL验证失败的一种解决方案 目前解决方法 mvn编译报错mavn sun.security.validator.ValidatorException]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的九月总结]]></title>
    <url>%2F2019%2F10%2F07%2Fseptember-plan%2F</url>
    <content type="text"><![CDATA[天凉好个秋~ 应该脚踏实地，应该认清现实，应该一往无前。 var ap = new APlayer({ element: document.getElementById("aplayer-IxgGolVt"), narrow: false, autoplay: false, showlrc: false, music: { title: "Move Mountains", author: "Alisa", url: "http://music.163.com/song/media/outer/url?id=1381578814.mp3", pic: "http://p1.music.126.net/wR8cPX0w4gj179SCzUHI6A==/109951164258264118.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 短暂的九月，曾经寄托了我多少的幻想，作为大三的开局之月，也是立下誓言后的实践时期，想着九月多长，自己的改变就会有多大，但是最后时间匆匆，恍然间带走了一叶知秋的梦想，金黄的季节却没生产出期待的果实。 这个月，是检验自己计划正确性和合理性的一个月，也是在阵痛中艰难转型的一个月，这个月走过，离自己目标的12月，只剩下残缺的三个月时间，时间不饶人，就算自己还没有完全转变过来，也只能匆匆上马 收获这个月，总算有了自己的计划 对自己的就业目标，总算有了相对清楚的认识，尽管计划多变，但我相信这些改变都在一步步接近我自己的真实水平，都在让我的目标变得更加清晰明确 还是计划做一个web后端工程师，虽然感觉编程不是很适合我，但是就目前而言，开发岗位依然是最为理想的目标 自己的技能在于JAVA后端开发和网络技术，但是基础没有十分夯实，很多知识点也还没完全理解，因此今年是打基础，提升专业素养的一年。 但是计划完成得不是很理想。 积累这个月，勉强完成了《高性能MYSQL》重要部分的阅读，有收获，在索引的运用和表的优化上有了一点初步的认识，但是更多的是对高级特性的不解，对具体应用场景的不确定。很多知识点还是不能很好地运用到后端开发中去 这个月 开始学习python，本想系统学习，但是因为自己计划的变更最终不了了之。目前学习到了基础知识，大致地过了一遍，对这个语言的基础有了初步的理解，接下来把这些知识点归纳，总结成文，并在之后（大概是11月把）在系统地学习下去 这个月 有在紧跟日常课程。作为通信工程的学生，平时的课程相当繁重，上个学期差点挂科的我不想再经历一次最后阶段疯狂复习的痛苦。加上这个学期的课程确实十分重要，对后期找工作都意义非常，因此，我依然得在平时把学习跟上，尽量不要拉开太大距离。但令人沮丧的是，感觉自己平时花在上面的时间太多了，导致自己的就业计划进展缓慢，然而，就算是自己花时间的努力完善笔记，最后依然和上课的学习进度拉开了差距，感觉是我过于重视笔记的重要性而忽视了实践应用的缘故 日常这个月 主要是在忙ICT招新的事情 这个月 主要是在学习上课 这个月 还花了一点时间在班学长上 不足ICT方面学习积累不足，没有看完NA 没有回顾计网 一直在想但没有进行的Leetcode算法学习 没有看完之前做过的项目 自己的java基础没有很大的提升 关键在于，自己学习的热情和兴趣没有想像中高涨，专注度依然没有提升，还是会想着做别的事情 下个月计划 预期达成的目标 借小程序，对项目的经验更加丰富 JAVA基础有了提升 专注度和认真程度有提升]]></content>
      <categories>
        <category>一如既往，只是日常</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>计划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各层常见协议首部格式解析]]></title>
    <url>%2F2019%2F09%2F24%2Fhcna%2F</url>
    <content type="text"><![CDATA[大概一篇能讲完吧… 以太网帧运行在以太网上的常见帧格式分为两种，分别是Ethenet_II帧格式、IEEE802.3帧格式（SNAP） ethenet II的type字段指定了上层的协议 IEEE802.3相同位置是长度，同时对数据进行更加细粒度的控制 ETHENET_II字段说明 DMAC是目的MAC地址。DMAC字段长度为6个字节，标识帧的接收者。 SMAC是源MAC地址。SMAC字段长度为6个字节，标识帧的发送者。 类型字段用于标识数据字段中包含的高层协议，该字段长度为2个字节。类型字段取值为0x0800的帧代表IP协议帧；类型字段取值为0x0806的帧代表ARP协议帧。 数据字段是网络层数据，最小长度必须为46字节以保证帧长至少为64字节，数据字段的最大长度为1500字节。 循环冗余校验字段（``FCS`）提供了一种错误检测机制。该字段长度为4个字节。 IEEE802.3 字段说明 Length 代替ETHENET_II字段的Type字段，记录帧的字长 LLC占用Data的3个字节 D.SAP 目的访问服务点 S.SAP 源服务访问点 Control 控制字段 SNAP 占用Data的5个字节 Org Code机构代码，一般为0 Type字段和Ethenet相同 DSAP和SSAP作用解释 当DSAP和SSAP都取特定值``0xff时，802.3帧就变成了Netware ETHERNET`帧，用来承载NetWare类型的数据。 当DSAP和SSAP都取特定值``0xaa时，802.3帧就变成了ETHERNET_SNAP帧。ETHERNET_SNAP`帧可以用于传输多种协议 DSAP和SSAP其他的取值均为纯``IEEE802.3`帧。 MAC地址（6字节48bit） 前24bit是OUI 供应商代码，后24bit是序列号 单播第1字节的第8bit为0；组播第1字节的第8bit为1；广播为全1 IPv4报文结构 字段解释 Version 4字节，版本号 Header Lenngth 4字节，显示首部长度，一般首部长度为20字节 DS Field 8字节，区分服务,也叫TOS Total Length 16字节，报文总长度,理论上可到达65535字节 identification 16字节，标识，和分片有关 Flags 3字节 标志，和分片有关 Fragment Offset 13字节，片偏移，和分片有关 Time To Live 8字节 TTL Protocol 8字节 协议，指定传输层协议 Header Checksum 16字节，首部校验和，只对首部进行差错检测 Source IP Address 32字节，源IP地址 Destination IP Address 32字节 目的IP地址 IP OPtion 可选项 data有效载荷 关于区分服务IPv4的DS Field字段提供Qos。 原先定义为TOS（RFC791），TOS中的IP Precedence字段只能将报文分成8类，在实际生活中完全不够用，如下 字段说明 IP Precedence 代表了报文的优先级 D/T/R D bit代表延迟（Delay），T bit代表吞吐量（Throughput），R bit代表可靠性（Reliability）。 后来，在RFC2474中对IPv4报文头的ToS字段进行了重新定义，称为DS（Differentiated Services）字段。 字段说明 DSCP DSCP有两种表达方式，一种是数值，范围是0~63（6个字节）；一种是关键字，分为CS、EF、AF、BE 其中xy表示不同类别，数字越小丢包概率越小 CS6和CS7默认用于协议报文，而且是大多数厂商设备的硬件队列里最高优先级的报文，因为如果这些报文无法接收的话会引起协议中断。 EF常用于承载语音的流量，因为语音要求低延迟，低抖动，低丢包率，是仅次于协议报文的最重要的报文。 AF4用来承载语音的信令流量,指电话的控制信令。 AF3可以用来承载IPTV的直播流量，直播的实时性很强，需要连续性和大吞吐量的保证。 AF2可以用来承载VOD（Video on Demand：视频点播）的流量，相对于直播流量来说，VOD对实时性要求没那么强烈，允许有时延或者缓冲。 AF1可以用来承载普通上网业务。 BE 是指传统IP的分组投递服务，只关注可达性，对其他方面不做要求，所有路由器必须支持BE 关于分片IP数据报分片主要考虑以下原因： 数据链路层有不同的承载协议，不同协议承载的最大数据长度各不相同 使用IP分片将数据分片传输，从而使数据不会超过MTU，对此，IPv4提供了三个字段：标识、标志、片偏移 Identification：标识号,归属于同一个标识号的数据包分片具有相同的==标识号==、==源IP地址==、==目的地址== ，同时，发送主机通常为他发送的每一个数据报的标识号加1。 flag：用于判断后面是否还有片，flag分为Reserve,DF和MF三部分。Reserve是保留位；DF表示是否分片（DF=1表示不分片）；MF表示是否还有后续分片。可以看出，三位同时最多只能由一个值为1 Fragment Offset:片偏移，用来分片重组时进行定位，偏移以8字节为单位 分片的缺点 分片本身的划分和重组会带来资源消耗，尤其是接收方，在接受分片时要为每个分片提供内存空间，分片过多会导致资源消耗严重 由于IP层没有重传的策略，所以当数据波中的任意分片丢失时，整个数据报都需要依赖传输层进行重传，又因为IP层的分片相对于传输层是透明的，所以往往传输层会重传整个数据包，这样会浪费资源，影响网络中的传输质量。 同时，分片的存在会对网络安全造成隐患。利用最后一个分片没有被接受，所有分片就不会重组并释放内存空间的特点，故意不向接收方发送最后一个分片，如果这样的攻击迅速，大量，就会导致接收方的内存空间占满，无法处理正常业务，达到DOS效果。同时也可以使分片的偏移量出现差错，导致接收方无法正常重组数据报 值得注意的时IP分组只有第一个分片带有四层信息，因此其余分片有可能因此逃过防火墙等的检测，从而给黑客可乘之机 在实际应用中，应尽量避免使用IP分片。事实上，IPv6协议中已经取消分片的策略。 由于TCP协议中存在MSS，限制了最大数据发送量，从而可以确保IP数据报大小不会超过数据链路层的MTU，因此，在TCP协议中不存在IP分片；但是在UDP协议下，不存在这样的保障机制，因此最好要在应用层就限定每个包的大小 关于校验IP层只对报文头部进行校验检错，不对数据进行差错检测，网络层实质上还是属于“尽最大努力交付” 为什么链路层、IP层和传输层都需要校验？ 答：链路层的差错检验是为了保证数据在链路传输的完整性， IP层只对头部进行校验，一来是因为链路层的校验已经尽可能地降低了差错率，二来是因为IP层范围庞杂，如果每经过一个路由器都对数据进行校验，会对性能产生影响，可以但没必须。同时校验头部是为了保障IP协议自身的正确性，同时也存在对二层的不信任，以及四层协议有校验也有无校验，做一个最低限度的保障。甚至还有说法是历史遗留问题。总之算是一个complicated issue了🤣 传输层协议例如TCP协议对整个报文段进行校验，也是出于对下层的不信任的原因。（大概吧） 关于选项选项允许IP首部被扩展，选项字段用来支持排错、测量以及安全等措施，一般较少使用 IPv6报文结构传输层报文结构UDPTCP]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>ICT</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习（02）]]></title>
    <url>%2F2019%2F09%2F23%2Fpython2%2F</url>
    <content type="text"><![CDATA[… 函数传递参数 必备参数 关键字参数 默认参数 不定长参数 必备参数（位置参数） 默认参数 默认参数必须指向不变对象！ 123456def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 123456#L[]是可变对象，每次调用会改变值，因此使用None加锁def add_end(L=None): if L is None: L = [] L.append('END') return L 不定长参数（可变参数） 12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 关键字参数 可以传入数量不定的dict参数 12345678def person(name, age, **kw): if 'city' in kw: # 有city参数 pass if 'job' in kw: # 有job参数 pass print('name:', name, 'age:', age, 'other:', kw) 指定关键字参数，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。 12def person(name, age, *, city, job): print(name, age, city, job) 如果已有可变参数，后面的参数就自动被设为指定关键字参数 12def person(name, age, *args, city, job): print(name, age, args, city, job) 高级特性切片使用形如[:]的语句来执行索引范围的操作 支持倒数切片 迭代使用Iterable 列表生成器 可以使用多重循环 生成器迭代器]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现牛顿法迭代法]]></title>
    <url>%2F2019%2F09%2F22%2Fpynewton%2F</url>
    <content type="text"><![CDATA[最近看到牛顿迭代法和二分法，现在用python实现一下 拿开方举例,转载自Python编程实现二分法和牛顿迭代法求平方根代码 一、使用二分法实现开方假设求根号5，二分法的基本思路是 123456a:折半： 5/2=2.5b:平方校验: 2.5*2.5=6.25&gt;5，并且得到当前上限2.5c:再次向下折半:2.5/2=1.25d:平方校验：1.25*1.25=1.5625&lt;5,得到当前下限1.25e:再次折半:2.5-(2.5-1.25)/2=1.875f:平方校验：1.875*1.875=3.515625&lt;5,得到当前下限1.875 每次得到当前值和5进行比较，并且记下下下限和上限，依次迭代，逐渐逼近平方根： 当结果校验超过原值，继续迭代 当结构校验低于原值，获得另一半，继续迭代 python代码 12345678910111213141516171819import mathfrom math import sqrtdef sqrt_binary(num): x = sqrt(num) y = num/2.0 low = 0.0 up = num*1.0 count = 1 while abs(y-x)&gt;0.000001: print(count,y) count += 1 if y*y&gt;num : up = y y = low+(y-low)/2 else: low = y y = up-(up-y)/2 return y 二、使用牛顿迭代法实现开方从函数意义上理解：我们是要求函数f(x)=x²，使f(x)=num的近似解，即x²-num=0的近似解。 从几何意义上理解：我们是要求抛物线g(x)=x²-num与x轴交点（g(x)=0）最接近的点。 我们假设g(x0)=0，即x0是正解，那么我们要做的就是让近似解x不断逼近x0，这是函数导数的定义： 可以由此得到 从几何图形上看，因为导数是切线，通过不断迭代，导数与x轴的交点会不断逼近x0。 对于一般情况： 将m=2代入： 123456789101112def sqrt_newton(num): x=sqrt(num) y=num/2.0 count=1 while abs(y-x)&gt;0.00000001: print count,y count+=1 y=((y*1.0)+(1.0*num)/y)/2.0000 return y print(sqrt_newton(5)) print(sqrt(5)) 三、利用牛顿迭代法实现立方123456789101112def cube_newton(num): x=num/3.0 y=0 count=1 while abs(x-y)&gt;0.00000001: print count,x count+=1 y=x x=(2.0/3.0)*x+(num*1.0)/(x*x*3.0) return x print(cube_newton(27))]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习(01)]]></title>
    <url>%2F2019%2F09%2F22%2Fpython1%2F</url>
    <content type="text"><![CDATA[开始系统学习python 本节主要回顾python的基本语法 基本语法标识符标识符由字母数字下划线组成，开头不能是数字，区分大小写 单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import 而导入。 以双下划线开头的 __foo代表类的私有成员 以双下划线开头和结尾的 __foo__代表 Python 里特殊方法专用的标识，如 init() 代表类的构造函数。 书写语法 多行语句 多行注释 Python 可以同一行显示多条语句，方法是用分号 ; 分开 print 默认输出是换行的，如果要实现不换行需要在变量末尾加上逗号 ,。 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释； 第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 Number类型1234567891011121314int(x [,base ]) 将x转换为一个整数 long(x [,base ]) 将x转换为一个长整数 float(x ) 将x转换到一个浮点数 complex(real [,imag ]) 创建一个复数 str(x ) 将对象 x 转换为字符串 repr(x ) 将对象 x 转换为表达式字符串 eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s ) 将序列 s 转换为一个元组 list(s ) 将序列 s 转换为一个列表 chr(x ) 将一个整数转换为一个字符 unichr(x ) 将一个整数转换为Unicode字符 ord(x ) 将一个字符转换为它的整数值 hex(x ) 将一个整数转换为一个十六进制字符串 oct(x ) 将一个整数转换为一个八进制字符串 Python math 模块提供了许多对浮点数的数学运算函数。 Python cmath 模块包含了一些用于复数运算的函数。 字符串和编码ASCII编码是1个字节，而Unicode编码通常是2个字节。 把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符： Python的字符串类型是str，在内存中以Unicode表示，传输时需要转换成bytes对bytes类型的数据用带b前缀的单引号或双引号表示 字符串开头的含义 b r 非转义原生字符 u unicode编码字符，python3默认字符串编码方式。 使用encode和decode进行编解码 List和tuplelist 有序集合 tuple 初始化后无法修改的有序列表 在定义的时候，tuple的元素就必须被确定下来 只有1个元素的tuple定义时必须加一个逗号,，来消除歧义 dict和setdict 又称map，使用键-值（key-value）存储，具有极快的查找速度 判断key不存在 通过in判断key是否存在 通过dict提供的get()方法，如果key不存在，可以返回None，或者自己指定的value dict是用空间来换取时间的一种方法。 hasn算法决定key不可变 set 不重复的key的集合 注意不可变对象，例如str]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UI设计尺寸问题——移动端]]></title>
    <url>%2F2019%2F09%2F18%2Fui%2F</url>
    <content type="text"><![CDATA[最近正在做前端的UI，本来以为会是一件很简单的事情，但一来二去也搞了整整一天。其中最让我郁闷的是在了PS设置画布尺寸的时候没有做充分的调查，理解错了同学口中的尺寸意义，导致UI画完了才发现尺寸整整小了一圈😂怎么办？重画呗~ 这次我吸取教训，整理了相关资料，这次一定要一举击破像素尺寸问题！ 物理尺寸英寸英寸(inch)是衡量设备尺寸的常用单位 它代表的是屏幕对角线的长度 一英寸≈2.54cm 分辨率像素像素是屏幕的最小单位，所有图片都是由无数个像素点构成。 像素应该是用来衡量尺寸的，像素点和尺寸成正比 屏幕分辨率 分辨率即垂直和水平方向上的像素点数,例如1080*1920 指具体设备的屏幕像素尺寸，一般我们所说的手机或者电脑等物理设备的分辨率的时候就是指屏幕分辨率 屏幕分辨率越高不代表屏幕越清晰，这还要取决设备的物理尺寸。换句话说，分辨率要和物理尺寸搭配才能体现清晰度，因为它直接能表现的是屏幕所能容纳的总的像素点数，如果设备物理尺寸不同，比较分辨率应该意义不大。当然这里的像素应该指的是物理像素，这点之后会提到。 这里要记住的是屏幕分辨率/设备独立像素(逻辑像素) = 倍率（设备像素比）这个结果才能真正定义清晰度 图像分辨率即图片所含的像素点数，表示一张图片在垂直和水平方向上的像素点数 和屏幕分辨率的定义差不多，只不过一个是定义屏幕的一个是定义图片的。同一尺寸下分辩率越高，图片越清晰 PPI（Pixel per Inch） 每英尺包含的像素点数，可以用来描述屏幕的清晰程度 PPI越高，图片质量越清晰。 这里注意在设计的时候一般约定 手机屏幕使用72PPI，海报使用300PPI 据说PPI达到300以上，就会给人真实感，因此海报的PPI要达到这个数值，而手机屏幕如果PPI设置过大，会导致加载过慢，卡顿的现象，一般72PPI也够用了 PPI的计算公式 DPI（Dot per Inch） 每英寸包括的点数。它可以是屏幕像素点、图片像素点也可以是打印机的墨点。 在描述图片和屏幕的时候可以认为DPI和PPI两者等价 在使用打印机的时候，由于打印机打印的点不是规则排列的，打印点之间会存在空隙，因此DPI也被称之为打印点的密度 打印机的DPI越高，打印图像的精细程度越高，同时墨点的耗费量和打印时间也越长 逻辑像素和物理像素以上我们看到的像素都是真实的像素，是物理像素 但是随着技术的发展出现了逻辑像素，即由多个物理像素组成一个逻辑像素。 这是由乔布斯在Iphone4的发布会上发布的视网膜屏幕（Retina Display）首次提出的 一个很直观的对比就是，由逻辑像素（假设一个逻辑像素由3个物理像素组成）组成的分辨率为10801920的图片，和一个由物理像素组成的分辨率为`10801920`的图片，在像素总量一定的前提下，前者的图片尺寸是后者的3倍，试想想如果把这个3倍的尺寸图片压缩到1倍的图片中，是不是清晰度骤然就提升了呢？ 设备独立像素设备独立像素，指设备的原始尺寸，这里的像素是逻辑像素 打开chrome的开发者工具，我们可以模拟各个手机型号的显示情况，每种型号上面会显示一个尺寸，比如iPhone X显示的尺寸是375x812，实际iPhone X的分辨率会比这高很多，这里显示的就是设备独立像素。 设备像素比DPRdevice pixel ratio简称DIP或DP,指设备的物理像素和逻辑像素之比，也称之为倍数 开发问题移动端iOS的尺寸单位为pt，Android的尺寸单位为dp，React Native中没有指定明确的单位，它们其实都是设备独立像素dp(Device Pixel)。 WEB端CSS像素使用的是设备独立像素(逻辑像素) UI准备图的时候根据屏幕的质量（2倍屏或3倍屏）准备2倍和3倍的像素尺寸进行绘图 在写CSS时，我们用到最多的单位是px，即CSS像素，当页面缩放比例为100%时，一个CSS像素等于一个设备独立像素。 但是CSS像素是很容易被改变的，当用户对浏览器进行了放大，CSS像素会被放大，这时一个CSS像素会跨越更多的物理像素。 页面的缩放系数 = CSS像素 / 设备独立像素 屏幕问题我们常使用P和K衡量屏幕质量 P代表的屏幕纵向的像素个数 1080P即代表屏幕纵向有1080个像素 K代表的屏幕横向的像素个数 K代表屏幕横向有几个1024像素，像像素超过4096一般就称之为4K屏 参考文章：关于移动端适配，你必须要知道的]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当我们面对命运的时候，我们在想些什么？]]></title>
    <url>%2F2019%2F09%2F18%2Fdestiney%2F</url>
    <content type="text"><![CDATA[“北海，我只能告诉你，在那之前要多想” 当我点开它的一瞬间，我在想什么？无奈？恐惧？有对过去的忏悔？有在默默祈祷？不管怎样，我至少能确定一点。 那一刻，我被命运主宰了。 我相信命运吗？在我保持理智的时候，我是会否认的，但每当在那些关键的节点的时候，我不得不承认，我无能为力，如果说我无法控制我未来走向，那究竟是谁在控制它？大概就是命运吧。 什么是命运？ ~很显然，我们暂时无法得知命运地全貌，但是我们能粗瞥它的真容。命运不会偏袒任何一方，它所做的所有都是合理的，尽管有时候会由于我们认知的不足而出现一些看似偶然，但实则必然的事情。它是绝对公正的。但是我们需要绝对公正吗?我不知道，这个问题用我现在的思维能力无法解答。但是很有趣，希望有一天我能知道 我知道这次我又是无能为力，面对即将出现的成绩我无可奈何，我任由命运给我答案。但是也正是这个节点，我开始疯狂回忆自己的所作所为，开始假想各种各样的结局，明明知道自己的结局已经是确定的，但是潜意识中，仍希望做着看似无谓的挣扎。这就是人性吧。 我把内心的挣扎归为是自由意志的体现，这也是为什么人类能不断前进的原因。因为人类就是这样一种为了否定命运而存在的动物。因为对自由的渴望，人类不断反抗着，希望掌握自己的命运，因此他们成功了，他们自由了，所以人成了万物的灵长。永不停止抗争和失去对自由的追求，这是我们的天性。 命运由我不由天命运由我不由天。尽管有的时候我们无法主宰命运，但是不能让命运决定我们的未来。道路是曲折的，但是前途是光明的，在我们的自由意志下，目标的实现是必然的。 只是，现在的我，开始开始承认命运的主宰地位了。当面对命运的时候，巨大的无力感让我无可奈何。束手无策的我失去了自己的自由。尽管思考再多，我也打不倒命运。 “北海，我只能告诉你，在那之前要多想” 是啊，如果人都只是在大难临头的时候才思考，那他离毁灭也不远了。比这更可怕的是被击倒后就习惯了下跪，再痛的伤愈合后也被忘得一干二净。想，要多想，要无时不刻地想。既然记忆总会趋向逝去，那就就不断地刷新强化，用毅力去坚持！这才是我们的自由意志！]]></content>
      <categories>
        <category>行路匆匆，随笔悠悠</category>
      </categories>
      <tags>
        <tag>感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迷失的人迷失了，相逢的人还会再相逢]]></title>
    <url>%2F2019%2F09%2F18%2Fspirit-away%2F</url>
    <content type="text"><![CDATA[var ap = new APlayer({ element: document.getElementById("aplayer-wZwItmdp"), narrow: false, autoplay: false, showlrc: false, music: { title: "いつも何度でも", author: "伊藤サチコ", url: "http://music.163.com/song/media/outer/url?id=584155.mp3", pic: "http://p1.music.126.net/GYWkXtrnAawOWO2nfLg3PA==/109951163028865726.jpg?param=130y130", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 前言我没有修改文字的习惯。我以为但凡是写下的都是内心的真实想法，都是转瞬即逝的珍贵记忆，尽管有时候写出来的东西混乱不堪，但是我依然能从这些文字总体会到当时我的思绪，所以说文字是一把打开记忆之门的钥匙，同时也是我不断反省的工具。因此所有的文字我都原样保留，特此说明，引以为警。 本以为又将是一次痛苦的折磨，但是室友们的敞开心扉，让我真的发现了一丝的光亮。我自诩孤独，只是我一直在抗拒，或者说一直在逃避。但是我真的不是一个人，他们就是我的朋友。我无需在今晚改变什么，但是我只确定一点，以后必须坚定不移地坚持这一点认识： 我不是一个人，我有真正的朋友，对朋友请敞开心扉。 2019.7.2 无面男 他像我但不是我。我们都是孤独中找不到自我的人，我们都是被欲望填充就失去自我的人。他终究是单纯的，最后也能找到自己的归宿。而我一直都是残缺的，对世界的联系是脆弱的，故事总是美好的，即使有无奈的结局但至少还能庆幸是虚假的，我是真实存在的，所有关于我的一切都是真实地残酷，这是一条无法回头的路，没有一丝的怜悯，没有一丝的侥幸，走着走着只能像是一滴浊泪坠入汪洋，分解地悄无声息又消失地无可奈何，世界不会因此有任何改变，存在也不会有任何意义， 人为什么要白白走这一遭啊！ 2019.7.3 我犯了一个很大的错。我直到现在都没有真正地去接纳这个世界。我是孤独的，因为我总是把眼光局限在了自己所能触及的范围内，被自己的臆想所束缚，所想即所得，犯了唯心主义的大忌。其次，我一直都是知道路的，但是我一直在路旁伫立。在静止的时间你确实可以好好反省，但是没有了流动，你就无法打破自己设下的环，想得太多，所得的结果却总是原地打转，只有负能量在不断放大，最终毒害了自己。 所以，不要停下来啊！ 遇到自己解决不了的事情就大胆说出来，所有的烦恼和困惑请及时抛弃，只有大胆地迈出这一步，你才能走出怪圈。造物主是缺乏想象力的，人们外貌特征总是会和别人有部分地相似，所有经历都像是别人经历的东拼四凑，因此没有人是特殊的，所有人都是平凡的。但平凡的人们却是富有想象力的，人们对于同样的事情，会有着不同的看法，会有着千奇百怪的解决办法，正因为大家都有一段经历的相似性，因此所有的解决方法才有了借鉴的可能，人因此能总结前人留下 的经验，为自己的经历增添新的思路看法。我们的目标从来不是解决完世界的所有问题，我们也不期望问题都有一个最佳的答案，因为我们在思考问题的过程中所迸发的无穷的智慧才是我们想要传承和发扬的宝藏。 因此人生是值得的。 再一次看看《千与千寻》，虽说是想找寻一下内心的平静，但是其实也有很大程度是因为自诩电影爱好者总想给自己赋予一些专业度。希望能通过再次回顾电影去探索电影背后的寓意，来满足自己的虚荣心。事实证明，尽管第二次重温电影，我依然没有能抓住电影的深层含义，开场第一幕音乐一响起，我就像浸泡在了浴缸里一样，舒服得不想再去动脑子，只想好好沉浸在电影带来的观感中。再加上后半段的剧情自己也已记忆模糊，所以只晓得跟着剧情走了，别的再不想（果然还是不懂脑子比较好~），总的来说，还是值得的 。 尽管如此，第二次观影还是给我带来了一些不一样的感触。比如第一次让我开始有所反省的就是无脸男的登场。无脸男喜欢小千，却不懂什么是爱 ，我反正觉得这就是我的影子。我渴望成功和荣誉（或者爱情也不是不可以–）但是却找不到方法，或者说很迷茫。无脸男为了讨小千欢心，不断地思考什么是人所想要的，最后他发现了金子。金子珍贵又充满的魔力，单纯的他很快就迷失在了其中，金子满足了他的欲望，欲望不断地膨胀，最后可以扭曲一个人的性格。被欲望填充的无脸男变得丑陋，油腻，狂妄自大，深陷其中不能自拔。进入大学的我也是一样的迷茫，四处碰壁，或沉迷于欲望中，大学的自由让我不得不面对周围许许多多的诱惑，然而我没有成功地克制住自己，任由欲望不断膨胀，我开始失去目标，开始变得无奈和颓废，明知道自己的祸根在哪，可就是无法舍弃。我再也回不到原来的状态了，那个永不放弃，单纯，勇敢的少年，正提前变得油腻，失去斗志。怎么办？电影中解决方法很有趣，无脸男吃多了欲望，多运动就好。随着运动不断排泄出无用的欲望，直到自己再一次孑然一身，身体也重新变得空灵起来。所以简单就是一种态度。我那两个在衡中的室友想念原来的生活方式也不是没有道理，现在的我几乎也要被他们说服了。 大学的我们有太多的选择，可很多时候我们又不会选择 这样的话，不如我们就从头来过，回到高中简简单单的模式，给自己减负。这就是舍得。 全片最让我感动的就是千寻和无脸男坐上有轨列车，列车在水上缓缓驶过，车外风景变换无常，车上旅客行路匆匆，我和你肩并肩坐着，不说话，就很美好。 记住自己的名字，忘记了就找不回自我。简单就是一句话，不忘初心，方得始终。 电影播完，字幕出场，伴随着《いつも何度でも》的ED响起，又是泛起一阵不舍，千言万语化作一句话：“这就是人生啊”。 立下一个不算是Flag的Flag：以后还是会再见的，《千と千尋の神隠し》]]></content>
      <categories>
        <category>行路匆匆，随笔悠悠</category>
      </categories>
      <tags>
        <tag>影评</tag>
        <tag>感悟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库事务初识]]></title>
    <url>%2F2019%2F09%2F18%2Ftransaction%2F</url>
    <content type="text"><![CDATA[前言： 本篇共2895字，阅读大约需要6分钟 本次将向大家科普一些数据库事务相关知识。为什么选择数据库作为本次介绍的主角？ 数据库是现代网络的重要组成部分。在信息爆炸的大数据时代，各行各业都因为大幅爆发的数据而正变得蒸蒸日上；海量，多样且在高速产生的数据正迸发着前所未有的价值。可以说，数据改变了我们的生活，也在创造我们的未来。因此，我们越来越需要更加灵活，功能强大的数据库来帮助我们存储数据，处理数据。随着时代的发展，掌握数据库操作相关技术已经是从事ICT行业的相关人员的必备选择。 基础概念在正式介绍事务之前，让我们先充充电，大致了解一些基本概念，这些在之后的学习将会用到。 什么是数据库 “数据库”是以一定方式储存在一起、能予多个用户共享、具有尽可能小的冗余度、与应用程序彼此独立的数据集合 什么是数据库管理系统 数据库管理系统（Data Base Management System，简称DBMS）是位于用户应用程序与操作系统之间的一层数据管理软件，是数据库系统的核心组成部分。为用户或应用程序提供访问数据库的方法：包括数据库的建立、查询、更新以及各种数据控制。 数据库管理系统不仅允许单个用户查询和修改数据，也可以支持多人操作。多人操作就会出现导致操作并发，出现拥塞。 什么是锁 锁是数据库服务器用来控制数据资源被并行使用的一种机制。被锁的对象只允许持有用户操作，只有等锁释放后，其他用户才能有拥有锁的机会。 大部分数据库使用下面两种锁策略： 写操作需要申请写锁，读操作申请读锁，一个表一次只能分配一个写锁，并且拒绝读请求直到写锁释放 写操作需要写锁，读操作不需要锁 其中常见的数据库如mysql可以根据不同的存储引擎选择不同的锁策略 锁的粒度 即锁的范围，在数据库中，服务器可以在3个不同级别应用锁 表锁 阻止多用户同时修改同一张表的数据 页锁 阻止多用户同时修改某表中的一页 行锁 阻止多用户同时修改某表中的某一行 什么是存储引擎 数据库存储引擎是数据库底层软件组织,数据库管理系统(DBMS)使用数据引擎进行创建、查询、更新和删除数据，不同的存储引擎提供不同的存储机制、锁定水平等功能。 锁的行为是由存储引擎决定的，存储引擎使用不当会引发死锁现象 例如mysql有三种存储引擎：InnoDB MyISAM MEMORY 什么是事务我们从一个场景开始说起。假设一个人在银行办理转账业务，现在他要将1万元转账到另一个人，按一般情况，转账的过程几乎是秒级完成，但是，当我们将过程拆解，转账步骤可分为3步：1.查询账户余额是否大于1万元，2.从原账户上减去1万元，3.在对方账户上增加1万元。我们可以发现，其实只要其中任意一个环节出现偏差，都有可能造成无法挽回的重大损失。因此，为了确保银行里金额不会损失，我们可以这么做：将这一万元暂时缓存下来，只有确认对方账户已完成扣除款项的操作，才将这一万元增加到对方账户；否则，所有操作都失效，原账户取消扣除款项，对方账户也不会无故增加数量，我们把这种操作称之为事务回滚，转账的操作就是一个事务。 事务的特性世界上总是充满各种各样的意外，很多都是我们无法左右的，例如服务器的损坏，突然的断电，系统的崩溃等等，如果没有事务的存在，数据库的操作可靠性将无法得到保证。但这些的前提是事务本身应当有完备的标准来确保事务本身的可靠性。因此，那些前辈们给事务指定了ACID四大特性，只有严格通过了ACID测试，事务才能发挥其作用。 原子性 一个事务必须被视为不可分割的最小工作单元，整个事务种的所有操作要么全部提交成功，要么全部失败回滚，对于一个 事务来说，不可能 执行其中一部分操作。 一致性 数据库总是从一个一致性的状态转移到另一个一致性的状态，事务不能破坏数据库的完整性以及业务逻辑的一致性。比如无论转账成功或失败，都不可能会多出或减少1万元，金额总数是不变的。&gt; 隔离性 一个事务在提交前的修改对其他事务通常是不可见的。一个事务不应影响其他事务的运行效果。 持久性 一旦事务提交，其修改是不可逆的。 事务的ACID特性保证了数据库操作的安全性和可靠性，但实际操作总没有想象中那么简单。同时，添加事务也需要数据库系统进行更多额外的工作，这对数据库系统的性能提出了一定的要求。 隔离级别实际上，要让事务保证完全隔离依然是一件十分困难的事情 ，完全的隔离要求数据库同一时间只能执行一个事务，这样会严重影响性能。现实中，往往是多个事务并发执行。 一旦隔离性无法得到保证，数据库的读写就会面临如下情况 脏读：事务读取了未提交的数据。例如事务A读取了事务B的更新的数据，但是事务B回滚了，导致A读取的为脏数据 不可重复读：事务A读取同一数据两次，但是在两次之间事务B对该数据进行了修改并提交，导致事务A读取两次读取不一致 幻读：事务A修改全表的数据，在未提交时，事务B向表中插入或删除数据，导致事务A读取的数据与需要修改的数据不一致 对此，sql标准中指定了四种隔离级别： 未提交读（READ UNCOMMITTED） 事务可以读取为提交的数据，不做隔离控制 提交读（READ COMMITTED） 不允许未提交读,一个事务开始前，只能“看见”已提交的事务修改，是大部分数据库的默认隔离级别（mysql除外） 可重复读（REPEATED READ） 保证同一事务中多次读取同一记录的结果是一致的，一般方法是事务中对符合条件的记录上排他锁，这样其他事务不能对该事务操作的数据进行修改，是mysql的默认隔离级别 可串行化(SERIALIZABLE) 是最高隔离级别，通过强制事务串行执行，在读取的每一行数据上加锁，导致其他事务不能对数据进行操作（包括增加、删除和修改），但是此级别也要注意大量锁的超时会极大地影响性能。 隔离级别 脏读 不可重复读 幻读 未提交读（READ UNCOMMITTED） 是 是 是 提交读（READ COMMITTED） 否 是 是 可重复读（REPEATED READ） 否 否 是 可串行化(SERIALIZABLE) 否 否 否 阻塞和死锁谈到并发就大致谈一下阻塞的概念 当多个事务对某一资源进行锁定时，其他没有分配到锁的事务势必等待锁的释放，这就造成了阻塞。 当阻塞时间达到永久，就形成了死锁。 在事务中，一旦两个及以上的事务在同一资源上相互占用，并请求锁定对方占用的资源，就会引发死锁现象。 解决思路 查询时间达到锁的超时时间后放弃请求 使用较低的隔离级别，让持有锁的时间减短，减少锁竞争 避免事务中用户交互，同时尽量顺序访问对象 ……. 不同的存储引擎对死锁实现了不同的死锁检测和死锁超时机制，因此大家在考虑死锁的解决方案的时候一定要结合数据库存储引擎的实现方案，并且做好事务日志，以便在问题发生时进行有效排查和高效解决。 事务的阻塞和死锁的出现实际上是多个进程并发的必然结果，大家如果想了解更多关于有关并发和锁，可以自行了解相关知识。 总结本篇笔者带大家初识数据库事务，初步了解了： 什么是数据库、什么是锁和事务; 事务的四个特性：原子性、隔离性、一致性、持久性; 数据库读写出现的情况：脏读、幻读和不可重复读; 事务的隔离级别; 死锁的出现和解决思路 因为篇幅有限，这里只能浅尝辄止。数据库是一门庞大复杂又极其重要的学科，如果大家有兴趣，鼓励大家自己凿渠引水，因笔者知识有限，文章内容所述难免存在谬误，恳请广大读者斧正。 知识链接数据库学习《SQL学习指南》，《高性能MYSQL》]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VRRP入门]]></title>
    <url>%2F2019%2F09%2F18%2Fvrrp%2F</url>
    <content type="text"><![CDATA[背景随着移动办公、网上购物、即时通讯、互联网金融、互联网教育等业务蓬勃发展，网络承载的业务越来越多，越来越重要。所以如何保证网络的不间断传输成为网络发展过程中急需解决的一个问题。 对此，其中一种解决方案是，使用一套使用一套冗余备份机制，避免传统组网引发的单点故障，以便在路由器出现故障的时候及时进行链路切换，保证业务平滑进行。 传统解决方案 配备多台路由器解决单点故障 使用动态路由协议解决多网关冲突，实现链路回切 但是传统方案依然存在几个问题 一些动态路由协议使用广播报文进行链路切换速度上比较缓慢 在许多情景下可能会不支持使用动态路由 于是，VRRP应运而生。 VRRP的基本概念VRRP 虚拟路由冗余协议(Virtual Router Redundancy Protocol，简称VRRP)是由IETF提出的解决局域网中配置静态网关出现单点失效现象的路由协议。 VRRP能够在不改变组网的情况下，将多台路由器虚拟成一个虚拟路由器，通过配置虚拟路由器的IP地址为默认网关，实现网关的备份 优势采用VRRP的链路保护机制比依赖动态路由协议的广播报文来进行链路切换的时间更短，同时弥补了不能使用动态路由情况下的链路保护。 名词解释 VRRP路由器：运行VRRP的设备，它可能属于一个或多个虚拟路由器。 虚拟路由器(备份组):同一个广播域的一组路由器组织成一个虚拟路由器，备份组中的所有路由器一起，共同提供一个虚拟IP地址，作为内部网络的网关地址。 虚拟IP地址：虚拟路由器的IP地址，一个虚拟路由器可以有一个或多个IP地址，由用户配置。 IP地拥有者:如果一个VRRP路由器将虚拟路由器的IP地址作为真实的接口地址，则该设备是IP地址拥有者。 虚拟MAC地址：是虚拟路由器根据虚拟路由器ID生成的MAC地址。 当虚拟路由器回应ARP请求时，使用虚拟MAC地址，而不是接口的真实MAC地址。 主IP地址：从接口的真实IP地址中选出来的一个主用IP地址，通常选择配置的第一个IP地址。VRRP广播报文使用主IP地址作为IP报文的源地址。 主（Master）路由器：在同一个备份组中的多个路由器中，只有一台处于活动状态，只有主路由器能转发以虚拟IP地址作为下一跳的报文。 备份（Backup）路由器：在同一个备份组中的多个路由器中，除主路由器外，其他路由器均为备份路由器，处于备份状态。 状态机 VRRP有三种状态：初始状态、活动状态、备份状态.其状态切换过程如下： 协议报文 vrrp只有一种报文：Advertisement报文；其目的IP地址是224.0.0.18（组播地址），目的MAC地址是01-00-5e-00-00-12，协议号是112 下面介绍vrrp的两种主要的工作模式：主备备份和负载分担 VRRP主备备份过程选举master选举规则：1. 比较优先级 2. 优先级相同的时候，存在master则保持master状态 3. 没有master则比较接口IP地址，大的当选master 备份组状态维持 master周期性发送vrrp通告报文给组内设备，从而通知自己处于正常状态 主备切换 条件：1. master主动放弃，发送优先级为0的通告报文 2. master故障，等待Master_Down_Interval定时器超时，选举产生master。这个 切换时间叫Skew_Time，计算方式（256－Backup设备的优先级）/256（单位秒） Master_Down_Interval定时器取值：3×Advertisement_Interval＋Skew_Time（单位秒）。 主备回切 如果原故障master路由器恢复，发现收到RouterB的VRRP报文中的优先级比自己低，RouterA立即抢占成为Master。 抢占模式： 默认高优先级backup路由器可以抢占低优先级master路由器位置，但是如果IP地址拥有者是可用的，则它总是处于抢占的状态，并成为Master设备 注意抢占延时，默认为0，但是为了防止在网络不稳定的情况下频繁切换导致流量中断，可以设置延时时间稍长 注意：如果VRRP的上行链路故障，由于主备通告正常，将无法引起主备切换 解决方法：利用VRRP的联动功能监视上行接口或链路故障，主动进行主备切换。 VRRP负载分担 负载分担是指多个VRRP备份组同时承担业务转发,从而解决流量分担不均，master负担过重的情况 负载分担方式需要建立多个VRRP备份组，各备份组的Master设备分担在不同设备上；单台设备可以加入多个备份组，在不同的备份组中扮演不同的角色。 VGMP简介VRRP虽然可以解决单点故障，实现主备备份，但是在配置防火墙的时候，由于防火墙功能的特殊性，在对其进行主备备份时，传统VRRP方式无法实现主、备用防火墙状态的一致性。无法实现VRRP状态一致性以及会话表状态的备份 此时，我们需要引入VGMP（VRRP Group Management Protocol） VGMP提出VRRP管理组的概念，将同一台防火墙上的多个VRRP备份组都加入到一个VRRP管理组，由管理组统一管理所有VRRP备份组。通过统一控制各VRRP备份组状态的切换，来保证管理组内的所有VRRP备份组状态都是一致的。 基本原理 当防火墙上的VGMP为Active/Standby状态时，组内所有VRRP备份组的状态统一为Active/Standby状态。 状态为Active的VGMP也会定期向对端发送HELLO报文，通知Standby端本身的运行状态（包括优先级、VRRP成员状态等） VGMP HELLO报文发送周期缺省为1秒。当Standby端三个HELLO报文周期没有收到对端发送的HELLO报 文时，会认为对端出现故障，从而将自己切换到Active状态。 VGMP管理机制状态一致性管理 VGMP管理组控制所有的VRRP备份组统一切换。 抢占管理 当原来出现故障的主设备故障恢复时，其优先级也会恢复，此时可以重新将自己的状态抢占为主。 用VGMP实现防火墙的备份以及故障自动迁移从而保证业务连续性的方法也属于防火墙的双机热备技术的一部分，因为篇幅有限，这里不再展开。 小结今天我们从现实一个问题作为切入点，简单介绍了利用VRRP做路由冗余备份，同时针对防火墙的特性，引申出VGMP的概念，也算是揭开了防火墙神秘面纱的一角。网络安全越来越成为我们生活中不可忽视的一部分，对于安全方面知识，下次有机会我们再继续讲述吧。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>ICT</tag>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入门搭建]]></title>
    <url>%2F2019%2F09%2F18%2Fhexofirst%2F</url>
    <content type="text"><![CDATA[前言花了大概三天的时间，总算大致把博客的雏形搞定了。虽然说很多东西最终都不是我自己完成的，只是在别人的基础上在表面进行敲敲打打（后悔自己学习了后端，囧Orz）但不管怎样，没有文章的博客终归只是没有灵魂的躯壳，尽管自己的水平有限，但是还是希望能通过书写自己的一点一滴来见证自己的成长吧！ 现在就开始我们今天的内容——hexo博客的搭建 初识Hexo先让我们了解一下hexo： Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 比起它的前辈wordpress，hexo最大的特点就是使用markdown编写文章，同时以node.js作为运行环境，你只需要在服务器上安装好node.js环境，就可以轻松安装hexo，这时你再回头看看wordpress的LAMP环境，是不是觉得繁琐又笨重呢？（hh其实hexo还是wordpress都是非常主流的博客框架，两者的优劣我们之后有时间或许可以再讨论） Hexo基本搭建知识前提 安装hexo前你最好了解的基本知识 markdown语法基础 node.js基本认识（其实用不到多少，但是最好能了解一点语法基础） html、css基础（为你个性化博客很有帮助） github相关知识（程序猿必备） 基础环境 本人用的Win10 搭建的基本流程 环境配置-&gt;本地调试-&gt;服务器部署 环境配置安装git bash 首先本地安装Git，具体可以自行百度。 安装完后再任意目录下点击右键 进入git bash，之后我们的操作都将在这里进行 （PS：实测cmd也可以，如果遇到一些操作在git bash里得不到响应可以在cmd中尝试进行，注意要在相同的路径下面） 安装node.js 首先安装好node.js环境，直接官网下载就行了 安装好后查看版本 正常无误即可 安装hexo 进入gitbash，使用node.js的包管理器npm，输入 1npm install -g hexo 等待安装完成即可 本地调试安装完成后在自己想要放博客的文件夹根目录下，输入 可以看到hexo生成以下文件 node_modules（项目依赖包） public（存放生成页面） scaffolds：命令生成文章等的模板 source：资源库，放置比如文章、跳转页面等 themes：主题 _config.yml：整个博客的配置 package.json：项目所需模块项目的配置信息 这时你已经可以启动你的博客了，输入 123hexo clean //清除之前的配置，防止博客配置出错的必要操作 hexo generate //可缩写为hexo g 创建资源文件 hexo server //可缩写为hexo s 启动server服务器 当显示下面代码时说明启动成功 12$INFO Start processing$INFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop 按流程输入localhost:4000当你看到下图说明基本配置成功了 3.服务器部署现在我们就要在公网上部署hexo了 这里我用的是Github Page对博客进行托管，有条件的同学也可以自己部署到服务器上，这是暂时不提 首先需要在GitHub上新建一个仓库，命名形式为yourusername.github.io（yourusername是你的git账户,重要) 然后进入setting查看是否开启 接着创建ssh密钥对，将公钥放在GitHub上 在全局setting-&gt;SSH and GPG keys-&gt;new SSH keys添加 如何本地创建SSH密钥对，请查 123$git config --global user.name "Your Name"$git config --global user.email "email@example.com"$ssh-keygen -t rsa -C "youremail@example.com" 接着打开博客文件夹根目录，前往_config.yml文件，找到如下字段，修改自己的repo 1deploy: type: git repo: git@github.com:yourusername/yourusername.github.io.git branch: master 然后在gitbash输入 1$npm install hexo-deployer-git --save 依次执行 123hexo cleanhexo generate //可缩写为hexo ghexo deploy //可缩写为hexo d 部署hexo 打开https://yourgithubname.github.io，看到与上一步相同的页面，证明博客搭建完成 至此，博客的基本搭建基本完成。 接着如果需要书写博客，使用如下命令 1hexo new &lt;layout&gt; &quot;title&quot; 这里的对应我们要添加的内容，如果是posts就是添加新的文章，如果是page就是添加新的页面。 1hexo publish &lt;layout&gt; &lt;filename&gt; 发布内容，实际上是将内容从drafts（草稿）文件夹移到posts（文章）文件夹。 接着重启就行 hexo优化配置虽然搭建了基本的博客，但是每次初始化时都要花很多时间（可以但没必要） 因此我们可以写一个脚本自动完成固定命令的执行 新建一个txt文件，输入 1 2 3 hexo clean hexo g hexo d 保存后将后缀名改成.bat就可以点击执行了 个性化你的Hexo 博客真正让人充满激情的地方在于它可以让你个性化，真正把博客作为你自己的小天地，这次我就结合自己搭建博客的实例分析一下如何个性化自己的博客吧 添加主题NEXT添加RSS菜单图标选择如何选择博客上丰富的图标？难道要下载到本地？为了提升博客的打开速度，建议所有的静态资源能通过HTTP下载就HTTP下载，因为部署博客的服务器带宽终究有限，如果因为加载静态资源而占用带宽，会降低博客的阅读体验 这里那awesome icon举例 在首页寻找自己想要的图标，然后再主题配置文件的菜单图标项填&lt;! class=&quot;fa fa-fw fa-envelope&gt;&lt;/i&gt;即可 引入评论插件gitalkgitalk插件将GitHub作为你的数据库，评论用户通过GitHub账号登录并发表评论，非常适合国内的开发者进行及交流讨论 注册oauth app 下载gitalk插件 集成Gitalk 引入Aplayer外链生成地址 踩坑搜索模块 config.yml没有注意空格 注意备份 require是node.js的语法。。 打包js文件 出现各种都是npm包的问题 json.package是用来处理依赖问题的，有了它，就不用像远古时代那样一个个install，同时写好webpack.comfig.js才可以打包成功]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用java优雅爬虫——Jsoup]]></title>
    <url>%2F2019%2F09%2F18%2Fjsoup%2F</url>
    <content type="text"><![CDATA[jsoup是一个用于处理真实HTML的Java库。它提供了一个非常方便的API，用于提取和操作数据，使用最好的DOM，CSS和类似jquery的方法。 目录 jsoup概述 使用场景 DOM解析 CSS选择器 HTML过滤 逻辑分析 总结 jousp概述官方解释： jsoup是一个用于处理真实HTML的Java库。它提供了一个非常方便的API，用于提取和操作数据，使用最好的DOM，CSS和类似jquery的方法。 个人接触到Jsoup是在用java写爬虫时，苦恼于大量使用正则匹配不仅降低了代码的可读性，相对也比较费时费力。这时候，一款爬虫框架突然映入眼帘，那就是jsoup。 作为一款轻量，功能强大的爬虫框架，jsoup让简单抓取网页信息变得优雅，便捷。 虽然是一个java库，但是它的使用逻辑却无比接近于jQuery，以至于只要是熟悉或是了解JQuery的人可以轻而易举地上手这款框架。 使用场景DOM解析jsoup的dom解析异常简单吗，只需要new一个ducumnet对象即可实现获取这个网页元素，接下来以解析一个网页为例。可以看到，将网页转化成ducument类，之后的Element类以及其子类都可以看成是一个个节点，通过调用相关方法实现整个文件节点的遍历。同时，Element类的getElementByTag让人很容易联想到js中的相关方法，因此只要有点JS基础和java基础的人看这段代码都不会觉得陌生。 这里以查询学生成绩信息为例： 1234567891011121314151617181920212223242526272829303132public class jsoupTest &#123; public String getGrade(String stu_num, String id_num) throws IOException &#123; String testURL = "http://jwc.cqupt.edu.cn/showS tuQmcj.php"; //目标网页 Connection con = Jsoup.connect(testURL); //获取连接 con.data("xh", stu_num); //填写参数 con.data("sfzh", id_num); Document document = con.post(); //选择发送方式,获取整个网页信息，存在documnet类里 Element pTable = document.body().getElementsByClass("pTable").get(0); //通过class属性 ，获取子类元素 Elements trs = pTable.getElementsByTag("tbody").get(0).children(); trs.forEach(tr -&gt; &#123; //遍历&lt;tr&gt;标签 if (!tr.children().isEmpty()) &#123; Element element = tr.getElementsByTag("td").get(0); if (!element.text().equals("课程类型")) &#123; GradeInfo gradeInfo = new GradeInfo(); gradeInfo.setProperty(tr.getElementsByTag("td").get(0).text()); String term = tr.getElementsByTag("td").get(1).text(); System.out.println(term); System.out.println(tr.getElementsByTag("td").get(2).text()); System.out.println(tr.getElementsByTag("td").get(5).text()); System.out.println(tr.getElementsByTag("td").get(6).text()); System.out.println(tr.getElementsByTag("td").get(7).text()); &#125; &#125; &#125;); return ""; &#125;&#125; 结果： 只需要几行就可以完成对html的基本解析，而且所有的操作都可以用js的逻辑解释。或多或少比原生正则匹配要实用的多。 CSS选择器jsoup决心是想向前端靠齐了，除了基本的DOM解析操作外，它同时加入了CSS选择器，这个操作乍一看似乎没什么用处，但是当你真正去学习如何使用后你的就会发现这是多少好用。在针对较复杂地语句匹配时，使用选择器可以轻而易举地筛选出你想要的元素，可以帮你节省大量代码。 使用方法：可以用Element.select(String selector)和Element.select(String selector)实现. 12345678910111213141516public void getBySelect() throws IOException &#123; String testURL = "&lt;html&gt;" + "&lt;head&gt;&lt;/head&gt;"+ "&lt;body&gt;"+"&lt;span id=\"grade\"&gt;成绩&lt;/span&gt;"+"&lt;span id = \"subject\"&gt;课程&lt;/span&gt;"+"&lt;span id = \"name\"&gt;姓名&lt;/span&gt;"+"&lt;span id = \"stunum\"&gt;学号&lt;/span&gt;"+ "&lt;span class = \"score\"&gt;85&lt;/span &gt;"+"&lt;span class = \"class\"&gt;语文&lt;/span&gt;"+"&lt;span class = \"stuname\"&gt;小明&lt;/span class = \"number\"&gt;"+"&lt;span&gt;201721001&lt;/span&gt;"+ "&lt;span id=\"grade\"&gt;80&lt;/span&gt;"+"&lt;span id = \"subject\"&gt;数学&lt;/span&gt;"+"&lt;span id = \"name\"&gt;小明&lt;/span&gt;"+"&lt;span id = \"stunum\"&gt;2017210001&lt;/span&gt;"+ "&lt;/body&gt;&lt;/html&gt;"; //利用字符串拼接出HTML标签 //获取连接 Document document = Jsoup.parse(testURL); //将HTML转化成可遍历地document类 Elements elements = document.select("span:matchesOwn(^8)"); for (Element element: elements) &#123; System.out.println(elements.text()); &#125; &#125; 结果： CSS选择器类似于JQuery和CSS中使用的选择器，可以通过特定的选择器语法将对指定元素进行筛选 对于选择器的筛选，这里推荐一篇文章：详解JSOUP的Select选择器语法 (链接已失效） HTML过滤这个功能也是偶然看见的，不过现在想来也理所当然，过滤网页信息本身就是Jsoup分内的事。当时正在看XSS攻击方面的知识，突然发现jsoup在安全方面已早有考虑，已本身优秀的HTML解析为基础，抵御XSS攻击自然也是十分优秀。 XSS注入本质就是在HTML中插入特定的标签改变原来标签的含义，因此防止XSS攻击的本质是能分辨并及时过滤掉多余或是无效的HTML标签。对此，jsoup有一个白名单机制，通过clean方法可以一步通过白名单设置的过滤规则清理所有的标签，同时也会保留适当标签和禁止图片显示的功能。 12345678910111213141516171819202122232425262728293031/** * xss过滤 * */public class JsoupUtil &#123; /** * 使用自带的basicWithImages 白名单 * 允许的便签有a,b,blockquote,br,cite,code,dd,dl,dt,em,i,li,ol,p,pre,q,small,span, * strike,strong,sub,sup,u,ul,img * 以及a标签的href,img标签的src,align,alt,height,width,title属性 */ private static final Whitelist whitelist = Whitelist.basicWithImages(); /** 配置过滤化参数,不对代码进行格式化 */ private static final Document.OutputSettings outputSettings = new Document.OutputSettings().prettyPrint(false); static &#123; // 富文本编辑时一些样式是使用style来进行实现的 // 比如红色字体 style="color:red;" // 所以需要给所有标签添加style属性 whitelist.addAttributes(":all", "style"); &#125; public static String clean(String content) &#123; return Jsoup.clean(content, "", whitelist, outputSettings); &#125; public static void main(String[] args) throws FileNotFoundException, IOException &#123; String text = "&lt;a href=\"http://www.baidu.com/a\" onclick=\"alert(1);\"&gt;sss&lt;/a&gt;&lt;script&gt;alert(0);&lt;/script&gt;sss"; System.out.println(clean(text)); &#125;&#125; 逻辑分析 jsoup作为一款轻便的爬虫框架，全部由Jonathan Hedley独立写出，因此代码相比其他一些笨重的框架要简洁很多，我通过网上一些解析jsoup源码的博客，加深对jsoup的理解。废话不多说，让我们看看jsoup的魅力吧！ 借用别人博客里整理的图片，可以看到，让java能像js那样使用类似标签的嵌套存储的方法就是在这里就是利用自定义的node抽象类，将属性存储在类似树状的结构中这，这样做不仅有利于之后的DOM树解析，也容易遍历，有利于性能的提高。 我们再看看CSS选择器的实现逻辑。这是selector的源码列表 jsoup在关于selector的实现大致是利用Evaluator抽象类，Selector选择的表达式都会通过QueryParser最终编译到对应的Evaluator类上，然后此类又有很多派生子类，从而分别实现不同功能。逻辑思路还算简单，但是具体代码我还不曾仔细研读，因此在此也不再赘述。不过其中进行嵌套实现对象的思路还是值得借鉴的。 在HTML过滤方面，jsoup防止XSS攻击的大致策略是 将HTMl字符串解析成document对象，这样保证了无法通过注入一段无用的脚本和字符串拼接导致网页的功能发生了改变 将一些高频出现的危险系数较高的标签加入白名单进行提前过滤 总结jsoup在操作便捷度上已经展现了它的实力，但是在性能上，考虑到它的底层还是通过正则进行匹配，因此对于一些一些简单的HTML解析或许直接正则的最快的；但是，当HTML页面比较复杂，这便是jsoup大显身手的时候了。 但是jsoup还是有很多不足，例如 只能处理静态页面，对于动态显示或者后端渲染后后的页面无法正常进行爬取，这时就需要利用其他的工具例如httpunit进行模拟的ajax请求。 jsoup总归还只是个人项目，在后期的维护方面还是存在一定的不确定性，如果需要用应用在一些大型的长久性的项目中还需三思。 jsoup的底层实现还是正则匹配，尽管jsoup本身够轻量，但它依然需要解析整个HTML，再进行进一步的搜索，因此在一些简单的网页解析中，肯定还是直接上正则来的直接来的方便。但是如果网页的结构足够复杂，使用正则的的代码量巨大，那么jsoup不失为一个不错的选择。 总而言之，jsoup作为一款轻量的爬虫框架，在HTML解析方面的表现还是很不错的，如果平时希望偷点懒，节省点时间和代码量，完全推荐大家使用。]]></content>
      <categories>
        <category>技术探索</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown常用格式测试]]></title>
    <url>%2F2019%2F09%2F18%2Ftest%2F</url>
    <content type="text"><![CDATA[这是对hexo 渲染的markdown模板的测试，同时也贴一下常用的markdown书写格式和插件使用，做一下备忘~ 以下所有代码只需要复制到markdown编辑器即可使用，同时要注意编辑器的版本和兼容性，比如我之间用的typora就总是不能正常显示html格式🤣，还好博客重建前更新了版本又可以用了… 插件测试Aplayer播放测试 var ap = new APlayer({ element: document.getElementById("aplayer-MkrCfcDb"), narrow: false, autoplay: false, showlrc: false, music: { title: "三轮学", author: "Jeff Williams", url: "http://music.163.com/song/media/outer/url?id=5231510.mp3", pic: "", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 12&#123;% aplayer "可以了" "陈奕迅" "音乐地址" "图片地址" %&#125;外链地址http://music.163.com/song/media/outer/url?id=478384.mp3 Dplayer播放测试(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"theme":"#FADFA3","loop":true,"video":{"url":"http://lc-dtfowewy.cn-n1.lcfile.com/e84e4b6519b04ed3c497/400%E7%B1%B3.mp4","pic":"http://devtest.qiniudn.com/若能绽放光芒.png"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() 1&#123;% dplayer "url=视频地址" "pic=图片地址" "loop=yes" "theme=#FADFA3" "autoplay=false"%&#125; MARKDOWN格式测试我是黑体字(不，你是高亮) 我是微软雅黑 我是华文彩云 我是红色 我是绿色 我是蓝色 我是尺寸 我是黑体，绿色，尺寸为5 123456789101112131415&lt;mark&gt;我是黑体字(不，你是高亮)&lt;/mark&gt;&lt;font face="微软雅黑"&gt;我是微软雅黑&lt;/font&gt;&lt;font face="STCAIYUN"&gt;我是华文彩云&lt;/font&gt;&lt;font color=red&gt;我是红色&lt;/font&gt;&lt;font color=#008000&gt;我是绿色&lt;/font&gt;&lt;font color=Blue&gt;我是蓝色&lt;/font&gt;&lt;font size=5&gt;我是尺寸&lt;/font&gt;&lt;font face="黑体" color=green size=5&gt;我是黑体，绿色，尺寸为5&lt;/font&gt; 博客插件记录我用的博客主题是sagiri这里感谢原作者的和辛勤努力和慷慨开源，在部署的时候遇到了一些插件缺失的问题，这里稍微提一下，做一下备份 1234567891011121314151617181920212223242526window.FastClick = require('fastclick');require('lazyload');require('fancybox')(window.$);window.notie = require('corner-notie');require('./utils');require('./motion');require('./affix');require('./pisces')();require('./scrollspy');require('./post-details')();require('./bootstrap');require('./evanyou');require('./leancloud')();require('./share')();require('./scroll');require('./since');require('./title');require('./type');require('./kanban');require('./mix');require('./clipboard');require('./pjax');require('./online');require('./search');require('./cursor-effects');]]></content>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高性能mysql学习笔记]]></title>
    <url>%2F2019%2F09%2F18%2Fmysql-h%2F</url>
    <content type="text"><![CDATA[最近在看《高性能MySql》，感觉有些地方确实需要反复咀嚼才能真正领会，现在将内容总结了一下，方便以后二次阅读 数据库索引原理及优化 Mysql架构MVCC为MVCC是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开 销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC的实现,是通过保存数据在某个时间点的快照来实现的。也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的 创建高性能索引基础 Mysql只能高效地使用最左前缀列 索引类型索引是在存储层实现的，不是在服务器层 B-Tree索引默认索引，除了Archive不支持之外 不同引擎使用的存储结构不同，NDB集群存储引擎使用T-Tree结构，innoDB使用B+Tree结构 不同引擎使用性能也不同，MyISAM使用前缀压缩技术使索引更小，通过数据的物理位置引用被索引的列；InonDB按照原数据格式压缩存储，根据主键引用被索引的列 B-Tree表示值都是按顺序存储，每一个叶子到根的距离相同 查询从索引的根节点开始，根节点中存放的是指向叶子节点的指针 叶子节点的指针指向被索引数据 B-Tree索引适用于全键型、键值范围和键前缀查找中的最左前缀查找 全值匹配 与所有列进行匹配 最左前缀匹配 只使用索引的第一列 列前缀匹配 某列的开头部分 范围值匹配 精确匹配某一列并范围匹配另外一列 只访问索引的查询 即覆盖查询 顺序查询，比如Order By 使用限制 非最左匹配查询 不能跳过索引的列 如果存在某列的范围查询，右边所有列都无法使用索引优化查询 哈希索引基于hash表实现，必须精确匹配才有效 只有memory引擎显式支持且默认支持 分为Slot槽和Value值 查找方法：计算hash值-&gt;找到指向指针-&gt;查找数据行 限制 只包含hash值和指针，不存储字段值，所以必须读取数据行。不过内存访问行速度较快，可以不考虑 索引数据不是按照索引值顺序存储，无法应用于排序 不支持部分匹配索引 只支持等值查询 存在哈希冲突，即不同索引列值具有相同的哈希值，当存在哈希冲突的时候必须遍历链表中的所有行指针，逐行比较 索引维护代价随着哈希冲突的增加越来越大 InnDB存在自适应哈希索引，当判断出查找频繁的索引值时，在B-Tree的基础上再创建一个Hash索引 可以创建自定义hash索引，进行伪hash索引，针对索引值较大时，但是必须保证索引值的唯一性 R-Tree空间索引MyISAM引擎支持空间索引，用作地理位置存储 全文索引查找文本中的关键字 … 索引的优点 减少服务器扫描数据量 帮助服务器避免排序和临时表 将随机IO变成3顺序IO 三星原则 索引是否将相关记录放在一起 索引中的数据顺序和排序顺序一致 包含查询中需要的全部列 高性能索引策略使用独立的列将索引列单独放在比较符号的一侧 前缀索引使用前缀索引索引很长的字符串，能让索引更小更快但是Mysql无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描 索引的选择性：不重复的索引值和数据表的记录总数的比值；唯一索引的选择性是1 尽可能要让前缀索引的选择性和全值素银的选择性相匹配 多列索引出现合并索引（Using Union）意味着索引建立糟糕 多个AND条件可以使用一个包含所有列的多列索引代替 多个OR条件会消耗大量CPU和内存资源 选择合适顺序一般思路： 将选择性最高的列放在索引最前列（不需要考虑排序或分组时） 计算选择性，选择性低的放在最前面 注意具体情况，留心WHERE字句中的排序、分组和范围条件等因素对查询的影响 聚簇索引聚簇索引是一种数据存储方式，不是索引类型 InnoDB中竖数据行存放在索引的叶子页中，聚簇即数据行和相邻键值紧凑存储在一起 优点 数据访问更快 使用覆盖索引扫扫描查询时可以直接使用主键值 数据相关保存 缺点 数据放在内存就没有优势了 插入速度依赖于插入顺序，按照主键插入最快 更新聚簇索引列代价较高 更新时存在列分裂的问题，即当页面插满的时候会将该页分裂容下该行，从而占用更多磁盘空间 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候 二级索引（非聚簇索引）很大 二级索引访问需要两次索引查找，因为二级索引节点保存的是行主键值而不是物理位置指向的指针 InnoDB和MyISAM的数据分布 MyISAM更加简单，二级索引存放的是指针；InnoDB存放的索引的主键值 覆盖索引 只读取索引即可以获取数据行，减少数据访问量；也更容易放入内存中 索引是顺序存储的，适合IO密集型 聚簇索引+覆盖索引比较高效 无法执行覆盖索引的原因 索引必须存索引列的值，mysql中一般只用B-Tree 没有任何索引能覆盖这个查询 不能使用LIKE操作 改良方法 使用延迟关联 索引扫描作为排序explain中type值为”index”说明使用索引扫描 当索引的列顺序和ORDERBY子句顺序一致并且所以列的排序方向一样时，mysql才能使用索引做结果排序 压缩（前缀压缩）索引MyISAM可以压缩索引大小 方法 先保存索引块的第一个值 其他值和第一个值比较得到相同前缀的字节数和剩余的不通过后缀部分 优势 占用更小的空间 缺点 无法使用二分查找等，有些操作如倒序操作速度不行 不适合CPU密集型应用 冗余和重复索引杜绝重复索引 冗余索引出现情况 需要扩展索引又要保证原索引性能不被影响 注意索引越多维护成本越高 索引和锁索引可以让査询锁定更少的行。 InnoDB在二级索引上使用共享(读）锁，但访问主键索引需要排他（写）锁。这消除了使用覆盖索引的可能性，并且使得SELECT FOR UPDATE比LOCK IN SHARE MODE或非锁定査询要慢很多。 索引案例支持多种过滤条件 字段选择少的放在前面 范围查找的字段放在后面 避免多个范围查询范围列后面无法索引 可以尝试将一个范围列变成等值列进行比较 优化排序 使用延迟关联，通使用覆盖索引查询返回需要的主键，再根据这些主键关联原表获得需要的行 维护索引和表维护目的 找到并修复损坏的表 维护准确而的索引统计信息 较少碎片 更新索引统计信息MySQL的查询优化器有两个API records_in_range() InnoDB传入两个边界值得到估算值 info() 返回个各种类型数据 MySQL优化器使用的是基于成本的模型，而衡量成本的主要指标就是一个查询需要扫描多少行 ANALYZE TABLE：重新生成统计信息 减少索引和数据碎片行碎片 这种碎片指的是数据行被存储为多个地方的多个片段中。即使査询只从索引中访问一行记录，行碎片也会导致性能下降。 行间碎片 行间碎片是指逻辑上顺序的页，或者行在磁盘上不是顺序存储的。行间碎片对诸如全表扫描和聚簇索引扫描之类的操作有很大的影响，因为这些操作原本能够从磁盘上顺序存储的数据中获益。 剩余空间碎片 剩余空间碎片是指数据页中有大量的空余空间。这会导致服务器读取大量不需要的数据，从而造成浪费。 MylSAM表，这三类碎片化都可能发生。但``InnoDB不会出现短小的行碎片；InnoDB`会移动短小的行并重写到一个片段中。 解决方法 OPTIMIZE TABLE或者导出再导入的方式来重新整理数据 MylSAM引擎以通过排序算法重建索引的方式来消除碎片 InnoDB可以先删除，然后再重新创建索引的方式来消除索引的碎片化。 mysql&gt; ALTER TABLE &lt;table&gt; ENGINE=&lt;efigiwe&gt;; 总结 查询性能优化为什么查询速度会变慢查询的生命周期 查询的时间花费 优化数据访问査询性能低下最基本的原因是访问的数据太多 确认应用程序是否在检索大量超过需要的数据。这通常意味着访问了太多的行，但有时候也可能是访问了太多的列。 确认MySQL服务器层是否在分析大量超过需要的数据行。 举例 查询不需要记录 使用LIMIT限制查询数量 多表关联时返回全部列 总是取出全部列 重复查询相同的数据 查看是否扫描额外的记录影响开销的三指标 响应时间 扫描行数 返回行数 响应时间 由服务时间和排队时间组成 扫描行数 理想情况应该和返回行数相同 访问类型 全表扫描(最慢) 索引扫描 范围扫描 唯一索引查询 常数引用（最快） mysql使用where的方式 在索引中使用where条件来过滤不匹配的记录。 使用索引覆盖扫描返回记录 从数据表中返回数据，然后过滤不满足条件的记录 解决方案 使用索引覆盖扫描 改变库表结构 重写查询 重构查询方式切分查询按时清理数据 分解关联查询 缓存效率更高 执行单个查询时减少锁的竞争 数据库的高性能和可扩展 查询执行查询执行的过程 客户端发送一条査询给服务器。 服务器先检査査询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。 MySQL根据优化器生成的执行计划，调用存储引擎的API来执行査询。 将结果返回给客户端。 MySQL客户端/服务器通信协议半双工，优点简单快速，缺点数据包越长性能越低 查询状态 Sleep线程正在等待客户端发送新的请求。Query线程正在执行查询或者正在将结果发送给客户端。Locked在MySQL服务器层，该线程正在等待表锁。在存储引擎级别实现的锁，例如InnoDB的行锁，并不会体现在线程状态中。对于MylSAM来说这是一个比较典型的状态，但在其他没有行锁的引擎中也经常会出现。Analyzing and statistics线程正在收集存储引擎的统计信息，并生成査询的执行计划。Copying to tmp table [on disk]线程正在执行査询，并且将其结果集都复制到一个临时表中，这种状态一般要么是在做GROUP BY操作，要么是文件排序操作，或者是UNION操作。如果这个状态后面还有 “on disk”标记，那表示MySQL正在将一个内存临时表放到磁盘上。Sorting result线程正在对结果集进行排序。 Sending data这表示多种情况：线程可能在多个状态之间传送数据，或者在生成结果集，或者在向客户端返回数据。 查询优化处理解析SQL、预处理、优化SQL执行计划 语法解析器和预处理 通过关键字解析SQL生成一棵解析树，使用mysql的语法规则验证和解析查询 预处理器根据MySQL规则进一步检查解析树是否合法，下一步还会验证权限 查询优化器 优化器将语法树转化成执行计划，其作用就是找到最合适的执行计划 MySQL使用基于成本的优化，使用了复杂的优化策略 优化策略 静态优化：直接对解析树分析，完成优化 动态优化：将WHERE转化 ，运行时优化，和查询的上下文有关 MySQL对查询的静态优化只做一次，但是对动态优化则每次执行时都需要重新评估 能处理的优化类型 重新定义关联表的顺序 外连接转化成内连接 使用等价变换原则 优化COUNT()、MIN()、MAX() 预估并转化成常数表达式 覆盖索引扫描 子查询优化 提前终止查询 等值传播 列表IN()的比较 …. 数据和索引的统计信息 统计信息由存储引擎实现，MySQL査询优化器在生成査询的执行计划时，需要向存储引擎获取相应的统计信息 执行计划 mysql是一棵左侧深度优先的树而不是平衡树 关联查询优化器 MySQL会选择合适的关联顺序让查询成本京可能降低 但是等关联表的个数增加的时候，执行计划的搜索空间(需要检查的阶乘种关联顺序)增长迅速，此时优化器就会使用贪婪模式寻找最优关联顺序 排序优化 索引排序不起作用的时候MySQL需要自己进行排序，如果数据量小则在内存中进行，如果数据量大则需要使用磁盘，这个过程统一称为文件排序(filesort) 排序算法 两次传输排序 读取行指针和需要排序的字段，对其进行排序，然后再根据排序结果读取所需要的数据行。 单次传输排序 先读取査询所需要的所有列，然后再根据给定列进行排序，最后直接返回排序结果 查询执行引擎查询优化阶段每一个表创建了一个handler实例，优化器调用实例接口（``handler API`）获取表相关信息 返回结果 即使不需要返回结果集，mysql任然会返沪查询的一些信息 查询如果可以别缓存，mysql也会在这个阶段将结果存放到查询缓存中 返回是一个增量、逐步返回的过程，好处是服务器无需存储大量洁后果，也不会因为要返回太多接轨而消耗大量内存；而且，客户端也呢个第一时间获得返回的结果 查询优化器的局限性关联子查询 MySQL会将相关的外层表压到子査询中，它认为这样可以更高效率地查找到数据行 优化 使用函数GR0UP_C0NCAT()在IN()中构造一个由逗号分隔的列表(性能不佳) 等效改写 一般使用左外连接比子查询稍快 去重时会产生临时中间表，此时子查询比关联更快 具体问题具体分析 UNION限制有时，MySQL无法将限制条件从外层“下推”到内层，这使得原本能够限制部分返回结果的条件无法应用到内层査询的优化上。 索引合并优化当WHERE子句中包含多个复杂条件的 时候，MySQL能够访问单个表的多个索引以合并和交叉过滤的方式来定位需要査找的行 等值传递例如，有一个非常大的IN()列表，而MySQL优化器发现存在WHERE、ON或者USING的子句，将这个列表的值和另一个表的某个列相关联。那么优化器会将IN()列表都复制应用到关联的各个表中。 并行执行MySQL无法并行执行查询 哈希关联mysql不支持哈希关联，所有关联都是嵌套循环关联 松散索引扫描MySQL不支持松散索引扫描，无法按照不连续的方式扫描一个索引 最大值和最小值优化使用LIMIT重写查询，从而使得扫描记录数尽可能少 在同一张表上查询更新一般i情况MySQL不允许对哦同一张表同时进行查询和更新。 但是可以通过生成表的形式绕过限制 查询优化器的提示略 优化特定类型的查询优化COUNT()查询COUNT()： 它可以统计某个列值的数量，也可以统计行数。在统计列值时要求列值是非空的（不统计NULL)。如果在C0UNT()的括号中指定了列或者列的表达式，则统计的就是这个表达式有值的结果数 另一个作用是统计结果集的行数。当MySQL确认括号内的表达式值不可能为空时，实际上就是在统计行数 MylSAM的C0UNTO函数总是非常快，不过这是有前提条件的，即只有没有任何WHERE条件的COUNT(*)才非常快 优化策略 使用近似值 索引覆盖扫描 优化关联查询 确保ON或者USING子句中的列上有索引 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化这个过程。 优化GROUP BY和DISTINCT无法使用索引的时候，Group BY使用两种策略完成 使用临时表 使用文件排序进行分组 在分组査询的SELECT中直接使用非分组列通常导致结果是不定的，当索引改变，或者优化器选择不同的优化策略时都可能导致结果不一样 如果没有通过ORDER BY子句显式地指定排序列，当査询使用GROUP BY子句的时候，结果集会自动按照分组的字段进行排序。如果不关心结果集的顺序，而这种默认排序又导致了需要文件排序，则可以使用ORDER BY NULL,让MySQL不再进行文件排序。也可以在GROUP BY子句中直接使用DESC或者ASC关键字，使分组的结果集按需要的方向排序 优化GROUP BY WITH ROLLUP尽可能将该功能转移到应用程序中 优化LIMIT分页 尽可能地使用索引覆盖扫描 做一次关联操作再返回所需的列（关联延迟） 利用offset记录上次查询点直接向下查询 优化 SQL_CALC_FOUND_ROWS分页的时候，另一个常用的技巧是在LIMFT语句中加上``SQL_CALC_FOUND_ROWS`提示(hint)，这样就可以获得去掉LIMIT以后满足条件的行数，因此可以作为分页的总数 另一种做法是先获取并缓存较多的数据，然后每次分页从缓存中获取 优化UNION查询否则就一定要使用UNION ALL,这一点很重要。如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致对整个临时表的数据做唯一性检査。这样做的代价非常高。 静态查询分析使用用户自定义变量不能使用的情况 使用自定义变量的査询，无法使用査询缓存。 不能在使用常量或者标识符的地方使用自定义变量,例如表名、列名和LIMIT子句中。 用户自定义变量的生命周期是在一个连接中有效，所以不能用它们来做连接间的通信。 如果使用连接池或者持久化连接，自定义变量可能让看起来毫无关系的代码发生交互（如果是这样，通常是代码bug或者连接池bug,这类情况确实可能发生）。 在5.0之前的版本，是大小写敏感的，所以要注意代码在不同MySQL版本间的兼容性问题。 不能显式地声明自定义变量的类型。确定未定义变量的具体类型的时机在不同 MySQL版本中也可能不一样。如果你希望变量是整数类型，那么最好在初始化的时候就赋值为0,如果希望是浮点型则赋值为0.0,如果希望是字符串则赋值为”，用&lt;M户自定义变量的类型在赋值的时候会改变。MySQL的用户自定义变量是一个动态类型。 MySQL优化器在某些场景下可能会将这些变量优化掉，这可能导致代码不按预想的方式运行。 赋值的顺序和赋值的时间点并不总是固定的，这依赖于优化器的决定。实际情况可能很让人困惑，后面我们将看到这一点。 赋值符号：=的优先级非常低，所以需要注意，赋值表达式应该使用明确的括号。 使用未定义变量不会产生任何语法错误，如果没有意识到这一点，非常容易犯错。 案例队列表计算两点之间距离使用用户自定义的函数总结]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List类知识点总结]]></title>
    <url>%2F2019%2F09%2F10%2Fcollection2%2F</url>
    <content type="text"><![CDATA[List类知识点总结 LinkedList链表集合，继承AbstractSequentialList，实现Deque、Cloneable、Serializable。 实现Deque，可以当作双端序列使用 Cloneable：可被克隆 Serializable：可被序列化 属性12345678transient int size = 0; // 长度transient Node&lt;E&gt; first; // 头节点transient Node&lt;E&gt; last; // 尾节点private static class Node&lt;E&gt; &#123; E item; // 元素 Node&lt;E&gt; next; // 下一节点 Node&lt;E&gt; prev; // 上一节点&#125; 可见LinkedList实现的是双向链表 构造123456public LinkedList() &#123;&#125;public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125; 方法方法大全 123456789101112131415161718192021222324252627282930313233343536373839404142434445// Queue接口的方法boolean add(E e) // 尾部添加一个节点E element() // 获取头节点元素boolean offer(E e) // 尾部添加元素E peek() // 获取头节点元素E poll() // 获取头节点元素并移除E remove() // 获取头节点元素并移除// Deque接口的方法void addFirst(E e) // 添加元素至头节点void addLast(E e) // 添加元素至尾节点ListIterator&lt;E&gt; listIterator(int index) // 迭代器Iterator&lt;E&gt; descendingIterator() // 逆序迭代器E getFirst() // 获取头节点元素E getLast() // 获取尾节点元素boolean offerFirst(E e) // 头部添加元素boolean offerLast(E e) // 尾部添加元素E peekFirst() // 获取头节点元素，空集合返回nullE peekLast() // 获取尾节点元素，空集合返回nullE pollFirst() // 获取头节点元素并移除，空集合返回nullE pollLast() // 获取尾节点元素并移除，空集合返回nullE pop() // 出栈，弹出链表头节点void push(E e) // 压栈，添加在链表头部（与add()相反）boolean remove(Object o) // 移除指定节点元素E removeFirst() // 获取头节点元素并移除E removeLast() // 获取尾节点元素并移除boolean removeFirstOccurrence(Object o) // 移除指定元素（出现的第一个）boolean removeLastOccurrence(Object o) // 移除指定元素（出现的最后一个）// 其它void add(int index, E element) // 指定位置添加一个节点boolean addAll(Collection&lt;? extends E&gt; c) // 尾部添加指定集合中所有元素boolean addAll(int index, Collection&lt;? extends E&gt; c) // 指定位置添加指定集合中所有元素E get(int index) // 获取指定位置节点元素E set(int index, E element) // 替换指定位置元素E remove(int index) // 获取指定位置节点元素并移除int indexOf(Object o) // 获取元素首次出现的位置int lastIndexOf(Object o) // 逆序获取元素首次出现的位置void clear() // 清除所有元素boolean contains(Object o) // 是否包含某元素int size() // 大小Spliterator&lt;E&gt; spliterator() // spliteratorObject[] toArray() // 将集合以数组形式返回&lt;T&gt; T[] toArray(T[] a) // 将集合以数组形式返回，类型、大小由参数决定，不足返回nullObject clone() // 浅复制集合 add123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; // 保存last final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // Node(pre, element, next)，创建新节点，next为null last = newNode; // last为新节点 if (l == null) first = newNode; // 链表为空时，初始化first else l.next = newNode; // 链表不为空时，上一节点的next指向newNode size++; modCount++;&#125;public void add(int index, E element) &#123; checkPositionIndex(index); // 检查index &gt;= 0 &amp;&amp; index &lt;= size if (index == size) // 尾部添加节点 linkLast(element); else linkBefore(element, node(index));&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; final Node&lt;E&gt; pred = succ.prev; // 保存原位置节点前一个节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 创建新节点，设置pre，next succ.prev = newNode; // 设置原位置节点的pre为新节点 if (pred == null) first = newNode; // 原位置节点前一个节点为null，设置新节点为头节点 else pred.next = newNode; // 原位置节点的前一个节点的next为新节点 size++; modCount++;&#125;public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c); // 转为addAll(index, c)&#125;public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); // 检查index &gt;= 0 &amp;&amp; index &lt;= size Object[] a = c.toArray(); // 转为数组 int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; succ = null; // 插入到尾部 pred = last; &#125; else &#123; succ = node(index); // 插入到位置i（0&lt;=i&lt;size） pred = succ.prev; &#125; for (Object o : a) &#123; // 遍历节点，进行插入操作 @SuppressWarnings("unchecked") E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); // 设置pre，next if (pred == null) first = newNode; // pred为null，重置first else pred.next = newNode; // 设置前一个节点的next pred = newNode; // 重置pred &#125; if (succ == null) &#123; last = pred; // 插入到尾部，重置last &#125; else &#123; pred.next = succ; // 设置插入的最后一个节点的next succ.prev = pred; // 设置原位置节点的prev &#125; size += numNew; modCount++; return true;&#125; 迭代方式 随机访问：list.get(i)，效率超级低 迭代器：list.iterator()，效率最高 foreach：for(Object i: list)，效率其次 ArrayList 动态数组，继承AbstractList，实现List、RandomAccess、Cloneable、Serializable。 Q：ArrayList继承了AbstractList，为什么还要实现List？ A：显式表明ArrayList实现了List，AbstractList只是为了提升List实现的代码重用。 RandomAccess：标记接口，表明List的某个实现支持随机访问。 Cloneable：可被克隆`` Serializable：可被序列化 不是线程安全，多线程可以选择Vector或CopyOnWriteArrayList。 属性1234// 保存数据的数组transient Object[] elementData;// ArrayList中实际数据的数量private int size; Q：elementData需要序列化，为什么还要被transient修饰，被transient修饰了，为什么还能被序列化？ A：elementData是一个缓存数组，通常会预留容量，所以elementData中只有实际存放的元素需要被序列化，故被transient修饰防止所有元素被序列化；ArrayList中元素还能被序列化是因为它重写了writeObject()方法（debug看到这个方法是被反射获取调用的）。 123456789101112private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; int expectedModCount = modCount; s.defaultWriteObject(); s.writeInt(size); for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); // 只序列化size个元素 &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 构造123456789101112131415161718192021222324252627282930// 不可变的空数组private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;// 初始化指定大小的数组public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: " + initialCapacity); &#125;&#125; /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 第一个构造函数会将elementData初始化为空数组 第二个构造函数会将elementData初始化为指定大小的数组 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为10。 方法方法大全 1234567891011121314151617181920212223242526272829303132333435boolean add(E e) // 尾部添加元素void add(int index, E element) // 在指定位置添加元素boolean addAll(Collection&lt;? extends E&gt; c) // 尾部添加集合中所有元素boolean addAll(int index, Collection&lt;? extends E&gt; c) // 指定位置添加集合中所有元素void clear() // 清除所有元素Object clone() // 浅复制集合boolean contains(Object o) // 是否包含某元素void ensureCapacity(int minCapacity) // 确保ArrayList的容器能装下minCapacity个元素void forEach(Consumer&lt;? super E&gt; action) // 对每个元素执行action操作（迭代器模式）E get(int index) // 获取指定位置元素int indexOf(Object o) // 获取指定元素位置boolean isEmpty() // 是否为空集合Iterator&lt;E&gt; iterator() // 获取迭代器int lastIndexOf(Object o) // 获取指定位置，倒序查找ListIterator&lt;E&gt; listIterator() // 返回迭代器ListIteratorListIterator&lt;E&gt; listIterator(int index) // 在指定位置返回迭代器ListIteratorE remove(int index) // 移除并获取指定位置元素boolean remove(Object o) // 移除元素，仅移除遍历到的第一个boolean removeAll(Collection&lt;?&gt; c) // 移除指定集合中所有元素boolean removeIf(Predicate&lt;? super E&gt; filter) // 移除满足条件的元素void replaceAll(UnaryOperator&lt;E&gt; operator) // 按照operator操作替换所有元素boolean retainAll(Collection&lt;?&gt; c) // 只保留包含在指定集合中的元素E set(int index, E element) // 设置指定位置的元素值int size() // 集合大小void sort(Comparator&lt;? super E&gt; c) // 根据指定比较方式comparator排序Spliterator&lt;E&gt; spliterator() // 获取集合的spliterator，不会用List&lt;E&gt; subList(int fromIndex, int toIndex) // 获取fromIndex-toIndex的子集合Object[] toArray() // 转化为数组&lt;T&gt; T[] toArray(T[] a) // 创建一个新数组传入，返回新数组类型元素，元素不够置nullvoid trimToSize() // 缩减ArrayList容器的大小至真实存放元素的数量 add/扩容机制1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // 确保容器能装下(size+1)个元素 elementData[size++] = e; return true;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); // 数组越界检查 ensureCapacityInternal(size + 1); // arraycopy(src, srcPos, dest, destPos, length) 将src[srcPos, srcPos+length]复制到dest[destPos, destPos+length] System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125;public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0;&#125;public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0;&#125;// 数组越界检查private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); // 越界抛出异常&#125; 当添加元素时，会调用如下方法，若elementData为空数组，则将它扩展为默认长度（10）的数组；若添加元素后elementData长度不够，将会将elementData扩展为原来的1.5倍。 1234567891011121314151617181920212223242526272829303132333435// 默认容器大小private static final int DEFAULT_CAPACITY = 10;private void ensureCapacityInternal(int minCapacity) &#123; //当要add 进第1个元素时，minCapacity为1，在Math.max()方法比较后，minCapacity 为10。 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;// 确保ArrayList能装下minCapacity个元素private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; /** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; 具体逻辑 如果通过元素值添加，确保容器能装下；如果通过索引添加，之前还需要数组越界检查 当我们要 add 进第1个元素到 ArrayList 时，elementData.length 为0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当add第2个元素时，minCapacity 为2，此时e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length &gt; 0不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第3、4···到第10个元素时，依然不会执行grow方法，数组容量都为10。 直到添加第11个元素，minCapacity(为11)比elementData.length（为10）要大。进入grow方法进行扩容。 同时注意一下如果需要容量大于MAX_ARRAY_SIZE会进入hugeCapacity()方法: 1234567891011private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较 //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; remove123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public E remove(int index) &#123; rangeCheck(index); // 越界检查 modCount++; E oldValue = elementData(index); // 取出旧值，待返回 int numMoved = size - index - 1; // 待移动的个数 if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 向前移动一个位置 elementData[--size] = null; // 最后一个元素置null return oldValue;&#125;public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); // 所有元素向前移动一个位置 return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125;public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); // 需要非空值，null报空指针异常 return batchRemove(c, false);&#125;// 批量删除private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; try &#123; for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) // complement=false elementData[w++] = elementData[r]; // 只保留c中不存在的元素 &#125; finally &#123; if (r != size) &#123; // 不解，为什么r可能!=size System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; &#125; if (w != size) &#123; for (int i = w; i &lt; size; i++) // 剩余元素全置null elementData[i] = null; modCount += size - w; size = w; // 重置size modified = true; // 修改flag &#125; &#125; return modified;&#125; set123456public E set(int index, E element) &#123; rangeCheck(index); // 越界检查 E oldValue = elementData(index); elementData[index] = element; // 设置新值 return oldValue; // 返回旧值&#125; get12345678910111213141516171819202122232425262728293031public E get(int index) &#123; rangeCheck(index); return elementData(index); // elementData(int index)方法，等同于elementData[index]&#125;public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) // 返回遍历到的第一个元素位置 return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1; // 不存在返回-1&#125;public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0; // 使用indexOf()方法检测是否包含&#125;public boolean containsAll(Collection&lt;?&gt; c) &#123; for (Object e : c) if (!contains(e)) // 一旦不包含某元素，返回false return false; return true;&#125;public int size() &#123; return size;&#125;public boolean isEmpty() &#123; return size == 0;&#125; System.arraycopy()和Arraya.copyOf()联系： 看两者源代码可以发现 copyOf() 内部实际调用了 System.arraycopy() 方法 区别： arraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。 ensureCapacity ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的 1234567891011121314151617/** 如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 迭代方式 随机访问：list.get(i)，效率最高 迭代器：list.iterator() foreach：for(Object i: list) Collections工具类Collections 工具类常用方法: 排序 查找,替换操作 同步控制(不推荐，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合) 排序操作123456void reverse(List list)//反转void shuffle(List list)//随机排序void sort(List list)//按自然排序的升序排序void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑void swap(List list, int i , int j)//交换两个索引位置的元素void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面。 查找替换操作1234567int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll)int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c)void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素。int frequency(Collection c, Object o)//统计元素出现次数int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target).boolean replaceAll(List list, Object oldVal, Object newVal), 用新元素替换旧元素 同步控制Collections提供了多个synchronizedXxx()方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。 我们知道 HashSet，TreeSet，ArrayList,LinkedList,HashMap,TreeMap 都是线程不安全的。Collections提供了多个静态方法可以把他们包装成线程同步的集合。 Collections还可以设置不可变集合，提供了如下三类方法： 1234emptyXxx(): 返回一个空的、不可变的集合对象，此处的集合既可以是List，也可以是Set，还可以是Map。singletonXxx(): 返回一个只包含指定对象（只有一个或一个元素）的不可变的集合对象，此处的集合可以是：List，Set，Map。unmodifiableXxx(): 返回指定集合对象的不可变视图，此处的集合可以是：List，Set，Map。上面三类方法的参数是原有的集合对象，返回值是该集合的”只读“版本。 Arrays常见操作 排序 : sort() 查找 : binarySearch() 比较: equals() 填充 : fill() 转列表: asList() 转字符串 : toString() 复制: copyOf() 方法sort12345678910111213141516171819202122232425262728293031323334353637383940// *************排序 sort**************** int a[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;; // sort(int[] a)方法按照数字顺序排列指定的数组。 Arrays.sort(a); System.out.println("Arrays.sort(a):"); for (int i : a) &#123; System.out.print(i); &#125; // 换行 System.out.println(); // sort(int[] a,int fromIndex,int toIndex)按升序排列数组的指定范围 int b[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;; Arrays.sort(b, 2, 6); System.out.println("Arrays.sort(b, 2, 6):"); for (int i : b) &#123; System.out.print(i); &#125; // 换行 System.out.println(); int c[] = &#123; 1, 3, 2, 7, 6, 5, 4, 9 &#125;; // parallelSort(int[] a) 按照数字顺序排列指定的数组(并行的)。同sort方法一样也有按范围的排序 Arrays.parallelSort(c); System.out.println("Arrays.parallelSort(c)："); for (int i : c) &#123; System.out.print(i); &#125; // 换行 System.out.println(); // parallelSort给字符数组排序，sort也可以 char d[] = &#123; 'a', 'f', 'b', 'c', 'e', 'A', 'C', 'B' &#125;; Arrays.parallelSort(d); System.out.println("Arrays.parallelSort(d)："); for (char d2 : d) &#123; System.out.print(d2); &#125; // 换行 System.out.println(); binarySearch() 12345678// *************查找 binarySearch()****************char[] e = &#123; &apos;a&apos;, &apos;f&apos;, &apos;b&apos;, &apos;c&apos;, &apos;e&apos;, &apos;A&apos;, &apos;C&apos;, &apos;B&apos; &#125;;// 排序后再进行二分查找，否则找不到Arrays.sort(e);System.out.println(&quot;Arrays.sort(e)&quot; + Arrays.toString(e));System.out.println(&quot;Arrays.binarySearch(e, &apos;c&apos;)：&quot;);int s = Arrays.binarySearch(e, &apos;c&apos;);System.out.println(&quot;字符c在数组的位置：&quot; + s);]]></content>
      <tags>
        <tag>java</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F18%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
